{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'she', 'royal', 'is', 'queen', 'he', 'king', 'the'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "corpus_raw = 'He is the king . The king is royal . She is the royal queen'\n",
    "corpus_raw = corpus_raw.lower()\n",
    "\n",
    "words = []\n",
    "for word in corpus_raw.split():\n",
    "    if word != '.': \n",
    "        words.append(word)\n",
    "        \n",
    "words = set(words)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "is\n"
     ]
    }
   ],
   "source": [
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(words)\n",
    "\n",
    "for i,word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word\n",
    "\n",
    "print(word2int['queen'])\n",
    "print(int2word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'the', 'king'], ['the', 'king', 'is', 'royal'], ['she', 'is', 'the', 'royal', 'queen']]\n"
     ]
    }
   ],
   "source": [
    "raw_sentences = corpus_raw.split('.')\n",
    "sentences = []\n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is'],\n",
       " ['he', 'the'],\n",
       " ['is', 'he'],\n",
       " ['is', 'the'],\n",
       " ['is', 'king'],\n",
       " ['the', 'he'],\n",
       " ['the', 'is'],\n",
       " ['the', 'king'],\n",
       " ['king', 'is'],\n",
       " ['king', 'the'],\n",
       " ['the', 'king'],\n",
       " ['the', 'is'],\n",
       " ['king', 'the'],\n",
       " ['king', 'is'],\n",
       " ['king', 'royal'],\n",
       " ['is', 'the'],\n",
       " ['is', 'king'],\n",
       " ['is', 'royal'],\n",
       " ['royal', 'king'],\n",
       " ['royal', 'is'],\n",
       " ['she', 'is'],\n",
       " ['she', 'the'],\n",
       " ['is', 'she'],\n",
       " ['is', 'the'],\n",
       " ['is', 'royal'],\n",
       " ['the', 'she'],\n",
       " ['the', 'is'],\n",
       " ['the', 'royal'],\n",
       " ['the', 'queen'],\n",
       " ['royal', 'is'],\n",
       " ['royal', 'the'],\n",
       " ['royal', 'queen'],\n",
       " ['queen', 'the'],\n",
       " ['queen', 'royal']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data  =  []\n",
    "WINDOW_SIZE  = 2\n",
    "\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 7) (34, 7)\n"
     ]
    }
   ],
   "source": [
    "x_train = [] \n",
    "y_train = [] \n",
    "for data_word in data:\n",
    "    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n",
    "\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "\n",
    "EMBEDDING_DIM = 5 \n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM]))\n",
    "hidden_representation = tf.add(tf.matmul(x,W1), b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 0  loss is :  2.839133\n",
      "Iteration # 1  loss is :  2.6726477\n",
      "Iteration # 2  loss is :  2.544283\n",
      "Iteration # 3  loss is :  2.4451938\n",
      "Iteration # 4  loss is :  2.3685036\n",
      "Iteration # 5  loss is :  2.3088212\n",
      "Iteration # 6  loss is :  2.2618968\n",
      "Iteration # 7  loss is :  2.224416\n",
      "Iteration # 8  loss is :  2.1938503\n",
      "Iteration # 9  loss is :  2.168321\n",
      "Iteration # 10  loss is :  2.1464636\n",
      "Iteration # 11  loss is :  2.127307\n",
      "Iteration # 12  loss is :  2.110165\n",
      "Iteration # 13  loss is :  2.094557\n",
      "Iteration # 14  loss is :  2.0801442\n",
      "Iteration # 15  loss is :  2.0666862\n",
      "Iteration # 16  loss is :  2.0540106\n",
      "Iteration # 17  loss is :  2.041991\n",
      "Iteration # 18  loss is :  2.0305333\n",
      "Iteration # 19  loss is :  2.0195653\n",
      "Iteration # 20  loss is :  2.0090318\n",
      "Iteration # 21  loss is :  1.9988889\n",
      "Iteration # 22  loss is :  1.9890996\n",
      "Iteration # 23  loss is :  1.9796358\n",
      "Iteration # 24  loss is :  1.970472\n",
      "Iteration # 25  loss is :  1.9615878\n",
      "Iteration # 26  loss is :  1.952965\n",
      "Iteration # 27  loss is :  1.944588\n",
      "Iteration # 28  loss is :  1.9364426\n",
      "Iteration # 29  loss is :  1.9285171\n",
      "Iteration # 30  loss is :  1.9208003\n",
      "Iteration # 31  loss is :  1.9132823\n",
      "Iteration # 32  loss is :  1.9059538\n",
      "Iteration # 33  loss is :  1.8988068\n",
      "Iteration # 34  loss is :  1.8918335\n",
      "Iteration # 35  loss is :  1.8850275\n",
      "Iteration # 36  loss is :  1.8783815\n",
      "Iteration # 37  loss is :  1.8718897\n",
      "Iteration # 38  loss is :  1.8655463\n",
      "Iteration # 39  loss is :  1.8593462\n",
      "Iteration # 40  loss is :  1.853284\n",
      "Iteration # 41  loss is :  1.847355\n",
      "Iteration # 42  loss is :  1.8415549\n",
      "Iteration # 43  loss is :  1.8358793\n",
      "Iteration # 44  loss is :  1.8303243\n",
      "Iteration # 45  loss is :  1.8248856\n",
      "Iteration # 46  loss is :  1.8195598\n",
      "Iteration # 47  loss is :  1.8143433\n",
      "Iteration # 48  loss is :  1.809233\n",
      "Iteration # 49  loss is :  1.804225\n",
      "Iteration # 50  loss is :  1.799317\n",
      "Iteration # 51  loss is :  1.7945052\n",
      "Iteration # 52  loss is :  1.7897877\n",
      "Iteration # 53  loss is :  1.7851609\n",
      "Iteration # 54  loss is :  1.7806226\n",
      "Iteration # 55  loss is :  1.7761701\n",
      "Iteration # 56  loss is :  1.7718011\n",
      "Iteration # 57  loss is :  1.7675134\n",
      "Iteration # 58  loss is :  1.7633044\n",
      "Iteration # 59  loss is :  1.7591717\n",
      "Iteration # 60  loss is :  1.7551142\n",
      "Iteration # 61  loss is :  1.7511289\n",
      "Iteration # 62  loss is :  1.7472144\n",
      "Iteration # 63  loss is :  1.7433685\n",
      "Iteration # 64  loss is :  1.7395897\n",
      "Iteration # 65  loss is :  1.7358761\n",
      "Iteration # 66  loss is :  1.7322259\n",
      "Iteration # 67  loss is :  1.7286375\n",
      "Iteration # 68  loss is :  1.7251097\n",
      "Iteration # 69  loss is :  1.7216406\n",
      "Iteration # 70  loss is :  1.7182287\n",
      "Iteration # 71  loss is :  1.7148731\n",
      "Iteration # 72  loss is :  1.7115719\n",
      "Iteration # 73  loss is :  1.7083242\n",
      "Iteration # 74  loss is :  1.7051287\n",
      "Iteration # 75  loss is :  1.7019837\n",
      "Iteration # 76  loss is :  1.6988888\n",
      "Iteration # 77  loss is :  1.695842\n",
      "Iteration # 78  loss is :  1.692843\n",
      "Iteration # 79  loss is :  1.6898903\n",
      "Iteration # 80  loss is :  1.686983\n",
      "Iteration # 81  loss is :  1.6841202\n",
      "Iteration # 82  loss is :  1.6813006\n",
      "Iteration # 83  loss is :  1.6785238\n",
      "Iteration # 84  loss is :  1.6757886\n",
      "Iteration # 85  loss is :  1.6730944\n",
      "Iteration # 86  loss is :  1.6704398\n",
      "Iteration # 87  loss is :  1.6678247\n",
      "Iteration # 88  loss is :  1.6652478\n",
      "Iteration # 89  loss is :  1.6627084\n",
      "Iteration # 90  loss is :  1.6602063\n",
      "Iteration # 91  loss is :  1.65774\n",
      "Iteration # 92  loss is :  1.6553096\n",
      "Iteration # 93  loss is :  1.6529137\n",
      "Iteration # 94  loss is :  1.6505523\n",
      "Iteration # 95  loss is :  1.6482242\n",
      "Iteration # 96  loss is :  1.6459293\n",
      "Iteration # 97  loss is :  1.6436666\n",
      "Iteration # 98  loss is :  1.6414359\n",
      "Iteration # 99  loss is :  1.6392365\n",
      "Iteration # 100  loss is :  1.6370674\n",
      "Iteration # 101  loss is :  1.6349287\n",
      "Iteration # 102  loss is :  1.6328197\n",
      "Iteration # 103  loss is :  1.6307398\n",
      "Iteration # 104  loss is :  1.6286885\n",
      "Iteration # 105  loss is :  1.6266652\n",
      "Iteration # 106  loss is :  1.6246698\n",
      "Iteration # 107  loss is :  1.6227014\n",
      "Iteration # 108  loss is :  1.6207598\n",
      "Iteration # 109  loss is :  1.6188446\n",
      "Iteration # 110  loss is :  1.616955\n",
      "Iteration # 111  loss is :  1.6150911\n",
      "Iteration # 112  loss is :  1.613252\n",
      "Iteration # 113  loss is :  1.6114378\n",
      "Iteration # 114  loss is :  1.6096474\n",
      "Iteration # 115  loss is :  1.6078807\n",
      "Iteration # 116  loss is :  1.6061378\n",
      "Iteration # 117  loss is :  1.6044173\n",
      "Iteration # 118  loss is :  1.6027198\n",
      "Iteration # 119  loss is :  1.6010445\n",
      "Iteration # 120  loss is :  1.5993909\n",
      "Iteration # 121  loss is :  1.5977585\n",
      "Iteration # 122  loss is :  1.5961474\n",
      "Iteration # 123  loss is :  1.5945572\n",
      "Iteration # 124  loss is :  1.5929872\n",
      "Iteration # 125  loss is :  1.5914372\n",
      "Iteration # 126  loss is :  1.5899069\n",
      "Iteration # 127  loss is :  1.5883958\n",
      "Iteration # 128  loss is :  1.5869038\n",
      "Iteration # 129  loss is :  1.5854304\n",
      "Iteration # 130  loss is :  1.5839752\n",
      "Iteration # 131  loss is :  1.5825382\n",
      "Iteration # 132  loss is :  1.5811186\n",
      "Iteration # 133  loss is :  1.5797167\n",
      "Iteration # 134  loss is :  1.5783315\n",
      "Iteration # 135  loss is :  1.576963\n",
      "Iteration # 136  loss is :  1.5756109\n",
      "Iteration # 137  loss is :  1.574275\n",
      "Iteration # 138  loss is :  1.5729549\n",
      "Iteration # 139  loss is :  1.5716501\n",
      "Iteration # 140  loss is :  1.5703607\n",
      "Iteration # 141  loss is :  1.569086\n",
      "Iteration # 142  loss is :  1.567826\n",
      "Iteration # 143  loss is :  1.5665804\n",
      "Iteration # 144  loss is :  1.5653487\n",
      "Iteration # 145  loss is :  1.564131\n",
      "Iteration # 146  loss is :  1.5629267\n",
      "Iteration # 147  loss is :  1.5617355\n",
      "Iteration # 148  loss is :  1.5605574\n",
      "Iteration # 149  loss is :  1.5593921\n",
      "Iteration # 150  loss is :  1.5582392\n",
      "Iteration # 151  loss is :  1.5570985\n",
      "Iteration # 152  loss is :  1.55597\n",
      "Iteration # 153  loss is :  1.554853\n",
      "Iteration # 154  loss is :  1.5537477\n",
      "Iteration # 155  loss is :  1.5526533\n",
      "Iteration # 156  loss is :  1.5515702\n",
      "Iteration # 157  loss is :  1.550498\n",
      "Iteration # 158  loss is :  1.5494363\n",
      "Iteration # 159  loss is :  1.5483849\n",
      "Iteration # 160  loss is :  1.5473437\n",
      "Iteration # 161  loss is :  1.5463127\n",
      "Iteration # 162  loss is :  1.5452912\n",
      "Iteration # 163  loss is :  1.5442793\n",
      "Iteration # 164  loss is :  1.5432768\n",
      "Iteration # 165  loss is :  1.5422834\n",
      "Iteration # 166  loss is :  1.5412991\n",
      "Iteration # 167  loss is :  1.5403236\n",
      "Iteration # 168  loss is :  1.5393567\n",
      "Iteration # 169  loss is :  1.5383983\n",
      "Iteration # 170  loss is :  1.5374479\n",
      "Iteration # 171  loss is :  1.536506\n",
      "Iteration # 172  loss is :  1.5355718\n",
      "Iteration # 173  loss is :  1.5346456\n",
      "Iteration # 174  loss is :  1.5337268\n",
      "Iteration # 175  loss is :  1.5328157\n",
      "Iteration # 176  loss is :  1.5319118\n",
      "Iteration # 177  loss is :  1.531015\n",
      "Iteration # 178  loss is :  1.5301255\n",
      "Iteration # 179  loss is :  1.5292425\n",
      "Iteration # 180  loss is :  1.5283668\n",
      "Iteration # 181  loss is :  1.5274973\n",
      "Iteration # 182  loss is :  1.5266347\n",
      "Iteration # 183  loss is :  1.5257779\n",
      "Iteration # 184  loss is :  1.5249279\n",
      "Iteration # 185  loss is :  1.5240837\n",
      "Iteration # 186  loss is :  1.5232457\n",
      "Iteration # 187  loss is :  1.5224137\n",
      "Iteration # 188  loss is :  1.5215871\n",
      "Iteration # 189  loss is :  1.5207665\n",
      "Iteration # 190  loss is :  1.5199515\n",
      "Iteration # 191  loss is :  1.5191417\n",
      "Iteration # 192  loss is :  1.5183375\n",
      "Iteration # 193  loss is :  1.5175385\n",
      "Iteration # 194  loss is :  1.5167449\n",
      "Iteration # 195  loss is :  1.5159559\n",
      "Iteration # 196  loss is :  1.5151722\n",
      "Iteration # 197  loss is :  1.5143934\n",
      "Iteration # 198  loss is :  1.5136194\n",
      "Iteration # 199  loss is :  1.5128502\n",
      "Iteration # 200  loss is :  1.5120857\n",
      "Iteration # 201  loss is :  1.5113255\n",
      "Iteration # 202  loss is :  1.5105699\n",
      "Iteration # 203  loss is :  1.5098188\n",
      "Iteration # 204  loss is :  1.5090721\n",
      "Iteration # 205  loss is :  1.5083295\n",
      "Iteration # 206  loss is :  1.5075911\n",
      "Iteration # 207  loss is :  1.5068569\n",
      "Iteration # 208  loss is :  1.5061268\n",
      "Iteration # 209  loss is :  1.5054007\n",
      "Iteration # 210  loss is :  1.5046784\n",
      "Iteration # 211  loss is :  1.5039599\n",
      "Iteration # 212  loss is :  1.5032455\n",
      "Iteration # 213  loss is :  1.5025346\n",
      "Iteration # 214  loss is :  1.5018274\n",
      "Iteration # 215  loss is :  1.501124\n",
      "Iteration # 216  loss is :  1.5004241\n",
      "Iteration # 217  loss is :  1.4997275\n",
      "Iteration # 218  loss is :  1.4990346\n",
      "Iteration # 219  loss is :  1.4983451\n",
      "Iteration # 220  loss is :  1.4976588\n",
      "Iteration # 221  loss is :  1.496976\n",
      "Iteration # 222  loss is :  1.4962964\n",
      "Iteration # 223  loss is :  1.49562\n",
      "Iteration # 224  loss is :  1.4949467\n",
      "Iteration # 225  loss is :  1.4942766\n",
      "Iteration # 226  loss is :  1.4936098\n",
      "Iteration # 227  loss is :  1.4929457\n",
      "Iteration # 228  loss is :  1.4922848\n",
      "Iteration # 229  loss is :  1.4916267\n",
      "Iteration # 230  loss is :  1.4909716\n",
      "Iteration # 231  loss is :  1.4903193\n",
      "Iteration # 232  loss is :  1.4896699\n",
      "Iteration # 233  loss is :  1.4890233\n",
      "Iteration # 234  loss is :  1.4883795\n",
      "Iteration # 235  loss is :  1.4877384\n",
      "Iteration # 236  loss is :  1.4870998\n",
      "Iteration # 237  loss is :  1.4864641\n",
      "Iteration # 238  loss is :  1.4858309\n",
      "Iteration # 239  loss is :  1.4852003\n",
      "Iteration # 240  loss is :  1.4845723\n",
      "Iteration # 241  loss is :  1.4839468\n",
      "Iteration # 242  loss is :  1.4833237\n",
      "Iteration # 243  loss is :  1.4827032\n",
      "Iteration # 244  loss is :  1.482085\n",
      "Iteration # 245  loss is :  1.4814692\n",
      "Iteration # 246  loss is :  1.480856\n",
      "Iteration # 247  loss is :  1.480245\n",
      "Iteration # 248  loss is :  1.4796362\n",
      "Iteration # 249  loss is :  1.4790299\n",
      "Iteration # 250  loss is :  1.4784257\n",
      "Iteration # 251  loss is :  1.4778239\n",
      "Iteration # 252  loss is :  1.4772241\n",
      "Iteration # 253  loss is :  1.4766268\n",
      "Iteration # 254  loss is :  1.4760313\n",
      "Iteration # 255  loss is :  1.4754382\n",
      "Iteration # 256  loss is :  1.4748472\n",
      "Iteration # 257  loss is :  1.4742582\n",
      "Iteration # 258  loss is :  1.4736714\n",
      "Iteration # 259  loss is :  1.4730865\n",
      "Iteration # 260  loss is :  1.4725039\n",
      "Iteration # 261  loss is :  1.471923\n",
      "Iteration # 262  loss is :  1.4713442\n",
      "Iteration # 263  loss is :  1.4707675\n",
      "Iteration # 264  loss is :  1.4701927\n",
      "Iteration # 265  loss is :  1.4696198\n",
      "Iteration # 266  loss is :  1.4690487\n",
      "Iteration # 267  loss is :  1.4684798\n",
      "Iteration # 268  loss is :  1.4679123\n",
      "Iteration # 269  loss is :  1.467347\n",
      "Iteration # 270  loss is :  1.4667836\n",
      "Iteration # 271  loss is :  1.4662219\n",
      "Iteration # 272  loss is :  1.465662\n",
      "Iteration # 273  loss is :  1.465104\n",
      "Iteration # 274  loss is :  1.4645476\n",
      "Iteration # 275  loss is :  1.4639931\n",
      "Iteration # 276  loss is :  1.4634404\n",
      "Iteration # 277  loss is :  1.4628894\n",
      "Iteration # 278  loss is :  1.4623402\n",
      "Iteration # 279  loss is :  1.4617926\n",
      "Iteration # 280  loss is :  1.4612467\n",
      "Iteration # 281  loss is :  1.4607025\n",
      "Iteration # 282  loss is :  1.46016\n",
      "Iteration # 283  loss is :  1.459619\n",
      "Iteration # 284  loss is :  1.45908\n",
      "Iteration # 285  loss is :  1.4585425\n",
      "Iteration # 286  loss is :  1.4580065\n",
      "Iteration # 287  loss is :  1.4574721\n",
      "Iteration # 288  loss is :  1.4569392\n",
      "Iteration # 289  loss is :  1.4564081\n",
      "Iteration # 290  loss is :  1.4558786\n",
      "Iteration # 291  loss is :  1.4553506\n",
      "Iteration # 292  loss is :  1.4548241\n",
      "Iteration # 293  loss is :  1.4542992\n",
      "Iteration # 294  loss is :  1.453776\n",
      "Iteration # 295  loss is :  1.4532542\n",
      "Iteration # 296  loss is :  1.452734\n",
      "Iteration # 297  loss is :  1.452215\n",
      "Iteration # 298  loss is :  1.4516977\n",
      "Iteration # 299  loss is :  1.4511821\n",
      "Iteration # 300  loss is :  1.4506677\n",
      "Iteration # 301  loss is :  1.4501548\n",
      "Iteration # 302  loss is :  1.4496436\n",
      "Iteration # 303  loss is :  1.4491338\n",
      "Iteration # 304  loss is :  1.4486253\n",
      "Iteration # 305  loss is :  1.4481184\n",
      "Iteration # 306  loss is :  1.4476129\n",
      "Iteration # 307  loss is :  1.4471087\n",
      "Iteration # 308  loss is :  1.4466063\n",
      "Iteration # 309  loss is :  1.446105\n",
      "Iteration # 310  loss is :  1.4456053\n",
      "Iteration # 311  loss is :  1.4451069\n",
      "Iteration # 312  loss is :  1.44461\n",
      "Iteration # 313  loss is :  1.4441143\n",
      "Iteration # 314  loss is :  1.4436201\n",
      "Iteration # 315  loss is :  1.4431275\n",
      "Iteration # 316  loss is :  1.4426361\n",
      "Iteration # 317  loss is :  1.442146\n",
      "Iteration # 318  loss is :  1.4416575\n",
      "Iteration # 319  loss is :  1.4411703\n",
      "Iteration # 320  loss is :  1.4406843\n",
      "Iteration # 321  loss is :  1.4401999\n",
      "Iteration # 322  loss is :  1.4397168\n",
      "Iteration # 323  loss is :  1.4392349\n",
      "Iteration # 324  loss is :  1.4387544\n",
      "Iteration # 325  loss is :  1.4382755\n",
      "Iteration # 326  loss is :  1.4377978\n",
      "Iteration # 327  loss is :  1.4373215\n",
      "Iteration # 328  loss is :  1.4368466\n",
      "Iteration # 329  loss is :  1.4363728\n",
      "Iteration # 330  loss is :  1.4359004\n",
      "Iteration # 331  loss is :  1.4354295\n",
      "Iteration # 332  loss is :  1.4349599\n",
      "Iteration # 333  loss is :  1.4344915\n",
      "Iteration # 334  loss is :  1.4340243\n",
      "Iteration # 335  loss is :  1.4335587\n",
      "Iteration # 336  loss is :  1.4330946\n",
      "Iteration # 337  loss is :  1.4326315\n",
      "Iteration # 338  loss is :  1.4321698\n",
      "Iteration # 339  loss is :  1.4317094\n",
      "Iteration # 340  loss is :  1.4312503\n",
      "Iteration # 341  loss is :  1.4307926\n",
      "Iteration # 342  loss is :  1.4303361\n",
      "Iteration # 343  loss is :  1.4298811\n",
      "Iteration # 344  loss is :  1.4294274\n",
      "Iteration # 345  loss is :  1.428975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 346  loss is :  1.4285235\n",
      "Iteration # 347  loss is :  1.4280739\n",
      "Iteration # 348  loss is :  1.4276253\n",
      "Iteration # 349  loss is :  1.4271783\n",
      "Iteration # 350  loss is :  1.4267322\n",
      "Iteration # 351  loss is :  1.4262877\n",
      "Iteration # 352  loss is :  1.4258443\n",
      "Iteration # 353  loss is :  1.4254025\n",
      "Iteration # 354  loss is :  1.4249618\n",
      "Iteration # 355  loss is :  1.4245224\n",
      "Iteration # 356  loss is :  1.4240843\n",
      "Iteration # 357  loss is :  1.4236476\n",
      "Iteration # 358  loss is :  1.4232123\n",
      "Iteration # 359  loss is :  1.4227781\n",
      "Iteration # 360  loss is :  1.4223452\n",
      "Iteration # 361  loss is :  1.4219139\n",
      "Iteration # 362  loss is :  1.4214838\n",
      "Iteration # 363  loss is :  1.4210547\n",
      "Iteration # 364  loss is :  1.4206272\n",
      "Iteration # 365  loss is :  1.4202011\n",
      "Iteration # 366  loss is :  1.4197761\n",
      "Iteration # 367  loss is :  1.4193525\n",
      "Iteration # 368  loss is :  1.4189303\n",
      "Iteration # 369  loss is :  1.4185092\n",
      "Iteration # 370  loss is :  1.4180897\n",
      "Iteration # 371  loss is :  1.4176711\n",
      "Iteration # 372  loss is :  1.4172543\n",
      "Iteration # 373  loss is :  1.4168386\n",
      "Iteration # 374  loss is :  1.4164244\n",
      "Iteration # 375  loss is :  1.4160111\n",
      "Iteration # 376  loss is :  1.4155995\n",
      "Iteration # 377  loss is :  1.415189\n",
      "Iteration # 378  loss is :  1.41478\n",
      "Iteration # 379  loss is :  1.4143723\n",
      "Iteration # 380  loss is :  1.4139658\n",
      "Iteration # 381  loss is :  1.4135606\n",
      "Iteration # 382  loss is :  1.4131569\n",
      "Iteration # 383  loss is :  1.4127545\n",
      "Iteration # 384  loss is :  1.4123535\n",
      "Iteration # 385  loss is :  1.4119538\n",
      "Iteration # 386  loss is :  1.4115553\n",
      "Iteration # 387  loss is :  1.4111582\n",
      "Iteration # 388  loss is :  1.4107623\n",
      "Iteration # 389  loss is :  1.4103678\n",
      "Iteration # 390  loss is :  1.4099748\n",
      "Iteration # 391  loss is :  1.4095831\n",
      "Iteration # 392  loss is :  1.4091926\n",
      "Iteration # 393  loss is :  1.4088036\n",
      "Iteration # 394  loss is :  1.4084159\n",
      "Iteration # 395  loss is :  1.4080296\n",
      "Iteration # 396  loss is :  1.4076445\n",
      "Iteration # 397  loss is :  1.4072609\n",
      "Iteration # 398  loss is :  1.4068785\n",
      "Iteration # 399  loss is :  1.4064975\n",
      "Iteration # 400  loss is :  1.4061179\n",
      "Iteration # 401  loss is :  1.4057397\n",
      "Iteration # 402  loss is :  1.4053627\n",
      "Iteration # 403  loss is :  1.4049871\n",
      "Iteration # 404  loss is :  1.4046129\n",
      "Iteration # 405  loss is :  1.4042401\n",
      "Iteration # 406  loss is :  1.4038686\n",
      "Iteration # 407  loss is :  1.4034984\n",
      "Iteration # 408  loss is :  1.4031296\n",
      "Iteration # 409  loss is :  1.4027623\n",
      "Iteration # 410  loss is :  1.4023962\n",
      "Iteration # 411  loss is :  1.4020315\n",
      "Iteration # 412  loss is :  1.4016681\n",
      "Iteration # 413  loss is :  1.4013059\n",
      "Iteration # 414  loss is :  1.4009455\n",
      "Iteration # 415  loss is :  1.4005861\n",
      "Iteration # 416  loss is :  1.4002284\n",
      "Iteration # 417  loss is :  1.3998718\n",
      "Iteration # 418  loss is :  1.3995167\n",
      "Iteration # 419  loss is :  1.3991629\n",
      "Iteration # 420  loss is :  1.3988105\n",
      "Iteration # 421  loss is :  1.3984594\n",
      "Iteration # 422  loss is :  1.3981098\n",
      "Iteration # 423  loss is :  1.3977615\n",
      "Iteration # 424  loss is :  1.3974146\n",
      "Iteration # 425  loss is :  1.397069\n",
      "Iteration # 426  loss is :  1.3967248\n",
      "Iteration # 427  loss is :  1.3963819\n",
      "Iteration # 428  loss is :  1.3960404\n",
      "Iteration # 429  loss is :  1.3957003\n",
      "Iteration # 430  loss is :  1.3953618\n",
      "Iteration # 431  loss is :  1.3950243\n",
      "Iteration # 432  loss is :  1.3946885\n",
      "Iteration # 433  loss is :  1.3943539\n",
      "Iteration # 434  loss is :  1.3940206\n",
      "Iteration # 435  loss is :  1.3936887\n",
      "Iteration # 436  loss is :  1.3933582\n",
      "Iteration # 437  loss is :  1.3930292\n",
      "Iteration # 438  loss is :  1.3927014\n",
      "Iteration # 439  loss is :  1.3923749\n",
      "Iteration # 440  loss is :  1.39205\n",
      "Iteration # 441  loss is :  1.3917265\n",
      "Iteration # 442  loss is :  1.391404\n",
      "Iteration # 443  loss is :  1.3910834\n",
      "Iteration # 444  loss is :  1.3907636\n",
      "Iteration # 445  loss is :  1.3904455\n",
      "Iteration # 446  loss is :  1.3901287\n",
      "Iteration # 447  loss is :  1.3898133\n",
      "Iteration # 448  loss is :  1.3894993\n",
      "Iteration # 449  loss is :  1.3891866\n",
      "Iteration # 450  loss is :  1.3888752\n",
      "Iteration # 451  loss is :  1.3885654\n",
      "Iteration # 452  loss is :  1.3882568\n",
      "Iteration # 453  loss is :  1.3879495\n",
      "Iteration # 454  loss is :  1.3876436\n",
      "Iteration # 455  loss is :  1.3873391\n",
      "Iteration # 456  loss is :  1.387036\n",
      "Iteration # 457  loss is :  1.3867341\n",
      "Iteration # 458  loss is :  1.3864336\n",
      "Iteration # 459  loss is :  1.3861346\n",
      "Iteration # 460  loss is :  1.3858368\n",
      "Iteration # 461  loss is :  1.3855405\n",
      "Iteration # 462  loss is :  1.3852454\n",
      "Iteration # 463  loss is :  1.3849517\n",
      "Iteration # 464  loss is :  1.3846594\n",
      "Iteration # 465  loss is :  1.3843683\n",
      "Iteration # 466  loss is :  1.3840789\n",
      "Iteration # 467  loss is :  1.3837904\n",
      "Iteration # 468  loss is :  1.3835036\n",
      "Iteration # 469  loss is :  1.3832178\n",
      "Iteration # 470  loss is :  1.3829336\n",
      "Iteration # 471  loss is :  1.3826506\n",
      "Iteration # 472  loss is :  1.382369\n",
      "Iteration # 473  loss is :  1.3820887\n",
      "Iteration # 474  loss is :  1.3818097\n",
      "Iteration # 475  loss is :  1.381532\n",
      "Iteration # 476  loss is :  1.3812557\n",
      "Iteration # 477  loss is :  1.3809805\n",
      "Iteration # 478  loss is :  1.380707\n",
      "Iteration # 479  loss is :  1.3804345\n",
      "Iteration # 480  loss is :  1.3801634\n",
      "Iteration # 481  loss is :  1.3798937\n",
      "Iteration # 482  loss is :  1.3796253\n",
      "Iteration # 483  loss is :  1.379358\n",
      "Iteration # 484  loss is :  1.3790922\n",
      "Iteration # 485  loss is :  1.3788276\n",
      "Iteration # 486  loss is :  1.3785645\n",
      "Iteration # 487  loss is :  1.3783025\n",
      "Iteration # 488  loss is :  1.3780419\n",
      "Iteration # 489  loss is :  1.3777825\n",
      "Iteration # 490  loss is :  1.3775244\n",
      "Iteration # 491  loss is :  1.3772675\n",
      "Iteration # 492  loss is :  1.377012\n",
      "Iteration # 493  loss is :  1.3767577\n",
      "Iteration # 494  loss is :  1.3765048\n",
      "Iteration # 495  loss is :  1.3762529\n",
      "Iteration # 496  loss is :  1.3760026\n",
      "Iteration # 497  loss is :  1.3757534\n",
      "Iteration # 498  loss is :  1.3755054\n",
      "Iteration # 499  loss is :  1.3752588\n",
      "Iteration # 500  loss is :  1.3750134\n",
      "Iteration # 501  loss is :  1.3747691\n",
      "Iteration # 502  loss is :  1.3745263\n",
      "Iteration # 503  loss is :  1.3742846\n",
      "Iteration # 504  loss is :  1.3740441\n",
      "Iteration # 505  loss is :  1.373805\n",
      "Iteration # 506  loss is :  1.3735669\n",
      "Iteration # 507  loss is :  1.3733301\n",
      "Iteration # 508  loss is :  1.3730947\n",
      "Iteration # 509  loss is :  1.3728604\n",
      "Iteration # 510  loss is :  1.3726271\n",
      "Iteration # 511  loss is :  1.3723955\n",
      "Iteration # 512  loss is :  1.3721647\n",
      "Iteration # 513  loss is :  1.3719352\n",
      "Iteration # 514  loss is :  1.371707\n",
      "Iteration # 515  loss is :  1.3714799\n",
      "Iteration # 516  loss is :  1.3712541\n",
      "Iteration # 517  loss is :  1.3710293\n",
      "Iteration # 518  loss is :  1.3708057\n",
      "Iteration # 519  loss is :  1.3705834\n",
      "Iteration # 520  loss is :  1.3703623\n",
      "Iteration # 521  loss is :  1.3701422\n",
      "Iteration # 522  loss is :  1.3699234\n",
      "Iteration # 523  loss is :  1.3697058\n",
      "Iteration # 524  loss is :  1.3694893\n",
      "Iteration # 525  loss is :  1.369274\n",
      "Iteration # 526  loss is :  1.3690598\n",
      "Iteration # 527  loss is :  1.3688467\n",
      "Iteration # 528  loss is :  1.3686348\n",
      "Iteration # 529  loss is :  1.3684242\n",
      "Iteration # 530  loss is :  1.3682145\n",
      "Iteration # 531  loss is :  1.3680059\n",
      "Iteration # 532  loss is :  1.3677984\n",
      "Iteration # 533  loss is :  1.3675922\n",
      "Iteration # 534  loss is :  1.367387\n",
      "Iteration # 535  loss is :  1.367183\n",
      "Iteration # 536  loss is :  1.3669802\n",
      "Iteration # 537  loss is :  1.3667783\n",
      "Iteration # 538  loss is :  1.3665775\n",
      "Iteration # 539  loss is :  1.3663778\n",
      "Iteration # 540  loss is :  1.3661793\n",
      "Iteration # 541  loss is :  1.3659818\n",
      "Iteration # 542  loss is :  1.3657854\n",
      "Iteration # 543  loss is :  1.3655901\n",
      "Iteration # 544  loss is :  1.3653958\n",
      "Iteration # 545  loss is :  1.3652028\n",
      "Iteration # 546  loss is :  1.3650104\n",
      "Iteration # 547  loss is :  1.3648194\n",
      "Iteration # 548  loss is :  1.3646294\n",
      "Iteration # 549  loss is :  1.3644404\n",
      "Iteration # 550  loss is :  1.3642524\n",
      "Iteration # 551  loss is :  1.3640655\n",
      "Iteration # 552  loss is :  1.3638798\n",
      "Iteration # 553  loss is :  1.3636948\n",
      "Iteration # 554  loss is :  1.3635111\n",
      "Iteration # 555  loss is :  1.3633281\n",
      "Iteration # 556  loss is :  1.3631464\n",
      "Iteration # 557  loss is :  1.3629656\n",
      "Iteration # 558  loss is :  1.3627858\n",
      "Iteration # 559  loss is :  1.362607\n",
      "Iteration # 560  loss is :  1.3624293\n",
      "Iteration # 561  loss is :  1.3622524\n",
      "Iteration # 562  loss is :  1.3620766\n",
      "Iteration # 563  loss is :  1.3619018\n",
      "Iteration # 564  loss is :  1.3617277\n",
      "Iteration # 565  loss is :  1.361555\n",
      "Iteration # 566  loss is :  1.361383\n",
      "Iteration # 567  loss is :  1.361212\n",
      "Iteration # 568  loss is :  1.361042\n",
      "Iteration # 569  loss is :  1.3608729\n",
      "Iteration # 570  loss is :  1.3607047\n",
      "Iteration # 571  loss is :  1.3605375\n",
      "Iteration # 572  loss is :  1.3603714\n",
      "Iteration # 573  loss is :  1.360206\n",
      "Iteration # 574  loss is :  1.3600416\n",
      "Iteration # 575  loss is :  1.359878\n",
      "Iteration # 576  loss is :  1.3597155\n",
      "Iteration # 577  loss is :  1.3595537\n",
      "Iteration # 578  loss is :  1.3593931\n",
      "Iteration # 579  loss is :  1.3592331\n",
      "Iteration # 580  loss is :  1.3590744\n",
      "Iteration # 581  loss is :  1.3589162\n",
      "Iteration # 582  loss is :  1.3587592\n",
      "Iteration # 583  loss is :  1.3586026\n",
      "Iteration # 584  loss is :  1.3584472\n",
      "Iteration # 585  loss is :  1.3582927\n",
      "Iteration # 586  loss is :  1.358139\n",
      "Iteration # 587  loss is :  1.3579862\n",
      "Iteration # 588  loss is :  1.3578342\n",
      "Iteration # 589  loss is :  1.3576832\n",
      "Iteration # 590  loss is :  1.357533\n",
      "Iteration # 591  loss is :  1.3573836\n",
      "Iteration # 592  loss is :  1.357235\n",
      "Iteration # 593  loss is :  1.3570873\n",
      "Iteration # 594  loss is :  1.3569404\n",
      "Iteration # 595  loss is :  1.3567942\n",
      "Iteration # 596  loss is :  1.3566492\n",
      "Iteration # 597  loss is :  1.3565046\n",
      "Iteration # 598  loss is :  1.3563609\n",
      "Iteration # 599  loss is :  1.3562181\n",
      "Iteration # 600  loss is :  1.3560761\n",
      "Iteration # 601  loss is :  1.355935\n",
      "Iteration # 602  loss is :  1.3557944\n",
      "Iteration # 603  loss is :  1.355655\n",
      "Iteration # 604  loss is :  1.3555161\n",
      "Iteration # 605  loss is :  1.3553779\n",
      "Iteration # 606  loss is :  1.3552407\n",
      "Iteration # 607  loss is :  1.3551041\n",
      "Iteration # 608  loss is :  1.3549683\n",
      "Iteration # 609  loss is :  1.3548335\n",
      "Iteration # 610  loss is :  1.3546991\n",
      "Iteration # 611  loss is :  1.3545656\n",
      "Iteration # 612  loss is :  1.354433\n",
      "Iteration # 613  loss is :  1.354301\n",
      "Iteration # 614  loss is :  1.3541698\n",
      "Iteration # 615  loss is :  1.3540392\n",
      "Iteration # 616  loss is :  1.3539095\n",
      "Iteration # 617  loss is :  1.3537803\n",
      "Iteration # 618  loss is :  1.3536521\n",
      "Iteration # 619  loss is :  1.3535244\n",
      "Iteration # 620  loss is :  1.3533976\n",
      "Iteration # 621  loss is :  1.3532715\n",
      "Iteration # 622  loss is :  1.353146\n",
      "Iteration # 623  loss is :  1.3530213\n",
      "Iteration # 624  loss is :  1.352897\n",
      "Iteration # 625  loss is :  1.3527738\n",
      "Iteration # 626  loss is :  1.352651\n",
      "Iteration # 627  loss is :  1.352529\n",
      "Iteration # 628  loss is :  1.3524077\n",
      "Iteration # 629  loss is :  1.352287\n",
      "Iteration # 630  loss is :  1.352167\n",
      "Iteration # 631  loss is :  1.3520478\n",
      "Iteration # 632  loss is :  1.3519292\n",
      "Iteration # 633  loss is :  1.3518112\n",
      "Iteration # 634  loss is :  1.3516939\n",
      "Iteration # 635  loss is :  1.351577\n",
      "Iteration # 636  loss is :  1.351461\n",
      "Iteration # 637  loss is :  1.3513455\n",
      "Iteration # 638  loss is :  1.3512309\n",
      "Iteration # 639  loss is :  1.3511169\n",
      "Iteration # 640  loss is :  1.3510032\n",
      "Iteration # 641  loss is :  1.3508904\n",
      "Iteration # 642  loss is :  1.3507781\n",
      "Iteration # 643  loss is :  1.3506665\n",
      "Iteration # 644  loss is :  1.3505555\n",
      "Iteration # 645  loss is :  1.3504453\n",
      "Iteration # 646  loss is :  1.3503355\n",
      "Iteration # 647  loss is :  1.3502263\n",
      "Iteration # 648  loss is :  1.3501177\n",
      "Iteration # 649  loss is :  1.3500098\n",
      "Iteration # 650  loss is :  1.3499023\n",
      "Iteration # 651  loss is :  1.3497956\n",
      "Iteration # 652  loss is :  1.3496895\n",
      "Iteration # 653  loss is :  1.3495837\n",
      "Iteration # 654  loss is :  1.3494787\n",
      "Iteration # 655  loss is :  1.3493743\n",
      "Iteration # 656  loss is :  1.3492703\n",
      "Iteration # 657  loss is :  1.3491671\n",
      "Iteration # 658  loss is :  1.3490644\n",
      "Iteration # 659  loss is :  1.3489621\n",
      "Iteration # 660  loss is :  1.3488605\n",
      "Iteration # 661  loss is :  1.3487594\n",
      "Iteration # 662  loss is :  1.3486588\n",
      "Iteration # 663  loss is :  1.3485588\n",
      "Iteration # 664  loss is :  1.3484594\n",
      "Iteration # 665  loss is :  1.3483605\n",
      "Iteration # 666  loss is :  1.3482621\n",
      "Iteration # 667  loss is :  1.3481644\n",
      "Iteration # 668  loss is :  1.348067\n",
      "Iteration # 669  loss is :  1.3479701\n",
      "Iteration # 670  loss is :  1.347874\n",
      "Iteration # 671  loss is :  1.3477782\n",
      "Iteration # 672  loss is :  1.3476831\n",
      "Iteration # 673  loss is :  1.3475884\n",
      "Iteration # 674  loss is :  1.3474942\n",
      "Iteration # 675  loss is :  1.3474004\n",
      "Iteration # 676  loss is :  1.3473073\n",
      "Iteration # 677  loss is :  1.3472146\n",
      "Iteration # 678  loss is :  1.3471224\n",
      "Iteration # 679  loss is :  1.3470306\n",
      "Iteration # 680  loss is :  1.3469394\n",
      "Iteration # 681  loss is :  1.3468488\n",
      "Iteration # 682  loss is :  1.3467586\n",
      "Iteration # 683  loss is :  1.3466688\n",
      "Iteration # 684  loss is :  1.3465796\n",
      "Iteration # 685  loss is :  1.3464907\n",
      "Iteration # 686  loss is :  1.3464024\n",
      "Iteration # 687  loss is :  1.3463145\n",
      "Iteration # 688  loss is :  1.3462272\n",
      "Iteration # 689  loss is :  1.3461403\n",
      "Iteration # 690  loss is :  1.346054\n",
      "Iteration # 691  loss is :  1.3459679\n",
      "Iteration # 692  loss is :  1.3458823\n",
      "Iteration # 693  loss is :  1.3457973\n",
      "Iteration # 694  loss is :  1.3457125\n",
      "Iteration # 695  loss is :  1.3456284\n",
      "Iteration # 696  loss is :  1.3455445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 697  loss is :  1.3454612\n",
      "Iteration # 698  loss is :  1.3453783\n",
      "Iteration # 699  loss is :  1.3452959\n",
      "Iteration # 700  loss is :  1.3452139\n",
      "Iteration # 701  loss is :  1.3451324\n",
      "Iteration # 702  loss is :  1.3450512\n",
      "Iteration # 703  loss is :  1.3449703\n",
      "Iteration # 704  loss is :  1.34489\n",
      "Iteration # 705  loss is :  1.3448102\n",
      "Iteration # 706  loss is :  1.3447307\n",
      "Iteration # 707  loss is :  1.3446516\n",
      "Iteration # 708  loss is :  1.344573\n",
      "Iteration # 709  loss is :  1.3444948\n",
      "Iteration # 710  loss is :  1.3444167\n",
      "Iteration # 711  loss is :  1.3443395\n",
      "Iteration # 712  loss is :  1.3442622\n",
      "Iteration # 713  loss is :  1.3441858\n",
      "Iteration # 714  loss is :  1.3441095\n",
      "Iteration # 715  loss is :  1.3440336\n",
      "Iteration # 716  loss is :  1.3439581\n",
      "Iteration # 717  loss is :  1.343883\n",
      "Iteration # 718  loss is :  1.3438083\n",
      "Iteration # 719  loss is :  1.3437339\n",
      "Iteration # 720  loss is :  1.3436599\n",
      "Iteration # 721  loss is :  1.3435864\n",
      "Iteration # 722  loss is :  1.3435134\n",
      "Iteration # 723  loss is :  1.3434405\n",
      "Iteration # 724  loss is :  1.3433682\n",
      "Iteration # 725  loss is :  1.3432959\n",
      "Iteration # 726  loss is :  1.3432243\n",
      "Iteration # 727  loss is :  1.3431529\n",
      "Iteration # 728  loss is :  1.343082\n",
      "Iteration # 729  loss is :  1.3430111\n",
      "Iteration # 730  loss is :  1.342941\n",
      "Iteration # 731  loss is :  1.3428711\n",
      "Iteration # 732  loss is :  1.3428016\n",
      "Iteration # 733  loss is :  1.3427323\n",
      "Iteration # 734  loss is :  1.3426635\n",
      "Iteration # 735  loss is :  1.3425949\n",
      "Iteration # 736  loss is :  1.3425267\n",
      "Iteration # 737  loss is :  1.3424588\n",
      "Iteration # 738  loss is :  1.3423913\n",
      "Iteration # 739  loss is :  1.3423243\n",
      "Iteration # 740  loss is :  1.3422574\n",
      "Iteration # 741  loss is :  1.3421909\n",
      "Iteration # 742  loss is :  1.3421247\n",
      "Iteration # 743  loss is :  1.3420589\n",
      "Iteration # 744  loss is :  1.3419935\n",
      "Iteration # 745  loss is :  1.3419281\n",
      "Iteration # 746  loss is :  1.3418633\n",
      "Iteration # 747  loss is :  1.3417988\n",
      "Iteration # 748  loss is :  1.3417345\n",
      "Iteration # 749  loss is :  1.3416705\n",
      "Iteration # 750  loss is :  1.341607\n",
      "Iteration # 751  loss is :  1.3415437\n",
      "Iteration # 752  loss is :  1.3414807\n",
      "Iteration # 753  loss is :  1.3414179\n",
      "Iteration # 754  loss is :  1.3413557\n",
      "Iteration # 755  loss is :  1.3412935\n",
      "Iteration # 756  loss is :  1.3412317\n",
      "Iteration # 757  loss is :  1.3411703\n",
      "Iteration # 758  loss is :  1.3411093\n",
      "Iteration # 759  loss is :  1.3410484\n",
      "Iteration # 760  loss is :  1.3409878\n",
      "Iteration # 761  loss is :  1.3409275\n",
      "Iteration # 762  loss is :  1.3408674\n",
      "Iteration # 763  loss is :  1.3408078\n",
      "Iteration # 764  loss is :  1.3407483\n",
      "Iteration # 765  loss is :  1.3406894\n",
      "Iteration # 766  loss is :  1.3406304\n",
      "Iteration # 767  loss is :  1.3405719\n",
      "Iteration # 768  loss is :  1.3405137\n",
      "Iteration # 769  loss is :  1.3404555\n",
      "Iteration # 770  loss is :  1.3403978\n",
      "Iteration # 771  loss is :  1.3403404\n",
      "Iteration # 772  loss is :  1.3402832\n",
      "Iteration # 773  loss is :  1.3402262\n",
      "Iteration # 774  loss is :  1.3401695\n",
      "Iteration # 775  loss is :  1.3401133\n",
      "Iteration # 776  loss is :  1.3400573\n",
      "Iteration # 777  loss is :  1.3400013\n",
      "Iteration # 778  loss is :  1.3399458\n",
      "Iteration # 779  loss is :  1.3398905\n",
      "Iteration # 780  loss is :  1.3398354\n",
      "Iteration # 781  loss is :  1.3397807\n",
      "Iteration # 782  loss is :  1.3397262\n",
      "Iteration # 783  loss is :  1.3396719\n",
      "Iteration # 784  loss is :  1.3396177\n",
      "Iteration # 785  loss is :  1.3395641\n",
      "Iteration # 786  loss is :  1.3395106\n",
      "Iteration # 787  loss is :  1.3394572\n",
      "Iteration # 788  loss is :  1.3394041\n",
      "Iteration # 789  loss is :  1.3393514\n",
      "Iteration # 790  loss is :  1.339299\n",
      "Iteration # 791  loss is :  1.3392466\n",
      "Iteration # 792  loss is :  1.3391944\n",
      "Iteration # 793  loss is :  1.3391428\n",
      "Iteration # 794  loss is :  1.3390913\n",
      "Iteration # 795  loss is :  1.3390398\n",
      "Iteration # 796  loss is :  1.3389887\n",
      "Iteration # 797  loss is :  1.3389379\n",
      "Iteration # 798  loss is :  1.3388872\n",
      "Iteration # 799  loss is :  1.3388369\n",
      "Iteration # 800  loss is :  1.3387868\n",
      "Iteration # 801  loss is :  1.3387367\n",
      "Iteration # 802  loss is :  1.3386871\n",
      "Iteration # 803  loss is :  1.3386375\n",
      "Iteration # 804  loss is :  1.3385882\n",
      "Iteration # 805  loss is :  1.3385394\n",
      "Iteration # 806  loss is :  1.3384905\n",
      "Iteration # 807  loss is :  1.338442\n",
      "Iteration # 808  loss is :  1.3383933\n",
      "Iteration # 809  loss is :  1.3383454\n",
      "Iteration # 810  loss is :  1.3382975\n",
      "Iteration # 811  loss is :  1.3382498\n",
      "Iteration # 812  loss is :  1.3382021\n",
      "Iteration # 813  loss is :  1.3381548\n",
      "Iteration # 814  loss is :  1.3381078\n",
      "Iteration # 815  loss is :  1.338061\n",
      "Iteration # 816  loss is :  1.3380142\n",
      "Iteration # 817  loss is :  1.3379679\n",
      "Iteration # 818  loss is :  1.3379215\n",
      "Iteration # 819  loss is :  1.3378755\n",
      "Iteration # 820  loss is :  1.3378298\n",
      "Iteration # 821  loss is :  1.3377839\n",
      "Iteration # 822  loss is :  1.3377386\n",
      "Iteration # 823  loss is :  1.3376935\n",
      "Iteration # 824  loss is :  1.3376483\n",
      "Iteration # 825  loss is :  1.3376036\n",
      "Iteration # 826  loss is :  1.3375589\n",
      "Iteration # 827  loss is :  1.3375144\n",
      "Iteration # 828  loss is :  1.33747\n",
      "Iteration # 829  loss is :  1.3374262\n",
      "Iteration # 830  loss is :  1.3373822\n",
      "Iteration # 831  loss is :  1.3373386\n",
      "Iteration # 832  loss is :  1.337295\n",
      "Iteration # 833  loss is :  1.3372519\n",
      "Iteration # 834  loss is :  1.3372087\n",
      "Iteration # 835  loss is :  1.3371657\n",
      "Iteration # 836  loss is :  1.337123\n",
      "Iteration # 837  loss is :  1.3370806\n",
      "Iteration # 838  loss is :  1.3370382\n",
      "Iteration # 839  loss is :  1.3369958\n",
      "Iteration # 840  loss is :  1.336954\n",
      "Iteration # 841  loss is :  1.3369122\n",
      "Iteration # 842  loss is :  1.3368706\n",
      "Iteration # 843  loss is :  1.3368291\n",
      "Iteration # 844  loss is :  1.3367878\n",
      "Iteration # 845  loss is :  1.3367467\n",
      "Iteration # 846  loss is :  1.3367057\n",
      "Iteration # 847  loss is :  1.336665\n",
      "Iteration # 848  loss is :  1.3366244\n",
      "Iteration # 849  loss is :  1.336584\n",
      "Iteration # 850  loss is :  1.3365438\n",
      "Iteration # 851  loss is :  1.3365037\n",
      "Iteration # 852  loss is :  1.3364639\n",
      "Iteration # 853  loss is :  1.336424\n",
      "Iteration # 854  loss is :  1.3363847\n",
      "Iteration # 855  loss is :  1.3363451\n",
      "Iteration # 856  loss is :  1.336306\n",
      "Iteration # 857  loss is :  1.3362668\n",
      "Iteration # 858  loss is :  1.336228\n",
      "Iteration # 859  loss is :  1.3361892\n",
      "Iteration # 860  loss is :  1.3361505\n",
      "Iteration # 861  loss is :  1.3361123\n",
      "Iteration # 862  loss is :  1.336074\n",
      "Iteration # 863  loss is :  1.3360358\n",
      "Iteration # 864  loss is :  1.3359979\n",
      "Iteration # 865  loss is :  1.33596\n",
      "Iteration # 866  loss is :  1.3359225\n",
      "Iteration # 867  loss is :  1.335885\n",
      "Iteration # 868  loss is :  1.3358479\n",
      "Iteration # 869  loss is :  1.3358107\n",
      "Iteration # 870  loss is :  1.3357735\n",
      "Iteration # 871  loss is :  1.3357368\n",
      "Iteration # 872  loss is :  1.3357\n",
      "Iteration # 873  loss is :  1.3356634\n",
      "Iteration # 874  loss is :  1.3356271\n",
      "Iteration # 875  loss is :  1.3355908\n",
      "Iteration # 876  loss is :  1.3355547\n",
      "Iteration # 877  loss is :  1.3355188\n",
      "Iteration # 878  loss is :  1.335483\n",
      "Iteration # 879  loss is :  1.3354474\n",
      "Iteration # 880  loss is :  1.3354118\n",
      "Iteration # 881  loss is :  1.3353763\n",
      "Iteration # 882  loss is :  1.3353411\n",
      "Iteration # 883  loss is :  1.3353062\n",
      "Iteration # 884  loss is :  1.335271\n",
      "Iteration # 885  loss is :  1.3352364\n",
      "Iteration # 886  loss is :  1.3352017\n",
      "Iteration # 887  loss is :  1.3351672\n",
      "Iteration # 888  loss is :  1.3351328\n",
      "Iteration # 889  loss is :  1.3350984\n",
      "Iteration # 890  loss is :  1.3350645\n",
      "Iteration # 891  loss is :  1.3350303\n",
      "Iteration # 892  loss is :  1.3349965\n",
      "Iteration # 893  loss is :  1.3349628\n",
      "Iteration # 894  loss is :  1.3349293\n",
      "Iteration # 895  loss is :  1.3348957\n",
      "Iteration # 896  loss is :  1.3348625\n",
      "Iteration # 897  loss is :  1.3348292\n",
      "Iteration # 898  loss is :  1.3347963\n",
      "Iteration # 899  loss is :  1.3347633\n",
      "Iteration # 900  loss is :  1.3347306\n",
      "Iteration # 901  loss is :  1.3346978\n",
      "Iteration # 902  loss is :  1.3346654\n",
      "Iteration # 903  loss is :  1.3346329\n",
      "Iteration # 904  loss is :  1.3346007\n",
      "Iteration # 905  loss is :  1.3345686\n",
      "Iteration # 906  loss is :  1.3345366\n",
      "Iteration # 907  loss is :  1.3345045\n",
      "Iteration # 908  loss is :  1.3344729\n",
      "Iteration # 909  loss is :  1.3344412\n",
      "Iteration # 910  loss is :  1.3344096\n",
      "Iteration # 911  loss is :  1.3343782\n",
      "Iteration # 912  loss is :  1.3343469\n",
      "Iteration # 913  loss is :  1.3343158\n",
      "Iteration # 914  loss is :  1.3342848\n",
      "Iteration # 915  loss is :  1.3342539\n",
      "Iteration # 916  loss is :  1.334223\n",
      "Iteration # 917  loss is :  1.3341924\n",
      "Iteration # 918  loss is :  1.3341618\n",
      "Iteration # 919  loss is :  1.3341314\n",
      "Iteration # 920  loss is :  1.3341011\n",
      "Iteration # 921  loss is :  1.3340708\n",
      "Iteration # 922  loss is :  1.3340406\n",
      "Iteration # 923  loss is :  1.3340106\n",
      "Iteration # 924  loss is :  1.3339809\n",
      "Iteration # 925  loss is :  1.333951\n",
      "Iteration # 926  loss is :  1.3339214\n",
      "Iteration # 927  loss is :  1.3338919\n",
      "Iteration # 928  loss is :  1.3338624\n",
      "Iteration # 929  loss is :  1.3338331\n",
      "Iteration # 930  loss is :  1.333804\n",
      "Iteration # 931  loss is :  1.3337748\n",
      "Iteration # 932  loss is :  1.333746\n",
      "Iteration # 933  loss is :  1.333717\n",
      "Iteration # 934  loss is :  1.3336881\n",
      "Iteration # 935  loss is :  1.3336596\n",
      "Iteration # 936  loss is :  1.333631\n",
      "Iteration # 937  loss is :  1.3336025\n",
      "Iteration # 938  loss is :  1.3335742\n",
      "Iteration # 939  loss is :  1.333546\n",
      "Iteration # 940  loss is :  1.3335179\n",
      "Iteration # 941  loss is :  1.3334899\n",
      "Iteration # 942  loss is :  1.333462\n",
      "Iteration # 943  loss is :  1.3334341\n",
      "Iteration # 944  loss is :  1.3334064\n",
      "Iteration # 945  loss is :  1.3333788\n",
      "Iteration # 946  loss is :  1.3333514\n",
      "Iteration # 947  loss is :  1.333324\n",
      "Iteration # 948  loss is :  1.3332968\n",
      "Iteration # 949  loss is :  1.3332695\n",
      "Iteration # 950  loss is :  1.3332423\n",
      "Iteration # 951  loss is :  1.3332154\n",
      "Iteration # 952  loss is :  1.3331884\n",
      "Iteration # 953  loss is :  1.3331615\n",
      "Iteration # 954  loss is :  1.3331349\n",
      "Iteration # 955  loss is :  1.3331082\n",
      "Iteration # 956  loss is :  1.3330818\n",
      "Iteration # 957  loss is :  1.3330554\n",
      "Iteration # 958  loss is :  1.333029\n",
      "Iteration # 959  loss is :  1.3330028\n",
      "Iteration # 960  loss is :  1.3329767\n",
      "Iteration # 961  loss is :  1.3329506\n",
      "Iteration # 962  loss is :  1.3329247\n",
      "Iteration # 963  loss is :  1.3328987\n",
      "Iteration # 964  loss is :  1.3328731\n",
      "Iteration # 965  loss is :  1.3328474\n",
      "Iteration # 966  loss is :  1.3328218\n",
      "Iteration # 967  loss is :  1.3327963\n",
      "Iteration # 968  loss is :  1.332771\n",
      "Iteration # 969  loss is :  1.3327457\n",
      "Iteration # 970  loss is :  1.3327204\n",
      "Iteration # 971  loss is :  1.3326954\n",
      "Iteration # 972  loss is :  1.3326703\n",
      "Iteration # 973  loss is :  1.3326453\n",
      "Iteration # 974  loss is :  1.3326205\n",
      "Iteration # 975  loss is :  1.3325957\n",
      "Iteration # 976  loss is :  1.3325711\n",
      "Iteration # 977  loss is :  1.3325465\n",
      "Iteration # 978  loss is :  1.3325219\n",
      "Iteration # 979  loss is :  1.3324976\n",
      "Iteration # 980  loss is :  1.3324732\n",
      "Iteration # 981  loss is :  1.332449\n",
      "Iteration # 982  loss is :  1.3324249\n",
      "Iteration # 983  loss is :  1.3324008\n",
      "Iteration # 984  loss is :  1.3323768\n",
      "Iteration # 985  loss is :  1.3323529\n",
      "Iteration # 986  loss is :  1.3323292\n",
      "Iteration # 987  loss is :  1.3323053\n",
      "Iteration # 988  loss is :  1.3322817\n",
      "Iteration # 989  loss is :  1.332258\n",
      "Iteration # 990  loss is :  1.3322345\n",
      "Iteration # 991  loss is :  1.3322111\n",
      "Iteration # 992  loss is :  1.3321879\n",
      "Iteration # 993  loss is :  1.3321645\n",
      "Iteration # 994  loss is :  1.3321414\n",
      "Iteration # 995  loss is :  1.3321183\n",
      "Iteration # 996  loss is :  1.3320953\n",
      "Iteration # 997  loss is :  1.3320723\n",
      "Iteration # 998  loss is :  1.3320494\n",
      "Iteration # 999  loss is :  1.3320266\n",
      "Iteration # 1000  loss is :  1.332004\n",
      "Iteration # 1001  loss is :  1.3319813\n",
      "Iteration # 1002  loss is :  1.3319589\n",
      "Iteration # 1003  loss is :  1.3319362\n",
      "Iteration # 1004  loss is :  1.331914\n",
      "Iteration # 1005  loss is :  1.3318917\n",
      "Iteration # 1006  loss is :  1.3318695\n",
      "Iteration # 1007  loss is :  1.3318472\n",
      "Iteration # 1008  loss is :  1.3318251\n",
      "Iteration # 1009  loss is :  1.3318032\n",
      "Iteration # 1010  loss is :  1.3317813\n",
      "Iteration # 1011  loss is :  1.3317595\n",
      "Iteration # 1012  loss is :  1.3317376\n",
      "Iteration # 1013  loss is :  1.3317158\n",
      "Iteration # 1014  loss is :  1.3316942\n",
      "Iteration # 1015  loss is :  1.3316725\n",
      "Iteration # 1016  loss is :  1.3316511\n",
      "Iteration # 1017  loss is :  1.3316298\n",
      "Iteration # 1018  loss is :  1.3316084\n",
      "Iteration # 1019  loss is :  1.331587\n",
      "Iteration # 1020  loss is :  1.3315659\n",
      "Iteration # 1021  loss is :  1.3315448\n",
      "Iteration # 1022  loss is :  1.3315235\n",
      "Iteration # 1023  loss is :  1.3315027\n",
      "Iteration # 1024  loss is :  1.3314818\n",
      "Iteration # 1025  loss is :  1.331461\n",
      "Iteration # 1026  loss is :  1.3314401\n",
      "Iteration # 1027  loss is :  1.3314195\n",
      "Iteration # 1028  loss is :  1.3313988\n",
      "Iteration # 1029  loss is :  1.331378\n",
      "Iteration # 1030  loss is :  1.3313575\n",
      "Iteration # 1031  loss is :  1.3313371\n",
      "Iteration # 1032  loss is :  1.3313168\n",
      "Iteration # 1033  loss is :  1.3312963\n",
      "Iteration # 1034  loss is :  1.3312762\n",
      "Iteration # 1035  loss is :  1.3312559\n",
      "Iteration # 1036  loss is :  1.3312358\n",
      "Iteration # 1037  loss is :  1.3312157\n",
      "Iteration # 1038  loss is :  1.3311957\n",
      "Iteration # 1039  loss is :  1.3311758\n",
      "Iteration # 1040  loss is :  1.331156\n",
      "Iteration # 1041  loss is :  1.3311361\n",
      "Iteration # 1042  loss is :  1.3311164\n",
      "Iteration # 1043  loss is :  1.3310968\n",
      "Iteration # 1044  loss is :  1.3310771\n",
      "Iteration # 1045  loss is :  1.3310575\n",
      "Iteration # 1046  loss is :  1.3310381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 1047  loss is :  1.3310187\n",
      "Iteration # 1048  loss is :  1.3309994\n",
      "Iteration # 1049  loss is :  1.33098\n",
      "Iteration # 1050  loss is :  1.3309609\n",
      "Iteration # 1051  loss is :  1.3309417\n",
      "Iteration # 1052  loss is :  1.3309226\n",
      "Iteration # 1053  loss is :  1.3309034\n",
      "Iteration # 1054  loss is :  1.3308846\n",
      "Iteration # 1055  loss is :  1.3308656\n",
      "Iteration # 1056  loss is :  1.3308467\n",
      "Iteration # 1057  loss is :  1.3308278\n",
      "Iteration # 1058  loss is :  1.3308091\n",
      "Iteration # 1059  loss is :  1.3307905\n",
      "Iteration # 1060  loss is :  1.3307718\n",
      "Iteration # 1061  loss is :  1.3307532\n",
      "Iteration # 1062  loss is :  1.3307348\n",
      "Iteration # 1063  loss is :  1.3307163\n",
      "Iteration # 1064  loss is :  1.3306979\n",
      "Iteration # 1065  loss is :  1.3306797\n",
      "Iteration # 1066  loss is :  1.3306612\n",
      "Iteration # 1067  loss is :  1.3306432\n",
      "Iteration # 1068  loss is :  1.3306248\n",
      "Iteration # 1069  loss is :  1.3306068\n",
      "Iteration # 1070  loss is :  1.3305887\n",
      "Iteration # 1071  loss is :  1.3305708\n",
      "Iteration # 1072  loss is :  1.3305529\n",
      "Iteration # 1073  loss is :  1.3305349\n",
      "Iteration # 1074  loss is :  1.330517\n",
      "Iteration # 1075  loss is :  1.3304993\n",
      "Iteration # 1076  loss is :  1.3304816\n",
      "Iteration # 1077  loss is :  1.330464\n",
      "Iteration # 1078  loss is :  1.3304465\n",
      "Iteration # 1079  loss is :  1.3304288\n",
      "Iteration # 1080  loss is :  1.3304113\n",
      "Iteration # 1081  loss is :  1.3303938\n",
      "Iteration # 1082  loss is :  1.3303765\n",
      "Iteration # 1083  loss is :  1.3303591\n",
      "Iteration # 1084  loss is :  1.330342\n",
      "Iteration # 1085  loss is :  1.3303246\n",
      "Iteration # 1086  loss is :  1.3303075\n",
      "Iteration # 1087  loss is :  1.3302904\n",
      "Iteration # 1088  loss is :  1.3302734\n",
      "Iteration # 1089  loss is :  1.3302562\n",
      "Iteration # 1090  loss is :  1.3302393\n",
      "Iteration # 1091  loss is :  1.3302222\n",
      "Iteration # 1092  loss is :  1.3302056\n",
      "Iteration # 1093  loss is :  1.3301888\n",
      "Iteration # 1094  loss is :  1.330172\n",
      "Iteration # 1095  loss is :  1.3301551\n",
      "Iteration # 1096  loss is :  1.3301384\n",
      "Iteration # 1097  loss is :  1.3301218\n",
      "Iteration # 1098  loss is :  1.3301053\n",
      "Iteration # 1099  loss is :  1.3300887\n",
      "Iteration # 1100  loss is :  1.3300724\n",
      "Iteration # 1101  loss is :  1.330056\n",
      "Iteration # 1102  loss is :  1.3300396\n",
      "Iteration # 1103  loss is :  1.3300234\n",
      "Iteration # 1104  loss is :  1.330007\n",
      "Iteration # 1105  loss is :  1.3299909\n",
      "Iteration # 1106  loss is :  1.3299745\n",
      "Iteration # 1107  loss is :  1.3299584\n",
      "Iteration # 1108  loss is :  1.3299423\n",
      "Iteration # 1109  loss is :  1.3299265\n",
      "Iteration # 1110  loss is :  1.3299104\n",
      "Iteration # 1111  loss is :  1.3298945\n",
      "Iteration # 1112  loss is :  1.3298787\n",
      "Iteration # 1113  loss is :  1.3298628\n",
      "Iteration # 1114  loss is :  1.329847\n",
      "Iteration # 1115  loss is :  1.3298312\n",
      "Iteration # 1116  loss is :  1.3298155\n",
      "Iteration # 1117  loss is :  1.3297999\n",
      "Iteration # 1118  loss is :  1.3297843\n",
      "Iteration # 1119  loss is :  1.3297687\n",
      "Iteration # 1120  loss is :  1.3297532\n",
      "Iteration # 1121  loss is :  1.3297377\n",
      "Iteration # 1122  loss is :  1.3297222\n",
      "Iteration # 1123  loss is :  1.329707\n",
      "Iteration # 1124  loss is :  1.3296916\n",
      "Iteration # 1125  loss is :  1.3296764\n",
      "Iteration # 1126  loss is :  1.329661\n",
      "Iteration # 1127  loss is :  1.329646\n",
      "Iteration # 1128  loss is :  1.3296309\n",
      "Iteration # 1129  loss is :  1.3296155\n",
      "Iteration # 1130  loss is :  1.3296006\n",
      "Iteration # 1131  loss is :  1.3295854\n",
      "Iteration # 1132  loss is :  1.3295705\n",
      "Iteration # 1133  loss is :  1.3295555\n",
      "Iteration # 1134  loss is :  1.3295407\n",
      "Iteration # 1135  loss is :  1.329526\n",
      "Iteration # 1136  loss is :  1.329511\n",
      "Iteration # 1137  loss is :  1.3294961\n",
      "Iteration # 1138  loss is :  1.3294816\n",
      "Iteration # 1139  loss is :  1.3294668\n",
      "Iteration # 1140  loss is :  1.3294522\n",
      "Iteration # 1141  loss is :  1.3294376\n",
      "Iteration # 1142  loss is :  1.3294231\n",
      "Iteration # 1143  loss is :  1.3294086\n",
      "Iteration # 1144  loss is :  1.3293941\n",
      "Iteration # 1145  loss is :  1.3293797\n",
      "Iteration # 1146  loss is :  1.3293654\n",
      "Iteration # 1147  loss is :  1.3293508\n",
      "Iteration # 1148  loss is :  1.3293366\n",
      "Iteration # 1149  loss is :  1.3293223\n",
      "Iteration # 1150  loss is :  1.3293082\n",
      "Iteration # 1151  loss is :  1.3292938\n",
      "Iteration # 1152  loss is :  1.3292799\n",
      "Iteration # 1153  loss is :  1.3292657\n",
      "Iteration # 1154  loss is :  1.3292516\n",
      "Iteration # 1155  loss is :  1.3292376\n",
      "Iteration # 1156  loss is :  1.3292238\n",
      "Iteration # 1157  loss is :  1.3292096\n",
      "Iteration # 1158  loss is :  1.3291959\n",
      "Iteration # 1159  loss is :  1.3291818\n",
      "Iteration # 1160  loss is :  1.3291681\n",
      "Iteration # 1161  loss is :  1.3291543\n",
      "Iteration # 1162  loss is :  1.3291405\n",
      "Iteration # 1163  loss is :  1.3291268\n",
      "Iteration # 1164  loss is :  1.3291131\n",
      "Iteration # 1165  loss is :  1.3290994\n",
      "Iteration # 1166  loss is :  1.3290858\n",
      "Iteration # 1167  loss is :  1.3290722\n",
      "Iteration # 1168  loss is :  1.3290588\n",
      "Iteration # 1169  loss is :  1.3290453\n",
      "Iteration # 1170  loss is :  1.3290318\n",
      "Iteration # 1171  loss is :  1.3290185\n",
      "Iteration # 1172  loss is :  1.3290051\n",
      "Iteration # 1173  loss is :  1.3289918\n",
      "Iteration # 1174  loss is :  1.3289784\n",
      "Iteration # 1175  loss is :  1.3289652\n",
      "Iteration # 1176  loss is :  1.3289518\n",
      "Iteration # 1177  loss is :  1.3289387\n",
      "Iteration # 1178  loss is :  1.3289256\n",
      "Iteration # 1179  loss is :  1.3289124\n",
      "Iteration # 1180  loss is :  1.3288994\n",
      "Iteration # 1181  loss is :  1.3288864\n",
      "Iteration # 1182  loss is :  1.3288734\n",
      "Iteration # 1183  loss is :  1.3288603\n",
      "Iteration # 1184  loss is :  1.3288474\n",
      "Iteration # 1185  loss is :  1.3288344\n",
      "Iteration # 1186  loss is :  1.3288217\n",
      "Iteration # 1187  loss is :  1.3288088\n",
      "Iteration # 1188  loss is :  1.3287958\n",
      "Iteration # 1189  loss is :  1.3287832\n",
      "Iteration # 1190  loss is :  1.3287705\n",
      "Iteration # 1191  loss is :  1.3287578\n",
      "Iteration # 1192  loss is :  1.328745\n",
      "Iteration # 1193  loss is :  1.3287325\n",
      "Iteration # 1194  loss is :  1.3287199\n",
      "Iteration # 1195  loss is :  1.3287073\n",
      "Iteration # 1196  loss is :  1.3286947\n",
      "Iteration # 1197  loss is :  1.3286823\n",
      "Iteration # 1198  loss is :  1.3286697\n",
      "Iteration # 1199  loss is :  1.3286573\n",
      "Iteration # 1200  loss is :  1.3286449\n",
      "Iteration # 1201  loss is :  1.3286327\n",
      "Iteration # 1202  loss is :  1.3286203\n",
      "Iteration # 1203  loss is :  1.328608\n",
      "Iteration # 1204  loss is :  1.3285956\n",
      "Iteration # 1205  loss is :  1.3285835\n",
      "Iteration # 1206  loss is :  1.3285713\n",
      "Iteration # 1207  loss is :  1.3285592\n",
      "Iteration # 1208  loss is :  1.3285471\n",
      "Iteration # 1209  loss is :  1.3285348\n",
      "Iteration # 1210  loss is :  1.3285227\n",
      "Iteration # 1211  loss is :  1.3285108\n",
      "Iteration # 1212  loss is :  1.3284987\n",
      "Iteration # 1213  loss is :  1.3284868\n",
      "Iteration # 1214  loss is :  1.328475\n",
      "Iteration # 1215  loss is :  1.328463\n",
      "Iteration # 1216  loss is :  1.328451\n",
      "Iteration # 1217  loss is :  1.3284391\n",
      "Iteration # 1218  loss is :  1.3284273\n",
      "Iteration # 1219  loss is :  1.3284158\n",
      "Iteration # 1220  loss is :  1.328404\n",
      "Iteration # 1221  loss is :  1.3283921\n",
      "Iteration # 1222  loss is :  1.3283805\n",
      "Iteration # 1223  loss is :  1.3283688\n",
      "Iteration # 1224  loss is :  1.3283571\n",
      "Iteration # 1225  loss is :  1.3283455\n",
      "Iteration # 1226  loss is :  1.3283339\n",
      "Iteration # 1227  loss is :  1.3283224\n",
      "Iteration # 1228  loss is :  1.328311\n",
      "Iteration # 1229  loss is :  1.3282995\n",
      "Iteration # 1230  loss is :  1.3282881\n",
      "Iteration # 1231  loss is :  1.3282766\n",
      "Iteration # 1232  loss is :  1.3282652\n",
      "Iteration # 1233  loss is :  1.3282537\n",
      "Iteration # 1234  loss is :  1.3282424\n",
      "Iteration # 1235  loss is :  1.3282311\n",
      "Iteration # 1236  loss is :  1.3282199\n",
      "Iteration # 1237  loss is :  1.3282086\n",
      "Iteration # 1238  loss is :  1.3281972\n",
      "Iteration # 1239  loss is :  1.3281863\n",
      "Iteration # 1240  loss is :  1.3281751\n",
      "Iteration # 1241  loss is :  1.3281639\n",
      "Iteration # 1242  loss is :  1.3281528\n",
      "Iteration # 1243  loss is :  1.3281417\n",
      "Iteration # 1244  loss is :  1.3281307\n",
      "Iteration # 1245  loss is :  1.3281196\n",
      "Iteration # 1246  loss is :  1.3281087\n",
      "Iteration # 1247  loss is :  1.3280976\n",
      "Iteration # 1248  loss is :  1.3280867\n",
      "Iteration # 1249  loss is :  1.3280759\n",
      "Iteration # 1250  loss is :  1.3280649\n",
      "Iteration # 1251  loss is :  1.3280541\n",
      "Iteration # 1252  loss is :  1.3280433\n",
      "Iteration # 1253  loss is :  1.3280325\n",
      "Iteration # 1254  loss is :  1.3280216\n",
      "Iteration # 1255  loss is :  1.3280109\n",
      "Iteration # 1256  loss is :  1.3280002\n",
      "Iteration # 1257  loss is :  1.3279896\n",
      "Iteration # 1258  loss is :  1.327979\n",
      "Iteration # 1259  loss is :  1.3279681\n",
      "Iteration # 1260  loss is :  1.3279576\n",
      "Iteration # 1261  loss is :  1.327947\n",
      "Iteration # 1262  loss is :  1.3279365\n",
      "Iteration # 1263  loss is :  1.3279258\n",
      "Iteration # 1264  loss is :  1.3279154\n",
      "Iteration # 1265  loss is :  1.3279048\n",
      "Iteration # 1266  loss is :  1.3278946\n",
      "Iteration # 1267  loss is :  1.3278841\n",
      "Iteration # 1268  loss is :  1.3278738\n",
      "Iteration # 1269  loss is :  1.3278633\n",
      "Iteration # 1270  loss is :  1.327853\n",
      "Iteration # 1271  loss is :  1.3278426\n",
      "Iteration # 1272  loss is :  1.3278323\n",
      "Iteration # 1273  loss is :  1.3278221\n",
      "Iteration # 1274  loss is :  1.3278117\n",
      "Iteration # 1275  loss is :  1.3278017\n",
      "Iteration # 1276  loss is :  1.3277915\n",
      "Iteration # 1277  loss is :  1.3277811\n",
      "Iteration # 1278  loss is :  1.3277712\n",
      "Iteration # 1279  loss is :  1.327761\n",
      "Iteration # 1280  loss is :  1.3277509\n",
      "Iteration # 1281  loss is :  1.3277407\n",
      "Iteration # 1282  loss is :  1.3277307\n",
      "Iteration # 1283  loss is :  1.3277208\n",
      "Iteration # 1284  loss is :  1.3277107\n",
      "Iteration # 1285  loss is :  1.3277007\n",
      "Iteration # 1286  loss is :  1.3276908\n",
      "Iteration # 1287  loss is :  1.3276807\n",
      "Iteration # 1288  loss is :  1.3276709\n",
      "Iteration # 1289  loss is :  1.3276612\n",
      "Iteration # 1290  loss is :  1.327651\n",
      "Iteration # 1291  loss is :  1.3276415\n",
      "Iteration # 1292  loss is :  1.3276316\n",
      "Iteration # 1293  loss is :  1.3276218\n",
      "Iteration # 1294  loss is :  1.327612\n",
      "Iteration # 1295  loss is :  1.3276024\n",
      "Iteration # 1296  loss is :  1.3275926\n",
      "Iteration # 1297  loss is :  1.3275828\n",
      "Iteration # 1298  loss is :  1.3275733\n",
      "Iteration # 1299  loss is :  1.3275636\n",
      "Iteration # 1300  loss is :  1.327554\n",
      "Iteration # 1301  loss is :  1.3275445\n",
      "Iteration # 1302  loss is :  1.3275349\n",
      "Iteration # 1303  loss is :  1.3275251\n",
      "Iteration # 1304  loss is :  1.3275157\n",
      "Iteration # 1305  loss is :  1.3275062\n",
      "Iteration # 1306  loss is :  1.3274968\n",
      "Iteration # 1307  loss is :  1.3274872\n",
      "Iteration # 1308  loss is :  1.3274778\n",
      "Iteration # 1309  loss is :  1.3274684\n",
      "Iteration # 1310  loss is :  1.3274591\n",
      "Iteration # 1311  loss is :  1.3274497\n",
      "Iteration # 1312  loss is :  1.3274404\n",
      "Iteration # 1313  loss is :  1.3274311\n",
      "Iteration # 1314  loss is :  1.3274218\n",
      "Iteration # 1315  loss is :  1.3274126\n",
      "Iteration # 1316  loss is :  1.3274032\n",
      "Iteration # 1317  loss is :  1.3273939\n",
      "Iteration # 1318  loss is :  1.3273847\n",
      "Iteration # 1319  loss is :  1.3273755\n",
      "Iteration # 1320  loss is :  1.3273665\n",
      "Iteration # 1321  loss is :  1.3273572\n",
      "Iteration # 1322  loss is :  1.3273481\n",
      "Iteration # 1323  loss is :  1.3273389\n",
      "Iteration # 1324  loss is :  1.3273301\n",
      "Iteration # 1325  loss is :  1.3273208\n",
      "Iteration # 1326  loss is :  1.3273118\n",
      "Iteration # 1327  loss is :  1.3273028\n",
      "Iteration # 1328  loss is :  1.3272939\n",
      "Iteration # 1329  loss is :  1.3272848\n",
      "Iteration # 1330  loss is :  1.3272759\n",
      "Iteration # 1331  loss is :  1.3272669\n",
      "Iteration # 1332  loss is :  1.327258\n",
      "Iteration # 1333  loss is :  1.327249\n",
      "Iteration # 1334  loss is :  1.3272402\n",
      "Iteration # 1335  loss is :  1.3272315\n",
      "Iteration # 1336  loss is :  1.3272225\n",
      "Iteration # 1337  loss is :  1.3272138\n",
      "Iteration # 1338  loss is :  1.327205\n",
      "Iteration # 1339  loss is :  1.3271962\n",
      "Iteration # 1340  loss is :  1.3271874\n",
      "Iteration # 1341  loss is :  1.3271786\n",
      "Iteration # 1342  loss is :  1.3271699\n",
      "Iteration # 1343  loss is :  1.3271611\n",
      "Iteration # 1344  loss is :  1.3271526\n",
      "Iteration # 1345  loss is :  1.3271438\n",
      "Iteration # 1346  loss is :  1.3271353\n",
      "Iteration # 1347  loss is :  1.3271266\n",
      "Iteration # 1348  loss is :  1.327118\n",
      "Iteration # 1349  loss is :  1.3271095\n",
      "Iteration # 1350  loss is :  1.327101\n",
      "Iteration # 1351  loss is :  1.3270924\n",
      "Iteration # 1352  loss is :  1.327084\n",
      "Iteration # 1353  loss is :  1.3270755\n",
      "Iteration # 1354  loss is :  1.327067\n",
      "Iteration # 1355  loss is :  1.3270586\n",
      "Iteration # 1356  loss is :  1.3270502\n",
      "Iteration # 1357  loss is :  1.3270416\n",
      "Iteration # 1358  loss is :  1.3270332\n",
      "Iteration # 1359  loss is :  1.3270249\n",
      "Iteration # 1360  loss is :  1.3270165\n",
      "Iteration # 1361  loss is :  1.3270082\n",
      "Iteration # 1362  loss is :  1.3269999\n",
      "Iteration # 1363  loss is :  1.3269914\n",
      "Iteration # 1364  loss is :  1.3269833\n",
      "Iteration # 1365  loss is :  1.326975\n",
      "Iteration # 1366  loss is :  1.3269668\n",
      "Iteration # 1367  loss is :  1.3269585\n",
      "Iteration # 1368  loss is :  1.3269503\n",
      "Iteration # 1369  loss is :  1.326942\n",
      "Iteration # 1370  loss is :  1.326934\n",
      "Iteration # 1371  loss is :  1.3269258\n",
      "Iteration # 1372  loss is :  1.3269178\n",
      "Iteration # 1373  loss is :  1.3269094\n",
      "Iteration # 1374  loss is :  1.3269014\n",
      "Iteration # 1375  loss is :  1.3268932\n",
      "Iteration # 1376  loss is :  1.3268852\n",
      "Iteration # 1377  loss is :  1.3268771\n",
      "Iteration # 1378  loss is :  1.3268691\n",
      "Iteration # 1379  loss is :  1.3268613\n",
      "Iteration # 1380  loss is :  1.3268532\n",
      "Iteration # 1381  loss is :  1.3268452\n",
      "Iteration # 1382  loss is :  1.3268372\n",
      "Iteration # 1383  loss is :  1.3268292\n",
      "Iteration # 1384  loss is :  1.3268213\n",
      "Iteration # 1385  loss is :  1.3268135\n",
      "Iteration # 1386  loss is :  1.3268057\n",
      "Iteration # 1387  loss is :  1.3267977\n",
      "Iteration # 1388  loss is :  1.3267899\n",
      "Iteration # 1389  loss is :  1.3267821\n",
      "Iteration # 1390  loss is :  1.3267742\n",
      "Iteration # 1391  loss is :  1.3267664\n",
      "Iteration # 1392  loss is :  1.3267586\n",
      "Iteration # 1393  loss is :  1.3267508\n",
      "Iteration # 1394  loss is :  1.3267431\n",
      "Iteration # 1395  loss is :  1.3267354\n",
      "Iteration # 1396  loss is :  1.3267276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 1397  loss is :  1.3267199\n",
      "Iteration # 1398  loss is :  1.3267124\n",
      "Iteration # 1399  loss is :  1.3267046\n",
      "Iteration # 1400  loss is :  1.3266971\n",
      "Iteration # 1401  loss is :  1.3266894\n",
      "Iteration # 1402  loss is :  1.3266817\n",
      "Iteration # 1403  loss is :  1.3266741\n",
      "Iteration # 1404  loss is :  1.3266665\n",
      "Iteration # 1405  loss is :  1.326659\n",
      "Iteration # 1406  loss is :  1.3266516\n",
      "Iteration # 1407  loss is :  1.326644\n",
      "Iteration # 1408  loss is :  1.3266363\n",
      "Iteration # 1409  loss is :  1.3266288\n",
      "Iteration # 1410  loss is :  1.3266213\n",
      "Iteration # 1411  loss is :  1.3266138\n",
      "Iteration # 1412  loss is :  1.3266065\n",
      "Iteration # 1413  loss is :  1.326599\n",
      "Iteration # 1414  loss is :  1.3265916\n",
      "Iteration # 1415  loss is :  1.3265842\n",
      "Iteration # 1416  loss is :  1.3265768\n",
      "Iteration # 1417  loss is :  1.3265694\n",
      "Iteration # 1418  loss is :  1.3265619\n",
      "Iteration # 1419  loss is :  1.3265548\n",
      "Iteration # 1420  loss is :  1.3265474\n",
      "Iteration # 1421  loss is :  1.3265401\n",
      "Iteration # 1422  loss is :  1.326533\n",
      "Iteration # 1423  loss is :  1.3265254\n",
      "Iteration # 1424  loss is :  1.3265183\n",
      "Iteration # 1425  loss is :  1.3265109\n",
      "Iteration # 1426  loss is :  1.3265039\n",
      "Iteration # 1427  loss is :  1.3264967\n",
      "Iteration # 1428  loss is :  1.3264894\n",
      "Iteration # 1429  loss is :  1.3264822\n",
      "Iteration # 1430  loss is :  1.326475\n",
      "Iteration # 1431  loss is :  1.3264679\n",
      "Iteration # 1432  loss is :  1.3264608\n",
      "Iteration # 1433  loss is :  1.3264536\n",
      "Iteration # 1434  loss is :  1.3264465\n",
      "Iteration # 1435  loss is :  1.3264394\n",
      "Iteration # 1436  loss is :  1.3264322\n",
      "Iteration # 1437  loss is :  1.3264253\n",
      "Iteration # 1438  loss is :  1.3264183\n",
      "Iteration # 1439  loss is :  1.3264111\n",
      "Iteration # 1440  loss is :  1.3264042\n",
      "Iteration # 1441  loss is :  1.3263972\n",
      "Iteration # 1442  loss is :  1.3263901\n",
      "Iteration # 1443  loss is :  1.3263831\n",
      "Iteration # 1444  loss is :  1.3263762\n",
      "Iteration # 1445  loss is :  1.3263693\n",
      "Iteration # 1446  loss is :  1.3263621\n",
      "Iteration # 1447  loss is :  1.3263555\n",
      "Iteration # 1448  loss is :  1.3263484\n",
      "Iteration # 1449  loss is :  1.3263417\n",
      "Iteration # 1450  loss is :  1.3263347\n",
      "Iteration # 1451  loss is :  1.3263278\n",
      "Iteration # 1452  loss is :  1.326321\n",
      "Iteration # 1453  loss is :  1.3263142\n",
      "Iteration # 1454  loss is :  1.3263073\n",
      "Iteration # 1455  loss is :  1.3263006\n",
      "Iteration # 1456  loss is :  1.3262938\n",
      "Iteration # 1457  loss is :  1.3262872\n",
      "Iteration # 1458  loss is :  1.3262804\n",
      "Iteration # 1459  loss is :  1.3262736\n",
      "Iteration # 1460  loss is :  1.3262666\n",
      "Iteration # 1461  loss is :  1.3262602\n",
      "Iteration # 1462  loss is :  1.3262532\n",
      "Iteration # 1463  loss is :  1.3262467\n",
      "Iteration # 1464  loss is :  1.32624\n",
      "Iteration # 1465  loss is :  1.3262333\n",
      "Iteration # 1466  loss is :  1.3262266\n",
      "Iteration # 1467  loss is :  1.32622\n",
      "Iteration # 1468  loss is :  1.3262134\n",
      "Iteration # 1469  loss is :  1.3262067\n",
      "Iteration # 1470  loss is :  1.3262\n",
      "Iteration # 1471  loss is :  1.3261937\n",
      "Iteration # 1472  loss is :  1.3261871\n",
      "Iteration # 1473  loss is :  1.3261805\n",
      "Iteration # 1474  loss is :  1.3261739\n",
      "Iteration # 1475  loss is :  1.3261673\n",
      "Iteration # 1476  loss is :  1.3261609\n",
      "Iteration # 1477  loss is :  1.3261544\n",
      "Iteration # 1478  loss is :  1.3261479\n",
      "Iteration # 1479  loss is :  1.3261414\n",
      "Iteration # 1480  loss is :  1.3261349\n",
      "Iteration # 1481  loss is :  1.3261284\n",
      "Iteration # 1482  loss is :  1.326122\n",
      "Iteration # 1483  loss is :  1.3261156\n",
      "Iteration # 1484  loss is :  1.3261092\n",
      "Iteration # 1485  loss is :  1.3261027\n",
      "Iteration # 1486  loss is :  1.3260965\n",
      "Iteration # 1487  loss is :  1.3260901\n",
      "Iteration # 1488  loss is :  1.3260837\n",
      "Iteration # 1489  loss is :  1.3260773\n",
      "Iteration # 1490  loss is :  1.326071\n",
      "Iteration # 1491  loss is :  1.3260646\n",
      "Iteration # 1492  loss is :  1.3260584\n",
      "Iteration # 1493  loss is :  1.3260522\n",
      "Iteration # 1494  loss is :  1.3260459\n",
      "Iteration # 1495  loss is :  1.3260394\n",
      "Iteration # 1496  loss is :  1.3260332\n",
      "Iteration # 1497  loss is :  1.326027\n",
      "Iteration # 1498  loss is :  1.3260207\n",
      "Iteration # 1499  loss is :  1.3260145\n",
      "Iteration # 1500  loss is :  1.3260083\n",
      "Iteration # 1501  loss is :  1.3260021\n",
      "Iteration # 1502  loss is :  1.3259959\n",
      "Iteration # 1503  loss is :  1.3259897\n",
      "Iteration # 1504  loss is :  1.3259838\n",
      "Iteration # 1505  loss is :  1.3259774\n",
      "Iteration # 1506  loss is :  1.3259712\n",
      "Iteration # 1507  loss is :  1.3259653\n",
      "Iteration # 1508  loss is :  1.3259591\n",
      "Iteration # 1509  loss is :  1.325953\n",
      "Iteration # 1510  loss is :  1.3259469\n",
      "Iteration # 1511  loss is :  1.3259408\n",
      "Iteration # 1512  loss is :  1.3259346\n",
      "Iteration # 1513  loss is :  1.3259287\n",
      "Iteration # 1514  loss is :  1.3259227\n",
      "Iteration # 1515  loss is :  1.3259165\n",
      "Iteration # 1516  loss is :  1.3259108\n",
      "Iteration # 1517  loss is :  1.3259046\n",
      "Iteration # 1518  loss is :  1.3258986\n",
      "Iteration # 1519  loss is :  1.3258927\n",
      "Iteration # 1520  loss is :  1.3258867\n",
      "Iteration # 1521  loss is :  1.3258808\n",
      "Iteration # 1522  loss is :  1.3258748\n",
      "Iteration # 1523  loss is :  1.3258688\n",
      "Iteration # 1524  loss is :  1.3258629\n",
      "Iteration # 1525  loss is :  1.3258572\n",
      "Iteration # 1526  loss is :  1.3258512\n",
      "Iteration # 1527  loss is :  1.3258454\n",
      "Iteration # 1528  loss is :  1.3258393\n",
      "Iteration # 1529  loss is :  1.3258334\n",
      "Iteration # 1530  loss is :  1.3258276\n",
      "Iteration # 1531  loss is :  1.3258219\n",
      "Iteration # 1532  loss is :  1.3258159\n",
      "Iteration # 1533  loss is :  1.3258102\n",
      "Iteration # 1534  loss is :  1.3258045\n",
      "Iteration # 1535  loss is :  1.3257985\n",
      "Iteration # 1536  loss is :  1.3257928\n",
      "Iteration # 1537  loss is :  1.3257871\n",
      "Iteration # 1538  loss is :  1.3257812\n",
      "Iteration # 1539  loss is :  1.3257755\n",
      "Iteration # 1540  loss is :  1.3257698\n",
      "Iteration # 1541  loss is :  1.3257641\n",
      "Iteration # 1542  loss is :  1.3257583\n",
      "Iteration # 1543  loss is :  1.3257526\n",
      "Iteration # 1544  loss is :  1.3257469\n",
      "Iteration # 1545  loss is :  1.3257412\n",
      "Iteration # 1546  loss is :  1.3257356\n",
      "Iteration # 1547  loss is :  1.3257298\n",
      "Iteration # 1548  loss is :  1.3257244\n",
      "Iteration # 1549  loss is :  1.3257186\n",
      "Iteration # 1550  loss is :  1.3257129\n",
      "Iteration # 1551  loss is :  1.3257074\n",
      "Iteration # 1552  loss is :  1.3257017\n",
      "Iteration # 1553  loss is :  1.3256961\n",
      "Iteration # 1554  loss is :  1.3256905\n",
      "Iteration # 1555  loss is :  1.3256849\n",
      "Iteration # 1556  loss is :  1.3256793\n",
      "Iteration # 1557  loss is :  1.3256739\n",
      "Iteration # 1558  loss is :  1.3256682\n",
      "Iteration # 1559  loss is :  1.3256627\n",
      "Iteration # 1560  loss is :  1.3256574\n",
      "Iteration # 1561  loss is :  1.3256516\n",
      "Iteration # 1562  loss is :  1.3256463\n",
      "Iteration # 1563  loss is :  1.3256407\n",
      "Iteration # 1564  loss is :  1.3256352\n",
      "Iteration # 1565  loss is :  1.3256297\n",
      "Iteration # 1566  loss is :  1.3256243\n",
      "Iteration # 1567  loss is :  1.3256189\n",
      "Iteration # 1568  loss is :  1.3256135\n",
      "Iteration # 1569  loss is :  1.325608\n",
      "Iteration # 1570  loss is :  1.3256025\n",
      "Iteration # 1571  loss is :  1.3255972\n",
      "Iteration # 1572  loss is :  1.3255916\n",
      "Iteration # 1573  loss is :  1.3255864\n",
      "Iteration # 1574  loss is :  1.325581\n",
      "Iteration # 1575  loss is :  1.3255756\n",
      "Iteration # 1576  loss is :  1.3255702\n",
      "Iteration # 1577  loss is :  1.3255649\n",
      "Iteration # 1578  loss is :  1.3255596\n",
      "Iteration # 1579  loss is :  1.3255543\n",
      "Iteration # 1580  loss is :  1.3255489\n",
      "Iteration # 1581  loss is :  1.3255435\n",
      "Iteration # 1582  loss is :  1.3255383\n",
      "Iteration # 1583  loss is :  1.325533\n",
      "Iteration # 1584  loss is :  1.3255278\n",
      "Iteration # 1585  loss is :  1.3255224\n",
      "Iteration # 1586  loss is :  1.3255172\n",
      "Iteration # 1587  loss is :  1.3255119\n",
      "Iteration # 1588  loss is :  1.3255067\n",
      "Iteration # 1589  loss is :  1.3255014\n",
      "Iteration # 1590  loss is :  1.3254962\n",
      "Iteration # 1591  loss is :  1.3254911\n",
      "Iteration # 1592  loss is :  1.325486\n",
      "Iteration # 1593  loss is :  1.3254806\n",
      "Iteration # 1594  loss is :  1.3254755\n",
      "Iteration # 1595  loss is :  1.3254703\n",
      "Iteration # 1596  loss is :  1.3254651\n",
      "Iteration # 1597  loss is :  1.3254598\n",
      "Iteration # 1598  loss is :  1.3254547\n",
      "Iteration # 1599  loss is :  1.3254498\n",
      "Iteration # 1600  loss is :  1.3254445\n",
      "Iteration # 1601  loss is :  1.3254393\n",
      "Iteration # 1602  loss is :  1.3254344\n",
      "Iteration # 1603  loss is :  1.3254292\n",
      "Iteration # 1604  loss is :  1.3254241\n",
      "Iteration # 1605  loss is :  1.3254191\n",
      "Iteration # 1606  loss is :  1.325414\n",
      "Iteration # 1607  loss is :  1.3254089\n",
      "Iteration # 1608  loss is :  1.325404\n",
      "Iteration # 1609  loss is :  1.3253988\n",
      "Iteration # 1610  loss is :  1.3253937\n",
      "Iteration # 1611  loss is :  1.3253887\n",
      "Iteration # 1612  loss is :  1.3253837\n",
      "Iteration # 1613  loss is :  1.3253787\n",
      "Iteration # 1614  loss is :  1.3253738\n",
      "Iteration # 1615  loss is :  1.3253688\n",
      "Iteration # 1616  loss is :  1.3253638\n",
      "Iteration # 1617  loss is :  1.3253587\n",
      "Iteration # 1618  loss is :  1.3253539\n",
      "Iteration # 1619  loss is :  1.3253489\n",
      "Iteration # 1620  loss is :  1.325344\n",
      "Iteration # 1621  loss is :  1.3253391\n",
      "Iteration # 1622  loss is :  1.3253341\n",
      "Iteration # 1623  loss is :  1.3253292\n",
      "Iteration # 1624  loss is :  1.3253244\n",
      "Iteration # 1625  loss is :  1.3253194\n",
      "Iteration # 1626  loss is :  1.3253144\n",
      "Iteration # 1627  loss is :  1.3253096\n",
      "Iteration # 1628  loss is :  1.3253047\n",
      "Iteration # 1629  loss is :  1.3252999\n",
      "Iteration # 1630  loss is :  1.3252951\n",
      "Iteration # 1631  loss is :  1.3252901\n",
      "Iteration # 1632  loss is :  1.3252854\n",
      "Iteration # 1633  loss is :  1.3252804\n",
      "Iteration # 1634  loss is :  1.3252757\n",
      "Iteration # 1635  loss is :  1.3252708\n",
      "Iteration # 1636  loss is :  1.325266\n",
      "Iteration # 1637  loss is :  1.3252612\n",
      "Iteration # 1638  loss is :  1.3252565\n",
      "Iteration # 1639  loss is :  1.3252517\n",
      "Iteration # 1640  loss is :  1.3252469\n",
      "Iteration # 1641  loss is :  1.325242\n",
      "Iteration # 1642  loss is :  1.3252373\n",
      "Iteration # 1643  loss is :  1.3252326\n",
      "Iteration # 1644  loss is :  1.325228\n",
      "Iteration # 1645  loss is :  1.3252232\n",
      "Iteration # 1646  loss is :  1.3252184\n",
      "Iteration # 1647  loss is :  1.3252137\n",
      "Iteration # 1648  loss is :  1.325209\n",
      "Iteration # 1649  loss is :  1.3252043\n",
      "Iteration # 1650  loss is :  1.3251997\n",
      "Iteration # 1651  loss is :  1.325195\n",
      "Iteration # 1652  loss is :  1.3251903\n",
      "Iteration # 1653  loss is :  1.3251857\n",
      "Iteration # 1654  loss is :  1.325181\n",
      "Iteration # 1655  loss is :  1.3251762\n",
      "Iteration # 1656  loss is :  1.3251717\n",
      "Iteration # 1657  loss is :  1.3251671\n",
      "Iteration # 1658  loss is :  1.3251624\n",
      "Iteration # 1659  loss is :  1.325158\n",
      "Iteration # 1660  loss is :  1.3251532\n",
      "Iteration # 1661  loss is :  1.3251486\n",
      "Iteration # 1662  loss is :  1.3251442\n",
      "Iteration # 1663  loss is :  1.3251394\n",
      "Iteration # 1664  loss is :  1.325135\n",
      "Iteration # 1665  loss is :  1.3251303\n",
      "Iteration # 1666  loss is :  1.3251258\n",
      "Iteration # 1667  loss is :  1.3251213\n",
      "Iteration # 1668  loss is :  1.3251166\n",
      "Iteration # 1669  loss is :  1.3251122\n",
      "Iteration # 1670  loss is :  1.3251077\n",
      "Iteration # 1671  loss is :  1.325103\n",
      "Iteration # 1672  loss is :  1.3250986\n",
      "Iteration # 1673  loss is :  1.3250941\n",
      "Iteration # 1674  loss is :  1.3250896\n",
      "Iteration # 1675  loss is :  1.3250852\n",
      "Iteration # 1676  loss is :  1.3250806\n",
      "Iteration # 1677  loss is :  1.3250761\n",
      "Iteration # 1678  loss is :  1.3250718\n",
      "Iteration # 1679  loss is :  1.3250673\n",
      "Iteration # 1680  loss is :  1.3250629\n",
      "Iteration # 1681  loss is :  1.3250585\n",
      "Iteration # 1682  loss is :  1.3250539\n",
      "Iteration # 1683  loss is :  1.3250495\n",
      "Iteration # 1684  loss is :  1.3250452\n",
      "Iteration # 1685  loss is :  1.3250407\n",
      "Iteration # 1686  loss is :  1.3250363\n",
      "Iteration # 1687  loss is :  1.325032\n",
      "Iteration # 1688  loss is :  1.3250276\n",
      "Iteration # 1689  loss is :  1.3250232\n",
      "Iteration # 1690  loss is :  1.3250189\n",
      "Iteration # 1691  loss is :  1.3250145\n",
      "Iteration # 1692  loss is :  1.3250102\n",
      "Iteration # 1693  loss is :  1.3250057\n",
      "Iteration # 1694  loss is :  1.3250015\n",
      "Iteration # 1695  loss is :  1.3249971\n",
      "Iteration # 1696  loss is :  1.3249927\n",
      "Iteration # 1697  loss is :  1.3249884\n",
      "Iteration # 1698  loss is :  1.3249841\n",
      "Iteration # 1699  loss is :  1.3249798\n",
      "Iteration # 1700  loss is :  1.3249754\n",
      "Iteration # 1701  loss is :  1.3249712\n",
      "Iteration # 1702  loss is :  1.3249669\n",
      "Iteration # 1703  loss is :  1.3249626\n",
      "Iteration # 1704  loss is :  1.3249584\n",
      "Iteration # 1705  loss is :  1.3249542\n",
      "Iteration # 1706  loss is :  1.32495\n",
      "Iteration # 1707  loss is :  1.3249457\n",
      "Iteration # 1708  loss is :  1.3249414\n",
      "Iteration # 1709  loss is :  1.3249371\n",
      "Iteration # 1710  loss is :  1.3249329\n",
      "Iteration # 1711  loss is :  1.3249286\n",
      "Iteration # 1712  loss is :  1.3249245\n",
      "Iteration # 1713  loss is :  1.3249203\n",
      "Iteration # 1714  loss is :  1.324916\n",
      "Iteration # 1715  loss is :  1.3249118\n",
      "Iteration # 1716  loss is :  1.3249078\n",
      "Iteration # 1717  loss is :  1.3249036\n",
      "Iteration # 1718  loss is :  1.3248993\n",
      "Iteration # 1719  loss is :  1.3248951\n",
      "Iteration # 1720  loss is :  1.3248911\n",
      "Iteration # 1721  loss is :  1.3248869\n",
      "Iteration # 1722  loss is :  1.3248826\n",
      "Iteration # 1723  loss is :  1.3248787\n",
      "Iteration # 1724  loss is :  1.3248745\n",
      "Iteration # 1725  loss is :  1.3248705\n",
      "Iteration # 1726  loss is :  1.3248664\n",
      "Iteration # 1727  loss is :  1.3248622\n",
      "Iteration # 1728  loss is :  1.3248582\n",
      "Iteration # 1729  loss is :  1.324854\n",
      "Iteration # 1730  loss is :  1.32485\n",
      "Iteration # 1731  loss is :  1.3248458\n",
      "Iteration # 1732  loss is :  1.3248416\n",
      "Iteration # 1733  loss is :  1.3248377\n",
      "Iteration # 1734  loss is :  1.3248336\n",
      "Iteration # 1735  loss is :  1.3248296\n",
      "Iteration # 1736  loss is :  1.3248255\n",
      "Iteration # 1737  loss is :  1.3248215\n",
      "Iteration # 1738  loss is :  1.3248174\n",
      "Iteration # 1739  loss is :  1.3248134\n",
      "Iteration # 1740  loss is :  1.3248093\n",
      "Iteration # 1741  loss is :  1.3248054\n",
      "Iteration # 1742  loss is :  1.3248013\n",
      "Iteration # 1743  loss is :  1.3247974\n",
      "Iteration # 1744  loss is :  1.3247935\n",
      "Iteration # 1745  loss is :  1.3247894\n",
      "Iteration # 1746  loss is :  1.3247854\n",
      "Iteration # 1747  loss is :  1.3247814\n",
      "Iteration # 1748  loss is :  1.3247776\n",
      "Iteration # 1749  loss is :  1.3247736\n",
      "Iteration # 1750  loss is :  1.3247696\n",
      "Iteration # 1751  loss is :  1.3247656\n",
      "Iteration # 1752  loss is :  1.3247617\n",
      "Iteration # 1753  loss is :  1.3247577\n",
      "Iteration # 1754  loss is :  1.3247538\n",
      "Iteration # 1755  loss is :  1.32475\n",
      "Iteration # 1756  loss is :  1.324746\n",
      "Iteration # 1757  loss is :  1.3247421\n",
      "Iteration # 1758  loss is :  1.3247383\n",
      "Iteration # 1759  loss is :  1.3247344\n",
      "Iteration # 1760  loss is :  1.3247304\n",
      "Iteration # 1761  loss is :  1.3247267\n",
      "Iteration # 1762  loss is :  1.3247226\n",
      "Iteration # 1763  loss is :  1.3247187\n",
      "Iteration # 1764  loss is :  1.324715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 1765  loss is :  1.3247112\n",
      "Iteration # 1766  loss is :  1.3247072\n",
      "Iteration # 1767  loss is :  1.3247035\n",
      "Iteration # 1768  loss is :  1.3246995\n",
      "Iteration # 1769  loss is :  1.3246958\n",
      "Iteration # 1770  loss is :  1.3246919\n",
      "Iteration # 1771  loss is :  1.3246881\n",
      "Iteration # 1772  loss is :  1.3246841\n",
      "Iteration # 1773  loss is :  1.3246806\n",
      "Iteration # 1774  loss is :  1.3246766\n",
      "Iteration # 1775  loss is :  1.3246728\n",
      "Iteration # 1776  loss is :  1.324669\n",
      "Iteration # 1777  loss is :  1.3246654\n",
      "Iteration # 1778  loss is :  1.3246615\n",
      "Iteration # 1779  loss is :  1.3246578\n",
      "Iteration # 1780  loss is :  1.3246539\n",
      "Iteration # 1781  loss is :  1.3246503\n",
      "Iteration # 1782  loss is :  1.3246466\n",
      "Iteration # 1783  loss is :  1.3246427\n",
      "Iteration # 1784  loss is :  1.3246391\n",
      "Iteration # 1785  loss is :  1.3246351\n",
      "Iteration # 1786  loss is :  1.3246315\n",
      "Iteration # 1787  loss is :  1.3246279\n",
      "Iteration # 1788  loss is :  1.3246241\n",
      "Iteration # 1789  loss is :  1.3246202\n",
      "Iteration # 1790  loss is :  1.3246168\n",
      "Iteration # 1791  loss is :  1.3246129\n",
      "Iteration # 1792  loss is :  1.3246093\n",
      "Iteration # 1793  loss is :  1.3246056\n",
      "Iteration # 1794  loss is :  1.3246018\n",
      "Iteration # 1795  loss is :  1.3245983\n",
      "Iteration # 1796  loss is :  1.3245944\n",
      "Iteration # 1797  loss is :  1.3245909\n",
      "Iteration # 1798  loss is :  1.3245873\n",
      "Iteration # 1799  loss is :  1.3245836\n",
      "Iteration # 1800  loss is :  1.32458\n",
      "Iteration # 1801  loss is :  1.3245763\n",
      "Iteration # 1802  loss is :  1.3245727\n",
      "Iteration # 1803  loss is :  1.3245689\n",
      "Iteration # 1804  loss is :  1.3245654\n",
      "Iteration # 1805  loss is :  1.3245617\n",
      "Iteration # 1806  loss is :  1.3245581\n",
      "Iteration # 1807  loss is :  1.3245546\n",
      "Iteration # 1808  loss is :  1.3245511\n",
      "Iteration # 1809  loss is :  1.3245474\n",
      "Iteration # 1810  loss is :  1.3245438\n",
      "Iteration # 1811  loss is :  1.3245401\n",
      "Iteration # 1812  loss is :  1.3245367\n",
      "Iteration # 1813  loss is :  1.3245331\n",
      "Iteration # 1814  loss is :  1.3245294\n",
      "Iteration # 1815  loss is :  1.324526\n",
      "Iteration # 1816  loss is :  1.3245224\n",
      "Iteration # 1817  loss is :  1.3245187\n",
      "Iteration # 1818  loss is :  1.3245153\n",
      "Iteration # 1819  loss is :  1.3245115\n",
      "Iteration # 1820  loss is :  1.3245082\n",
      "Iteration # 1821  loss is :  1.3245046\n",
      "Iteration # 1822  loss is :  1.3245012\n",
      "Iteration # 1823  loss is :  1.3244977\n",
      "Iteration # 1824  loss is :  1.3244941\n",
      "Iteration # 1825  loss is :  1.3244907\n",
      "Iteration # 1826  loss is :  1.324487\n",
      "Iteration # 1827  loss is :  1.3244836\n",
      "Iteration # 1828  loss is :  1.3244803\n",
      "Iteration # 1829  loss is :  1.3244766\n",
      "Iteration # 1830  loss is :  1.3244731\n",
      "Iteration # 1831  loss is :  1.3244698\n",
      "Iteration # 1832  loss is :  1.3244661\n",
      "Iteration # 1833  loss is :  1.3244628\n",
      "Iteration # 1834  loss is :  1.3244594\n",
      "Iteration # 1835  loss is :  1.3244559\n",
      "Iteration # 1836  loss is :  1.3244524\n",
      "Iteration # 1837  loss is :  1.324449\n",
      "Iteration # 1838  loss is :  1.3244455\n",
      "Iteration # 1839  loss is :  1.3244421\n",
      "Iteration # 1840  loss is :  1.3244387\n",
      "Iteration # 1841  loss is :  1.3244352\n",
      "Iteration # 1842  loss is :  1.3244318\n",
      "Iteration # 1843  loss is :  1.3244284\n",
      "Iteration # 1844  loss is :  1.324425\n",
      "Iteration # 1845  loss is :  1.3244215\n",
      "Iteration # 1846  loss is :  1.3244182\n",
      "Iteration # 1847  loss is :  1.324415\n",
      "Iteration # 1848  loss is :  1.3244114\n",
      "Iteration # 1849  loss is :  1.3244082\n",
      "Iteration # 1850  loss is :  1.3244047\n",
      "Iteration # 1851  loss is :  1.3244014\n",
      "Iteration # 1852  loss is :  1.3243979\n",
      "Iteration # 1853  loss is :  1.3243946\n",
      "Iteration # 1854  loss is :  1.3243912\n",
      "Iteration # 1855  loss is :  1.3243878\n",
      "Iteration # 1856  loss is :  1.3243846\n",
      "Iteration # 1857  loss is :  1.3243812\n",
      "Iteration # 1858  loss is :  1.3243779\n",
      "Iteration # 1859  loss is :  1.3243746\n",
      "Iteration # 1860  loss is :  1.3243713\n",
      "Iteration # 1861  loss is :  1.324368\n",
      "Iteration # 1862  loss is :  1.3243647\n",
      "Iteration # 1863  loss is :  1.3243613\n",
      "Iteration # 1864  loss is :  1.324358\n",
      "Iteration # 1865  loss is :  1.3243548\n",
      "Iteration # 1866  loss is :  1.3243514\n",
      "Iteration # 1867  loss is :  1.324348\n",
      "Iteration # 1868  loss is :  1.3243449\n",
      "Iteration # 1869  loss is :  1.3243415\n",
      "Iteration # 1870  loss is :  1.3243383\n",
      "Iteration # 1871  loss is :  1.3243351\n",
      "Iteration # 1872  loss is :  1.3243319\n",
      "Iteration # 1873  loss is :  1.3243284\n",
      "Iteration # 1874  loss is :  1.3243253\n",
      "Iteration # 1875  loss is :  1.3243219\n",
      "Iteration # 1876  loss is :  1.3243188\n",
      "Iteration # 1877  loss is :  1.3243154\n",
      "Iteration # 1878  loss is :  1.3243123\n",
      "Iteration # 1879  loss is :  1.3243091\n",
      "Iteration # 1880  loss is :  1.3243058\n",
      "Iteration # 1881  loss is :  1.3243026\n",
      "Iteration # 1882  loss is :  1.3242995\n",
      "Iteration # 1883  loss is :  1.3242962\n",
      "Iteration # 1884  loss is :  1.3242929\n",
      "Iteration # 1885  loss is :  1.3242898\n",
      "Iteration # 1886  loss is :  1.3242865\n",
      "Iteration # 1887  loss is :  1.3242835\n",
      "Iteration # 1888  loss is :  1.3242804\n",
      "Iteration # 1889  loss is :  1.324277\n",
      "Iteration # 1890  loss is :  1.3242738\n",
      "Iteration # 1891  loss is :  1.3242707\n",
      "Iteration # 1892  loss is :  1.3242675\n",
      "Iteration # 1893  loss is :  1.3242644\n",
      "Iteration # 1894  loss is :  1.3242613\n",
      "Iteration # 1895  loss is :  1.3242581\n",
      "Iteration # 1896  loss is :  1.324255\n",
      "Iteration # 1897  loss is :  1.3242519\n",
      "Iteration # 1898  loss is :  1.3242487\n",
      "Iteration # 1899  loss is :  1.3242456\n",
      "Iteration # 1900  loss is :  1.3242425\n",
      "Iteration # 1901  loss is :  1.3242393\n",
      "Iteration # 1902  loss is :  1.3242362\n",
      "Iteration # 1903  loss is :  1.324233\n",
      "Iteration # 1904  loss is :  1.3242298\n",
      "Iteration # 1905  loss is :  1.3242267\n",
      "Iteration # 1906  loss is :  1.3242235\n",
      "Iteration # 1907  loss is :  1.3242205\n",
      "Iteration # 1908  loss is :  1.3242176\n",
      "Iteration # 1909  loss is :  1.3242143\n",
      "Iteration # 1910  loss is :  1.3242112\n",
      "Iteration # 1911  loss is :  1.3242083\n",
      "Iteration # 1912  loss is :  1.3242052\n",
      "Iteration # 1913  loss is :  1.3242022\n",
      "Iteration # 1914  loss is :  1.324199\n",
      "Iteration # 1915  loss is :  1.3241961\n",
      "Iteration # 1916  loss is :  1.324193\n",
      "Iteration # 1917  loss is :  1.3241899\n",
      "Iteration # 1918  loss is :  1.3241867\n",
      "Iteration # 1919  loss is :  1.3241837\n",
      "Iteration # 1920  loss is :  1.3241807\n",
      "Iteration # 1921  loss is :  1.3241777\n",
      "Iteration # 1922  loss is :  1.3241746\n",
      "Iteration # 1923  loss is :  1.3241717\n",
      "Iteration # 1924  loss is :  1.3241687\n",
      "Iteration # 1925  loss is :  1.3241657\n",
      "Iteration # 1926  loss is :  1.3241626\n",
      "Iteration # 1927  loss is :  1.3241596\n",
      "Iteration # 1928  loss is :  1.3241566\n",
      "Iteration # 1929  loss is :  1.3241534\n",
      "Iteration # 1930  loss is :  1.3241506\n",
      "Iteration # 1931  loss is :  1.3241477\n",
      "Iteration # 1932  loss is :  1.3241446\n",
      "Iteration # 1933  loss is :  1.3241416\n",
      "Iteration # 1934  loss is :  1.3241385\n",
      "Iteration # 1935  loss is :  1.3241357\n",
      "Iteration # 1936  loss is :  1.3241327\n",
      "Iteration # 1937  loss is :  1.3241299\n",
      "Iteration # 1938  loss is :  1.3241268\n",
      "Iteration # 1939  loss is :  1.3241239\n",
      "Iteration # 1940  loss is :  1.324121\n",
      "Iteration # 1941  loss is :  1.3241179\n",
      "Iteration # 1942  loss is :  1.3241149\n",
      "Iteration # 1943  loss is :  1.324112\n",
      "Iteration # 1944  loss is :  1.3241092\n",
      "Iteration # 1945  loss is :  1.3241062\n",
      "Iteration # 1946  loss is :  1.3241032\n",
      "Iteration # 1947  loss is :  1.3241004\n",
      "Iteration # 1948  loss is :  1.3240976\n",
      "Iteration # 1949  loss is :  1.3240945\n",
      "Iteration # 1950  loss is :  1.3240917\n",
      "Iteration # 1951  loss is :  1.3240888\n",
      "Iteration # 1952  loss is :  1.324086\n",
      "Iteration # 1953  loss is :  1.3240829\n",
      "Iteration # 1954  loss is :  1.3240801\n",
      "Iteration # 1955  loss is :  1.3240772\n",
      "Iteration # 1956  loss is :  1.3240741\n",
      "Iteration # 1957  loss is :  1.3240714\n",
      "Iteration # 1958  loss is :  1.3240687\n",
      "Iteration # 1959  loss is :  1.3240656\n",
      "Iteration # 1960  loss is :  1.3240627\n",
      "Iteration # 1961  loss is :  1.32406\n",
      "Iteration # 1962  loss is :  1.324057\n",
      "Iteration # 1963  loss is :  1.3240542\n",
      "Iteration # 1964  loss is :  1.3240514\n",
      "Iteration # 1965  loss is :  1.3240484\n",
      "Iteration # 1966  loss is :  1.3240458\n",
      "Iteration # 1967  loss is :  1.3240428\n",
      "Iteration # 1968  loss is :  1.3240399\n",
      "Iteration # 1969  loss is :  1.3240372\n",
      "Iteration # 1970  loss is :  1.3240343\n",
      "Iteration # 1971  loss is :  1.3240315\n",
      "Iteration # 1972  loss is :  1.3240287\n",
      "Iteration # 1973  loss is :  1.3240259\n",
      "Iteration # 1974  loss is :  1.3240231\n",
      "Iteration # 1975  loss is :  1.3240201\n",
      "Iteration # 1976  loss is :  1.3240175\n",
      "Iteration # 1977  loss is :  1.3240147\n",
      "Iteration # 1978  loss is :  1.3240118\n",
      "Iteration # 1979  loss is :  1.3240091\n",
      "Iteration # 1980  loss is :  1.3240062\n",
      "Iteration # 1981  loss is :  1.3240035\n",
      "Iteration # 1982  loss is :  1.3240006\n",
      "Iteration # 1983  loss is :  1.323998\n",
      "Iteration # 1984  loss is :  1.323995\n",
      "Iteration # 1985  loss is :  1.3239924\n",
      "Iteration # 1986  loss is :  1.3239896\n",
      "Iteration # 1987  loss is :  1.323987\n",
      "Iteration # 1988  loss is :  1.3239841\n",
      "Iteration # 1989  loss is :  1.3239813\n",
      "Iteration # 1990  loss is :  1.3239785\n",
      "Iteration # 1991  loss is :  1.3239758\n",
      "Iteration # 1992  loss is :  1.3239731\n",
      "Iteration # 1993  loss is :  1.3239703\n",
      "Iteration # 1994  loss is :  1.3239677\n",
      "Iteration # 1995  loss is :  1.3239648\n",
      "Iteration # 1996  loss is :  1.3239622\n",
      "Iteration # 1997  loss is :  1.3239594\n",
      "Iteration # 1998  loss is :  1.3239567\n",
      "Iteration # 1999  loss is :  1.323954\n",
      "Iteration # 2000  loss is :  1.3239512\n",
      "Iteration # 2001  loss is :  1.3239487\n",
      "Iteration # 2002  loss is :  1.323946\n",
      "Iteration # 2003  loss is :  1.3239433\n",
      "Iteration # 2004  loss is :  1.3239405\n",
      "Iteration # 2005  loss is :  1.3239379\n",
      "Iteration # 2006  loss is :  1.3239352\n",
      "Iteration # 2007  loss is :  1.3239325\n",
      "Iteration # 2008  loss is :  1.3239298\n",
      "Iteration # 2009  loss is :  1.3239272\n",
      "Iteration # 2010  loss is :  1.3239244\n",
      "Iteration # 2011  loss is :  1.3239218\n",
      "Iteration # 2012  loss is :  1.323919\n",
      "Iteration # 2013  loss is :  1.3239164\n",
      "Iteration # 2014  loss is :  1.3239138\n",
      "Iteration # 2015  loss is :  1.3239112\n",
      "Iteration # 2016  loss is :  1.3239084\n",
      "Iteration # 2017  loss is :  1.323906\n",
      "Iteration # 2018  loss is :  1.3239032\n",
      "Iteration # 2019  loss is :  1.3239006\n",
      "Iteration # 2020  loss is :  1.3238978\n",
      "Iteration # 2021  loss is :  1.3238952\n",
      "Iteration # 2022  loss is :  1.3238927\n",
      "Iteration # 2023  loss is :  1.32389\n",
      "Iteration # 2024  loss is :  1.3238875\n",
      "Iteration # 2025  loss is :  1.3238848\n",
      "Iteration # 2026  loss is :  1.3238821\n",
      "Iteration # 2027  loss is :  1.3238797\n",
      "Iteration # 2028  loss is :  1.323877\n",
      "Iteration # 2029  loss is :  1.3238744\n",
      "Iteration # 2030  loss is :  1.3238719\n",
      "Iteration # 2031  loss is :  1.3238692\n",
      "Iteration # 2032  loss is :  1.3238666\n",
      "Iteration # 2033  loss is :  1.323864\n",
      "Iteration # 2034  loss is :  1.3238614\n",
      "Iteration # 2035  loss is :  1.3238589\n",
      "Iteration # 2036  loss is :  1.3238564\n",
      "Iteration # 2037  loss is :  1.3238536\n",
      "Iteration # 2038  loss is :  1.3238512\n",
      "Iteration # 2039  loss is :  1.3238485\n",
      "Iteration # 2040  loss is :  1.3238459\n",
      "Iteration # 2041  loss is :  1.3238434\n",
      "Iteration # 2042  loss is :  1.3238407\n",
      "Iteration # 2043  loss is :  1.3238382\n",
      "Iteration # 2044  loss is :  1.3238357\n",
      "Iteration # 2045  loss is :  1.3238331\n",
      "Iteration # 2046  loss is :  1.3238306\n",
      "Iteration # 2047  loss is :  1.3238281\n",
      "Iteration # 2048  loss is :  1.3238256\n",
      "Iteration # 2049  loss is :  1.323823\n",
      "Iteration # 2050  loss is :  1.3238205\n",
      "Iteration # 2051  loss is :  1.323818\n",
      "Iteration # 2052  loss is :  1.3238155\n",
      "Iteration # 2053  loss is :  1.3238128\n",
      "Iteration # 2054  loss is :  1.3238103\n",
      "Iteration # 2055  loss is :  1.3238078\n",
      "Iteration # 2056  loss is :  1.3238053\n",
      "Iteration # 2057  loss is :  1.3238027\n",
      "Iteration # 2058  loss is :  1.3238004\n",
      "Iteration # 2059  loss is :  1.3237978\n",
      "Iteration # 2060  loss is :  1.3237954\n",
      "Iteration # 2061  loss is :  1.3237928\n",
      "Iteration # 2062  loss is :  1.3237903\n",
      "Iteration # 2063  loss is :  1.3237879\n",
      "Iteration # 2064  loss is :  1.3237853\n",
      "Iteration # 2065  loss is :  1.3237829\n",
      "Iteration # 2066  loss is :  1.3237805\n",
      "Iteration # 2067  loss is :  1.3237779\n",
      "Iteration # 2068  loss is :  1.3237755\n",
      "Iteration # 2069  loss is :  1.323773\n",
      "Iteration # 2070  loss is :  1.3237705\n",
      "Iteration # 2071  loss is :  1.323768\n",
      "Iteration # 2072  loss is :  1.3237655\n",
      "Iteration # 2073  loss is :  1.3237631\n",
      "Iteration # 2074  loss is :  1.3237606\n",
      "Iteration # 2075  loss is :  1.3237582\n",
      "Iteration # 2076  loss is :  1.3237557\n",
      "Iteration # 2077  loss is :  1.3237534\n",
      "Iteration # 2078  loss is :  1.323751\n",
      "Iteration # 2079  loss is :  1.3237485\n",
      "Iteration # 2080  loss is :  1.323746\n",
      "Iteration # 2081  loss is :  1.3237437\n",
      "Iteration # 2082  loss is :  1.3237412\n",
      "Iteration # 2083  loss is :  1.3237388\n",
      "Iteration # 2084  loss is :  1.3237363\n",
      "Iteration # 2085  loss is :  1.3237339\n",
      "Iteration # 2086  loss is :  1.3237315\n",
      "Iteration # 2087  loss is :  1.3237292\n",
      "Iteration # 2088  loss is :  1.3237268\n",
      "Iteration # 2089  loss is :  1.3237243\n",
      "Iteration # 2090  loss is :  1.3237219\n",
      "Iteration # 2091  loss is :  1.3237195\n",
      "Iteration # 2092  loss is :  1.3237171\n",
      "Iteration # 2093  loss is :  1.3237147\n",
      "Iteration # 2094  loss is :  1.3237123\n",
      "Iteration # 2095  loss is :  1.3237101\n",
      "Iteration # 2096  loss is :  1.3237076\n",
      "Iteration # 2097  loss is :  1.3237052\n",
      "Iteration # 2098  loss is :  1.3237028\n",
      "Iteration # 2099  loss is :  1.3237005\n",
      "Iteration # 2100  loss is :  1.323698\n",
      "Iteration # 2101  loss is :  1.3236957\n",
      "Iteration # 2102  loss is :  1.3236934\n",
      "Iteration # 2103  loss is :  1.3236911\n",
      "Iteration # 2104  loss is :  1.3236886\n",
      "Iteration # 2105  loss is :  1.3236864\n",
      "Iteration # 2106  loss is :  1.323684\n",
      "Iteration # 2107  loss is :  1.3236816\n",
      "Iteration # 2108  loss is :  1.3236792\n",
      "Iteration # 2109  loss is :  1.323677\n",
      "Iteration # 2110  loss is :  1.3236746\n",
      "Iteration # 2111  loss is :  1.3236723\n",
      "Iteration # 2112  loss is :  1.3236699\n",
      "Iteration # 2113  loss is :  1.3236676\n",
      "Iteration # 2114  loss is :  1.3236654\n",
      "Iteration # 2115  loss is :  1.3236629\n",
      "Iteration # 2116  loss is :  1.3236607\n",
      "Iteration # 2117  loss is :  1.3236583\n",
      "Iteration # 2118  loss is :  1.323656\n",
      "Iteration # 2119  loss is :  1.3236536\n",
      "Iteration # 2120  loss is :  1.3236513\n",
      "Iteration # 2121  loss is :  1.323649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 2122  loss is :  1.3236468\n",
      "Iteration # 2123  loss is :  1.3236444\n",
      "Iteration # 2124  loss is :  1.3236423\n",
      "Iteration # 2125  loss is :  1.3236399\n",
      "Iteration # 2126  loss is :  1.3236376\n",
      "Iteration # 2127  loss is :  1.3236353\n",
      "Iteration # 2128  loss is :  1.323633\n",
      "Iteration # 2129  loss is :  1.3236307\n",
      "Iteration # 2130  loss is :  1.3236284\n",
      "Iteration # 2131  loss is :  1.3236262\n",
      "Iteration # 2132  loss is :  1.3236239\n",
      "Iteration # 2133  loss is :  1.3236216\n",
      "Iteration # 2134  loss is :  1.3236194\n",
      "Iteration # 2135  loss is :  1.3236171\n",
      "Iteration # 2136  loss is :  1.323615\n",
      "Iteration # 2137  loss is :  1.3236127\n",
      "Iteration # 2138  loss is :  1.3236103\n",
      "Iteration # 2139  loss is :  1.3236082\n",
      "Iteration # 2140  loss is :  1.3236058\n",
      "Iteration # 2141  loss is :  1.3236036\n",
      "Iteration # 2142  loss is :  1.3236015\n",
      "Iteration # 2143  loss is :  1.3235991\n",
      "Iteration # 2144  loss is :  1.3235968\n",
      "Iteration # 2145  loss is :  1.3235946\n",
      "Iteration # 2146  loss is :  1.3235924\n",
      "Iteration # 2147  loss is :  1.3235903\n",
      "Iteration # 2148  loss is :  1.323588\n",
      "Iteration # 2149  loss is :  1.3235857\n",
      "Iteration # 2150  loss is :  1.3235835\n",
      "Iteration # 2151  loss is :  1.3235812\n",
      "Iteration # 2152  loss is :  1.323579\n",
      "Iteration # 2153  loss is :  1.3235769\n",
      "Iteration # 2154  loss is :  1.3235747\n",
      "Iteration # 2155  loss is :  1.3235725\n",
      "Iteration # 2156  loss is :  1.3235704\n",
      "Iteration # 2157  loss is :  1.323568\n",
      "Iteration # 2158  loss is :  1.3235658\n",
      "Iteration # 2159  loss is :  1.3235637\n",
      "Iteration # 2160  loss is :  1.3235614\n",
      "Iteration # 2161  loss is :  1.3235594\n",
      "Iteration # 2162  loss is :  1.3235571\n",
      "Iteration # 2163  loss is :  1.3235549\n",
      "Iteration # 2164  loss is :  1.3235527\n",
      "Iteration # 2165  loss is :  1.3235506\n",
      "Iteration # 2166  loss is :  1.3235483\n",
      "Iteration # 2167  loss is :  1.3235463\n",
      "Iteration # 2168  loss is :  1.323544\n",
      "Iteration # 2169  loss is :  1.323542\n",
      "Iteration # 2170  loss is :  1.3235397\n",
      "Iteration # 2171  loss is :  1.3235375\n",
      "Iteration # 2172  loss is :  1.3235353\n",
      "Iteration # 2173  loss is :  1.3235332\n",
      "Iteration # 2174  loss is :  1.323531\n",
      "Iteration # 2175  loss is :  1.323529\n",
      "Iteration # 2176  loss is :  1.3235267\n",
      "Iteration # 2177  loss is :  1.3235246\n",
      "Iteration # 2178  loss is :  1.3235226\n",
      "Iteration # 2179  loss is :  1.3235204\n",
      "Iteration # 2180  loss is :  1.3235183\n",
      "Iteration # 2181  loss is :  1.3235159\n",
      "Iteration # 2182  loss is :  1.323514\n",
      "Iteration # 2183  loss is :  1.3235118\n",
      "Iteration # 2184  loss is :  1.3235097\n",
      "Iteration # 2185  loss is :  1.3235075\n",
      "Iteration # 2186  loss is :  1.3235054\n",
      "Iteration # 2187  loss is :  1.3235034\n",
      "Iteration # 2188  loss is :  1.3235012\n",
      "Iteration # 2189  loss is :  1.3234991\n",
      "Iteration # 2190  loss is :  1.3234968\n",
      "Iteration # 2191  loss is :  1.3234949\n",
      "Iteration # 2192  loss is :  1.3234929\n",
      "Iteration # 2193  loss is :  1.3234907\n",
      "Iteration # 2194  loss is :  1.3234886\n",
      "Iteration # 2195  loss is :  1.3234864\n",
      "Iteration # 2196  loss is :  1.3234844\n",
      "Iteration # 2197  loss is :  1.3234822\n",
      "Iteration # 2198  loss is :  1.3234802\n",
      "Iteration # 2199  loss is :  1.3234781\n",
      "Iteration # 2200  loss is :  1.323476\n",
      "Iteration # 2201  loss is :  1.3234739\n",
      "Iteration # 2202  loss is :  1.3234719\n",
      "Iteration # 2203  loss is :  1.3234698\n",
      "Iteration # 2204  loss is :  1.3234676\n",
      "Iteration # 2205  loss is :  1.3234656\n",
      "Iteration # 2206  loss is :  1.3234636\n",
      "Iteration # 2207  loss is :  1.3234614\n",
      "Iteration # 2208  loss is :  1.3234595\n",
      "Iteration # 2209  loss is :  1.3234574\n",
      "Iteration # 2210  loss is :  1.3234552\n",
      "Iteration # 2211  loss is :  1.3234533\n",
      "Iteration # 2212  loss is :  1.3234512\n",
      "Iteration # 2213  loss is :  1.3234491\n",
      "Iteration # 2214  loss is :  1.3234471\n",
      "Iteration # 2215  loss is :  1.3234451\n",
      "Iteration # 2216  loss is :  1.3234429\n",
      "Iteration # 2217  loss is :  1.323441\n",
      "Iteration # 2218  loss is :  1.323439\n",
      "Iteration # 2219  loss is :  1.323437\n",
      "Iteration # 2220  loss is :  1.3234347\n",
      "Iteration # 2221  loss is :  1.3234329\n",
      "Iteration # 2222  loss is :  1.3234308\n",
      "Iteration # 2223  loss is :  1.3234289\n",
      "Iteration # 2224  loss is :  1.3234268\n",
      "Iteration # 2225  loss is :  1.3234246\n",
      "Iteration # 2226  loss is :  1.3234227\n",
      "Iteration # 2227  loss is :  1.3234206\n",
      "Iteration # 2228  loss is :  1.3234186\n",
      "Iteration # 2229  loss is :  1.3234167\n",
      "Iteration # 2230  loss is :  1.3234146\n",
      "Iteration # 2231  loss is :  1.3234125\n",
      "Iteration # 2232  loss is :  1.3234107\n",
      "Iteration # 2233  loss is :  1.3234087\n",
      "Iteration # 2234  loss is :  1.3234067\n",
      "Iteration # 2235  loss is :  1.3234047\n",
      "Iteration # 2236  loss is :  1.3234026\n",
      "Iteration # 2237  loss is :  1.3234007\n",
      "Iteration # 2238  loss is :  1.3233987\n",
      "Iteration # 2239  loss is :  1.3233967\n",
      "Iteration # 2240  loss is :  1.3233947\n",
      "Iteration # 2241  loss is :  1.3233926\n",
      "Iteration # 2242  loss is :  1.3233907\n",
      "Iteration # 2243  loss is :  1.3233887\n",
      "Iteration # 2244  loss is :  1.3233867\n",
      "Iteration # 2245  loss is :  1.3233848\n",
      "Iteration # 2246  loss is :  1.3233827\n",
      "Iteration # 2247  loss is :  1.3233808\n",
      "Iteration # 2248  loss is :  1.3233788\n",
      "Iteration # 2249  loss is :  1.3233769\n",
      "Iteration # 2250  loss is :  1.323375\n",
      "Iteration # 2251  loss is :  1.323373\n",
      "Iteration # 2252  loss is :  1.323371\n",
      "Iteration # 2253  loss is :  1.323369\n",
      "Iteration # 2254  loss is :  1.3233671\n",
      "Iteration # 2255  loss is :  1.3233652\n",
      "Iteration # 2256  loss is :  1.3233633\n",
      "Iteration # 2257  loss is :  1.3233614\n",
      "Iteration # 2258  loss is :  1.3233594\n",
      "Iteration # 2259  loss is :  1.3233575\n",
      "Iteration # 2260  loss is :  1.3233556\n",
      "Iteration # 2261  loss is :  1.3233535\n",
      "Iteration # 2262  loss is :  1.3233517\n",
      "Iteration # 2263  loss is :  1.3233496\n",
      "Iteration # 2264  loss is :  1.3233478\n",
      "Iteration # 2265  loss is :  1.3233459\n",
      "Iteration # 2266  loss is :  1.323344\n",
      "Iteration # 2267  loss is :  1.323342\n",
      "Iteration # 2268  loss is :  1.32334\n",
      "Iteration # 2269  loss is :  1.3233382\n",
      "Iteration # 2270  loss is :  1.3233365\n",
      "Iteration # 2271  loss is :  1.3233343\n",
      "Iteration # 2272  loss is :  1.3233324\n",
      "Iteration # 2273  loss is :  1.3233305\n",
      "Iteration # 2274  loss is :  1.3233286\n",
      "Iteration # 2275  loss is :  1.3233267\n",
      "Iteration # 2276  loss is :  1.3233248\n",
      "Iteration # 2277  loss is :  1.323323\n",
      "Iteration # 2278  loss is :  1.323321\n",
      "Iteration # 2279  loss is :  1.3233191\n",
      "Iteration # 2280  loss is :  1.3233173\n",
      "Iteration # 2281  loss is :  1.3233153\n",
      "Iteration # 2282  loss is :  1.3233135\n",
      "Iteration # 2283  loss is :  1.3233116\n",
      "Iteration # 2284  loss is :  1.3233097\n",
      "Iteration # 2285  loss is :  1.3233078\n",
      "Iteration # 2286  loss is :  1.323306\n",
      "Iteration # 2287  loss is :  1.323304\n",
      "Iteration # 2288  loss is :  1.3233023\n",
      "Iteration # 2289  loss is :  1.3233004\n",
      "Iteration # 2290  loss is :  1.3232985\n",
      "Iteration # 2291  loss is :  1.3232965\n",
      "Iteration # 2292  loss is :  1.3232946\n",
      "Iteration # 2293  loss is :  1.3232929\n",
      "Iteration # 2294  loss is :  1.3232908\n",
      "Iteration # 2295  loss is :  1.323289\n",
      "Iteration # 2296  loss is :  1.3232871\n",
      "Iteration # 2297  loss is :  1.3232853\n",
      "Iteration # 2298  loss is :  1.3232834\n",
      "Iteration # 2299  loss is :  1.3232816\n",
      "Iteration # 2300  loss is :  1.3232799\n",
      "Iteration # 2301  loss is :  1.323278\n",
      "Iteration # 2302  loss is :  1.3232762\n",
      "Iteration # 2303  loss is :  1.3232743\n",
      "Iteration # 2304  loss is :  1.3232725\n",
      "Iteration # 2305  loss is :  1.3232706\n",
      "Iteration # 2306  loss is :  1.3232688\n",
      "Iteration # 2307  loss is :  1.323267\n",
      "Iteration # 2308  loss is :  1.3232651\n",
      "Iteration # 2309  loss is :  1.3232633\n",
      "Iteration # 2310  loss is :  1.3232615\n",
      "Iteration # 2311  loss is :  1.3232597\n",
      "Iteration # 2312  loss is :  1.3232579\n",
      "Iteration # 2313  loss is :  1.3232559\n",
      "Iteration # 2314  loss is :  1.3232542\n",
      "Iteration # 2315  loss is :  1.3232523\n",
      "Iteration # 2316  loss is :  1.3232505\n",
      "Iteration # 2317  loss is :  1.3232489\n",
      "Iteration # 2318  loss is :  1.323247\n",
      "Iteration # 2319  loss is :  1.323245\n",
      "Iteration # 2320  loss is :  1.3232433\n",
      "Iteration # 2321  loss is :  1.3232415\n",
      "Iteration # 2322  loss is :  1.3232397\n",
      "Iteration # 2323  loss is :  1.3232379\n",
      "Iteration # 2324  loss is :  1.3232361\n",
      "Iteration # 2325  loss is :  1.3232343\n",
      "Iteration # 2326  loss is :  1.3232324\n",
      "Iteration # 2327  loss is :  1.3232306\n",
      "Iteration # 2328  loss is :  1.3232291\n",
      "Iteration # 2329  loss is :  1.3232272\n",
      "Iteration # 2330  loss is :  1.3232254\n",
      "Iteration # 2331  loss is :  1.3232236\n",
      "Iteration # 2332  loss is :  1.3232218\n",
      "Iteration # 2333  loss is :  1.32322\n",
      "Iteration # 2334  loss is :  1.3232183\n",
      "Iteration # 2335  loss is :  1.3232164\n",
      "Iteration # 2336  loss is :  1.3232147\n",
      "Iteration # 2337  loss is :  1.3232127\n",
      "Iteration # 2338  loss is :  1.3232111\n",
      "Iteration # 2339  loss is :  1.3232093\n",
      "Iteration # 2340  loss is :  1.3232076\n",
      "Iteration # 2341  loss is :  1.3232058\n",
      "Iteration # 2342  loss is :  1.323204\n",
      "Iteration # 2343  loss is :  1.3232025\n",
      "Iteration # 2344  loss is :  1.3232006\n",
      "Iteration # 2345  loss is :  1.3231988\n",
      "Iteration # 2346  loss is :  1.3231971\n",
      "Iteration # 2347  loss is :  1.3231953\n",
      "Iteration # 2348  loss is :  1.3231936\n",
      "Iteration # 2349  loss is :  1.3231919\n",
      "Iteration # 2350  loss is :  1.32319\n",
      "Iteration # 2351  loss is :  1.3231883\n",
      "Iteration # 2352  loss is :  1.3231866\n",
      "Iteration # 2353  loss is :  1.323185\n",
      "Iteration # 2354  loss is :  1.3231831\n",
      "Iteration # 2355  loss is :  1.3231814\n",
      "Iteration # 2356  loss is :  1.3231797\n",
      "Iteration # 2357  loss is :  1.3231779\n",
      "Iteration # 2358  loss is :  1.3231763\n",
      "Iteration # 2359  loss is :  1.3231745\n",
      "Iteration # 2360  loss is :  1.3231727\n",
      "Iteration # 2361  loss is :  1.323171\n",
      "Iteration # 2362  loss is :  1.3231692\n",
      "Iteration # 2363  loss is :  1.3231676\n",
      "Iteration # 2364  loss is :  1.3231659\n",
      "Iteration # 2365  loss is :  1.3231642\n",
      "Iteration # 2366  loss is :  1.3231624\n",
      "Iteration # 2367  loss is :  1.3231609\n",
      "Iteration # 2368  loss is :  1.3231591\n",
      "Iteration # 2369  loss is :  1.3231573\n",
      "Iteration # 2370  loss is :  1.3231555\n",
      "Iteration # 2371  loss is :  1.323154\n",
      "Iteration # 2372  loss is :  1.3231523\n",
      "Iteration # 2373  loss is :  1.3231505\n",
      "Iteration # 2374  loss is :  1.3231488\n",
      "Iteration # 2375  loss is :  1.3231472\n",
      "Iteration # 2376  loss is :  1.3231455\n",
      "Iteration # 2377  loss is :  1.3231438\n",
      "Iteration # 2378  loss is :  1.323142\n",
      "Iteration # 2379  loss is :  1.3231403\n",
      "Iteration # 2380  loss is :  1.3231387\n",
      "Iteration # 2381  loss is :  1.3231369\n",
      "Iteration # 2382  loss is :  1.3231353\n",
      "Iteration # 2383  loss is :  1.3231337\n",
      "Iteration # 2384  loss is :  1.323132\n",
      "Iteration # 2385  loss is :  1.3231302\n",
      "Iteration # 2386  loss is :  1.3231287\n",
      "Iteration # 2387  loss is :  1.3231269\n",
      "Iteration # 2388  loss is :  1.3231252\n",
      "Iteration # 2389  loss is :  1.3231237\n",
      "Iteration # 2390  loss is :  1.3231218\n",
      "Iteration # 2391  loss is :  1.3231202\n",
      "Iteration # 2392  loss is :  1.3231187\n",
      "Iteration # 2393  loss is :  1.3231169\n",
      "Iteration # 2394  loss is :  1.3231153\n",
      "Iteration # 2395  loss is :  1.3231136\n",
      "Iteration # 2396  loss is :  1.3231118\n",
      "Iteration # 2397  loss is :  1.3231103\n",
      "Iteration # 2398  loss is :  1.3231087\n",
      "Iteration # 2399  loss is :  1.3231071\n",
      "Iteration # 2400  loss is :  1.3231056\n",
      "Iteration # 2401  loss is :  1.3231037\n",
      "Iteration # 2402  loss is :  1.323102\n",
      "Iteration # 2403  loss is :  1.3231003\n",
      "Iteration # 2404  loss is :  1.3230988\n",
      "Iteration # 2405  loss is :  1.3230972\n",
      "Iteration # 2406  loss is :  1.3230956\n",
      "Iteration # 2407  loss is :  1.3230939\n",
      "Iteration # 2408  loss is :  1.3230922\n",
      "Iteration # 2409  loss is :  1.3230906\n",
      "Iteration # 2410  loss is :  1.323089\n",
      "Iteration # 2411  loss is :  1.3230875\n",
      "Iteration # 2412  loss is :  1.3230858\n",
      "Iteration # 2413  loss is :  1.3230841\n",
      "Iteration # 2414  loss is :  1.3230826\n",
      "Iteration # 2415  loss is :  1.3230809\n",
      "Iteration # 2416  loss is :  1.3230792\n",
      "Iteration # 2417  loss is :  1.3230776\n",
      "Iteration # 2418  loss is :  1.323076\n",
      "Iteration # 2419  loss is :  1.3230745\n",
      "Iteration # 2420  loss is :  1.3230728\n",
      "Iteration # 2421  loss is :  1.3230712\n",
      "Iteration # 2422  loss is :  1.3230696\n",
      "Iteration # 2423  loss is :  1.323068\n",
      "Iteration # 2424  loss is :  1.3230662\n",
      "Iteration # 2425  loss is :  1.3230648\n",
      "Iteration # 2426  loss is :  1.3230631\n",
      "Iteration # 2427  loss is :  1.3230616\n",
      "Iteration # 2428  loss is :  1.32306\n",
      "Iteration # 2429  loss is :  1.3230582\n",
      "Iteration # 2430  loss is :  1.3230567\n",
      "Iteration # 2431  loss is :  1.3230551\n",
      "Iteration # 2432  loss is :  1.3230536\n",
      "Iteration # 2433  loss is :  1.323052\n",
      "Iteration # 2434  loss is :  1.3230505\n",
      "Iteration # 2435  loss is :  1.3230488\n",
      "Iteration # 2436  loss is :  1.3230473\n",
      "Iteration # 2437  loss is :  1.3230456\n",
      "Iteration # 2438  loss is :  1.3230441\n",
      "Iteration # 2439  loss is :  1.3230425\n",
      "Iteration # 2440  loss is :  1.323041\n",
      "Iteration # 2441  loss is :  1.3230393\n",
      "Iteration # 2442  loss is :  1.3230377\n",
      "Iteration # 2443  loss is :  1.3230362\n",
      "Iteration # 2444  loss is :  1.3230346\n",
      "Iteration # 2445  loss is :  1.3230332\n",
      "Iteration # 2446  loss is :  1.3230315\n",
      "Iteration # 2447  loss is :  1.3230301\n",
      "Iteration # 2448  loss is :  1.3230284\n",
      "Iteration # 2449  loss is :  1.3230267\n",
      "Iteration # 2450  loss is :  1.3230251\n",
      "Iteration # 2451  loss is :  1.3230237\n",
      "Iteration # 2452  loss is :  1.3230221\n",
      "Iteration # 2453  loss is :  1.3230206\n",
      "Iteration # 2454  loss is :  1.323019\n",
      "Iteration # 2455  loss is :  1.3230175\n",
      "Iteration # 2456  loss is :  1.3230159\n",
      "Iteration # 2457  loss is :  1.3230144\n",
      "Iteration # 2458  loss is :  1.323013\n",
      "Iteration # 2459  loss is :  1.3230113\n",
      "Iteration # 2460  loss is :  1.3230098\n",
      "Iteration # 2461  loss is :  1.3230083\n",
      "Iteration # 2462  loss is :  1.3230066\n",
      "Iteration # 2463  loss is :  1.3230051\n",
      "Iteration # 2464  loss is :  1.3230036\n",
      "Iteration # 2465  loss is :  1.3230021\n",
      "Iteration # 2466  loss is :  1.3230007\n",
      "Iteration # 2467  loss is :  1.322999\n",
      "Iteration # 2468  loss is :  1.3229975\n",
      "Iteration # 2469  loss is :  1.3229959\n",
      "Iteration # 2470  loss is :  1.3229945\n",
      "Iteration # 2471  loss is :  1.3229929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 2472  loss is :  1.3229914\n",
      "Iteration # 2473  loss is :  1.3229897\n",
      "Iteration # 2474  loss is :  1.3229882\n",
      "Iteration # 2475  loss is :  1.3229868\n",
      "Iteration # 2476  loss is :  1.3229853\n",
      "Iteration # 2477  loss is :  1.3229837\n",
      "Iteration # 2478  loss is :  1.3229822\n",
      "Iteration # 2479  loss is :  1.3229806\n",
      "Iteration # 2480  loss is :  1.3229792\n",
      "Iteration # 2481  loss is :  1.3229778\n",
      "Iteration # 2482  loss is :  1.3229762\n",
      "Iteration # 2483  loss is :  1.3229747\n",
      "Iteration # 2484  loss is :  1.3229731\n",
      "Iteration # 2485  loss is :  1.3229717\n",
      "Iteration # 2486  loss is :  1.3229703\n",
      "Iteration # 2487  loss is :  1.3229686\n",
      "Iteration # 2488  loss is :  1.3229672\n",
      "Iteration # 2489  loss is :  1.3229657\n",
      "Iteration # 2490  loss is :  1.3229643\n",
      "Iteration # 2491  loss is :  1.3229626\n",
      "Iteration # 2492  loss is :  1.3229612\n",
      "Iteration # 2493  loss is :  1.3229597\n",
      "Iteration # 2494  loss is :  1.3229584\n",
      "Iteration # 2495  loss is :  1.3229568\n",
      "Iteration # 2496  loss is :  1.3229551\n",
      "Iteration # 2497  loss is :  1.3229538\n",
      "Iteration # 2498  loss is :  1.3229523\n",
      "Iteration # 2499  loss is :  1.322951\n",
      "Iteration # 2500  loss is :  1.3229494\n",
      "Iteration # 2501  loss is :  1.322948\n",
      "Iteration # 2502  loss is :  1.3229465\n",
      "Iteration # 2503  loss is :  1.322945\n",
      "Iteration # 2504  loss is :  1.3229436\n",
      "Iteration # 2505  loss is :  1.322942\n",
      "Iteration # 2506  loss is :  1.3229406\n",
      "Iteration # 2507  loss is :  1.3229392\n",
      "Iteration # 2508  loss is :  1.3229376\n",
      "Iteration # 2509  loss is :  1.322936\n",
      "Iteration # 2510  loss is :  1.3229346\n",
      "Iteration # 2511  loss is :  1.3229333\n",
      "Iteration # 2512  loss is :  1.3229318\n",
      "Iteration # 2513  loss is :  1.3229303\n",
      "Iteration # 2514  loss is :  1.3229288\n",
      "Iteration # 2515  loss is :  1.3229275\n",
      "Iteration # 2516  loss is :  1.3229259\n",
      "Iteration # 2517  loss is :  1.3229246\n",
      "Iteration # 2518  loss is :  1.3229231\n",
      "Iteration # 2519  loss is :  1.3229218\n",
      "Iteration # 2520  loss is :  1.3229202\n",
      "Iteration # 2521  loss is :  1.3229188\n",
      "Iteration # 2522  loss is :  1.3229172\n",
      "Iteration # 2523  loss is :  1.3229159\n",
      "Iteration # 2524  loss is :  1.3229144\n",
      "Iteration # 2525  loss is :  1.3229128\n",
      "Iteration # 2526  loss is :  1.3229114\n",
      "Iteration # 2527  loss is :  1.3229101\n",
      "Iteration # 2528  loss is :  1.3229086\n",
      "Iteration # 2529  loss is :  1.3229072\n",
      "Iteration # 2530  loss is :  1.3229058\n",
      "Iteration # 2531  loss is :  1.3229043\n",
      "Iteration # 2532  loss is :  1.322903\n",
      "Iteration # 2533  loss is :  1.3229016\n",
      "Iteration # 2534  loss is :  1.3229\n",
      "Iteration # 2535  loss is :  1.3228986\n",
      "Iteration # 2536  loss is :  1.3228972\n",
      "Iteration # 2537  loss is :  1.3228959\n",
      "Iteration # 2538  loss is :  1.3228942\n",
      "Iteration # 2539  loss is :  1.3228929\n",
      "Iteration # 2540  loss is :  1.3228914\n",
      "Iteration # 2541  loss is :  1.3228902\n",
      "Iteration # 2542  loss is :  1.3228887\n",
      "Iteration # 2543  loss is :  1.3228872\n",
      "Iteration # 2544  loss is :  1.3228859\n",
      "Iteration # 2545  loss is :  1.3228846\n",
      "Iteration # 2546  loss is :  1.322883\n",
      "Iteration # 2547  loss is :  1.3228817\n",
      "Iteration # 2548  loss is :  1.3228803\n",
      "Iteration # 2549  loss is :  1.322879\n",
      "Iteration # 2550  loss is :  1.3228774\n",
      "Iteration # 2551  loss is :  1.3228761\n",
      "Iteration # 2552  loss is :  1.3228747\n",
      "Iteration # 2553  loss is :  1.3228731\n",
      "Iteration # 2554  loss is :  1.3228718\n",
      "Iteration # 2555  loss is :  1.3228705\n",
      "Iteration # 2556  loss is :  1.3228691\n",
      "Iteration # 2557  loss is :  1.3228678\n",
      "Iteration # 2558  loss is :  1.3228662\n",
      "Iteration # 2559  loss is :  1.3228648\n",
      "Iteration # 2560  loss is :  1.3228636\n",
      "Iteration # 2561  loss is :  1.322862\n",
      "Iteration # 2562  loss is :  1.3228607\n",
      "Iteration # 2563  loss is :  1.3228592\n",
      "Iteration # 2564  loss is :  1.3228581\n",
      "Iteration # 2565  loss is :  1.3228565\n",
      "Iteration # 2566  loss is :  1.3228552\n",
      "Iteration # 2567  loss is :  1.3228538\n",
      "Iteration # 2568  loss is :  1.3228524\n",
      "Iteration # 2569  loss is :  1.3228511\n",
      "Iteration # 2570  loss is :  1.3228499\n",
      "Iteration # 2571  loss is :  1.3228483\n",
      "Iteration # 2572  loss is :  1.322847\n",
      "Iteration # 2573  loss is :  1.3228457\n",
      "Iteration # 2574  loss is :  1.3228441\n",
      "Iteration # 2575  loss is :  1.3228427\n",
      "Iteration # 2576  loss is :  1.3228415\n",
      "Iteration # 2577  loss is :  1.3228402\n",
      "Iteration # 2578  loss is :  1.3228388\n",
      "Iteration # 2579  loss is :  1.3228374\n",
      "Iteration # 2580  loss is :  1.3228359\n",
      "Iteration # 2581  loss is :  1.3228347\n",
      "Iteration # 2582  loss is :  1.3228333\n",
      "Iteration # 2583  loss is :  1.322832\n",
      "Iteration # 2584  loss is :  1.3228307\n",
      "Iteration # 2585  loss is :  1.3228292\n",
      "Iteration # 2586  loss is :  1.3228278\n",
      "Iteration # 2587  loss is :  1.3228266\n",
      "Iteration # 2588  loss is :  1.3228252\n",
      "Iteration # 2589  loss is :  1.3228238\n",
      "Iteration # 2590  loss is :  1.3228225\n",
      "Iteration # 2591  loss is :  1.3228211\n",
      "Iteration # 2592  loss is :  1.3228198\n",
      "Iteration # 2593  loss is :  1.3228185\n",
      "Iteration # 2594  loss is :  1.3228172\n",
      "Iteration # 2595  loss is :  1.3228159\n",
      "Iteration # 2596  loss is :  1.3228145\n",
      "Iteration # 2597  loss is :  1.3228133\n",
      "Iteration # 2598  loss is :  1.3228118\n",
      "Iteration # 2599  loss is :  1.3228104\n",
      "Iteration # 2600  loss is :  1.322809\n",
      "Iteration # 2601  loss is :  1.3228078\n",
      "Iteration # 2602  loss is :  1.3228064\n",
      "Iteration # 2603  loss is :  1.322805\n",
      "Iteration # 2604  loss is :  1.3228037\n",
      "Iteration # 2605  loss is :  1.3228024\n",
      "Iteration # 2606  loss is :  1.3228011\n",
      "Iteration # 2607  loss is :  1.3227998\n",
      "Iteration # 2608  loss is :  1.3227985\n",
      "Iteration # 2609  loss is :  1.3227971\n",
      "Iteration # 2610  loss is :  1.3227959\n",
      "Iteration # 2611  loss is :  1.3227946\n",
      "Iteration # 2612  loss is :  1.3227931\n",
      "Iteration # 2613  loss is :  1.3227919\n",
      "Iteration # 2614  loss is :  1.3227906\n",
      "Iteration # 2615  loss is :  1.3227893\n",
      "Iteration # 2616  loss is :  1.322788\n",
      "Iteration # 2617  loss is :  1.3227867\n",
      "Iteration # 2618  loss is :  1.3227854\n",
      "Iteration # 2619  loss is :  1.3227841\n",
      "Iteration # 2620  loss is :  1.3227829\n",
      "Iteration # 2621  loss is :  1.3227816\n",
      "Iteration # 2622  loss is :  1.3227801\n",
      "Iteration # 2623  loss is :  1.3227789\n",
      "Iteration # 2624  loss is :  1.3227776\n",
      "Iteration # 2625  loss is :  1.3227762\n",
      "Iteration # 2626  loss is :  1.322775\n",
      "Iteration # 2627  loss is :  1.3227737\n",
      "Iteration # 2628  loss is :  1.3227724\n",
      "Iteration # 2629  loss is :  1.3227711\n",
      "Iteration # 2630  loss is :  1.3227699\n",
      "Iteration # 2631  loss is :  1.3227686\n",
      "Iteration # 2632  loss is :  1.3227673\n",
      "Iteration # 2633  loss is :  1.322766\n",
      "Iteration # 2634  loss is :  1.3227646\n",
      "Iteration # 2635  loss is :  1.3227633\n",
      "Iteration # 2636  loss is :  1.322762\n",
      "Iteration # 2637  loss is :  1.3227607\n",
      "Iteration # 2638  loss is :  1.3227595\n",
      "Iteration # 2639  loss is :  1.3227582\n",
      "Iteration # 2640  loss is :  1.3227569\n",
      "Iteration # 2641  loss is :  1.3227557\n",
      "Iteration # 2642  loss is :  1.3227545\n",
      "Iteration # 2643  loss is :  1.322753\n",
      "Iteration # 2644  loss is :  1.3227519\n",
      "Iteration # 2645  loss is :  1.3227506\n",
      "Iteration # 2646  loss is :  1.3227493\n",
      "Iteration # 2647  loss is :  1.322748\n",
      "Iteration # 2648  loss is :  1.3227469\n",
      "Iteration # 2649  loss is :  1.3227454\n",
      "Iteration # 2650  loss is :  1.3227443\n",
      "Iteration # 2651  loss is :  1.322743\n",
      "Iteration # 2652  loss is :  1.3227416\n",
      "Iteration # 2653  loss is :  1.3227406\n",
      "Iteration # 2654  loss is :  1.3227391\n",
      "Iteration # 2655  loss is :  1.3227378\n",
      "Iteration # 2656  loss is :  1.3227366\n",
      "Iteration # 2657  loss is :  1.3227353\n",
      "Iteration # 2658  loss is :  1.322734\n",
      "Iteration # 2659  loss is :  1.3227329\n",
      "Iteration # 2660  loss is :  1.3227316\n",
      "Iteration # 2661  loss is :  1.3227303\n",
      "Iteration # 2662  loss is :  1.322729\n",
      "Iteration # 2663  loss is :  1.3227279\n",
      "Iteration # 2664  loss is :  1.3227266\n",
      "Iteration # 2665  loss is :  1.3227254\n",
      "Iteration # 2666  loss is :  1.3227242\n",
      "Iteration # 2667  loss is :  1.3227229\n",
      "Iteration # 2668  loss is :  1.3227216\n",
      "Iteration # 2669  loss is :  1.3227205\n",
      "Iteration # 2670  loss is :  1.3227191\n",
      "Iteration # 2671  loss is :  1.3227179\n",
      "Iteration # 2672  loss is :  1.3227166\n",
      "Iteration # 2673  loss is :  1.3227155\n",
      "Iteration # 2674  loss is :  1.3227143\n",
      "Iteration # 2675  loss is :  1.3227129\n",
      "Iteration # 2676  loss is :  1.3227118\n",
      "Iteration # 2677  loss is :  1.3227105\n",
      "Iteration # 2678  loss is :  1.3227092\n",
      "Iteration # 2679  loss is :  1.322708\n",
      "Iteration # 2680  loss is :  1.3227068\n",
      "Iteration # 2681  loss is :  1.3227055\n",
      "Iteration # 2682  loss is :  1.3227043\n",
      "Iteration # 2683  loss is :  1.3227031\n",
      "Iteration # 2684  loss is :  1.3227017\n",
      "Iteration # 2685  loss is :  1.3227007\n",
      "Iteration # 2686  loss is :  1.3226994\n",
      "Iteration # 2687  loss is :  1.3226981\n",
      "Iteration # 2688  loss is :  1.322697\n",
      "Iteration # 2689  loss is :  1.3226957\n",
      "Iteration # 2690  loss is :  1.3226944\n",
      "Iteration # 2691  loss is :  1.3226935\n",
      "Iteration # 2692  loss is :  1.322692\n",
      "Iteration # 2693  loss is :  1.3226908\n",
      "Iteration # 2694  loss is :  1.3226897\n",
      "Iteration # 2695  loss is :  1.3226885\n",
      "Iteration # 2696  loss is :  1.3226873\n",
      "Iteration # 2697  loss is :  1.3226861\n",
      "Iteration # 2698  loss is :  1.3226848\n",
      "Iteration # 2699  loss is :  1.3226836\n",
      "Iteration # 2700  loss is :  1.3226824\n",
      "Iteration # 2701  loss is :  1.3226813\n",
      "Iteration # 2702  loss is :  1.3226801\n",
      "Iteration # 2703  loss is :  1.3226787\n",
      "Iteration # 2704  loss is :  1.3226777\n",
      "Iteration # 2705  loss is :  1.3226764\n",
      "Iteration # 2706  loss is :  1.3226751\n",
      "Iteration # 2707  loss is :  1.322674\n",
      "Iteration # 2708  loss is :  1.3226728\n",
      "Iteration # 2709  loss is :  1.3226717\n",
      "Iteration # 2710  loss is :  1.3226705\n",
      "Iteration # 2711  loss is :  1.3226691\n",
      "Iteration # 2712  loss is :  1.3226681\n",
      "Iteration # 2713  loss is :  1.3226669\n",
      "Iteration # 2714  loss is :  1.3226656\n",
      "Iteration # 2715  loss is :  1.3226645\n",
      "Iteration # 2716  loss is :  1.3226633\n",
      "Iteration # 2717  loss is :  1.3226621\n",
      "Iteration # 2718  loss is :  1.3226608\n",
      "Iteration # 2719  loss is :  1.3226597\n",
      "Iteration # 2720  loss is :  1.3226587\n",
      "Iteration # 2721  loss is :  1.3226573\n",
      "Iteration # 2722  loss is :  1.3226562\n",
      "Iteration # 2723  loss is :  1.322655\n",
      "Iteration # 2724  loss is :  1.3226539\n",
      "Iteration # 2725  loss is :  1.3226527\n",
      "Iteration # 2726  loss is :  1.3226516\n",
      "Iteration # 2727  loss is :  1.3226502\n",
      "Iteration # 2728  loss is :  1.3226491\n",
      "Iteration # 2729  loss is :  1.322648\n",
      "Iteration # 2730  loss is :  1.3226467\n",
      "Iteration # 2731  loss is :  1.3226458\n",
      "Iteration # 2732  loss is :  1.3226444\n",
      "Iteration # 2733  loss is :  1.3226433\n",
      "Iteration # 2734  loss is :  1.3226422\n",
      "Iteration # 2735  loss is :  1.3226409\n",
      "Iteration # 2736  loss is :  1.32264\n",
      "Iteration # 2737  loss is :  1.3226386\n",
      "Iteration # 2738  loss is :  1.3226376\n",
      "Iteration # 2739  loss is :  1.3226362\n",
      "Iteration # 2740  loss is :  1.322635\n",
      "Iteration # 2741  loss is :  1.3226341\n",
      "Iteration # 2742  loss is :  1.3226328\n",
      "Iteration # 2743  loss is :  1.3226317\n",
      "Iteration # 2744  loss is :  1.3226305\n",
      "Iteration # 2745  loss is :  1.3226293\n",
      "Iteration # 2746  loss is :  1.3226283\n",
      "Iteration # 2747  loss is :  1.3226271\n",
      "Iteration # 2748  loss is :  1.3226259\n",
      "Iteration # 2749  loss is :  1.3226248\n",
      "Iteration # 2750  loss is :  1.3226236\n",
      "Iteration # 2751  loss is :  1.3226224\n",
      "Iteration # 2752  loss is :  1.3226213\n",
      "Iteration # 2753  loss is :  1.32262\n",
      "Iteration # 2754  loss is :  1.3226191\n",
      "Iteration # 2755  loss is :  1.3226178\n",
      "Iteration # 2756  loss is :  1.3226168\n",
      "Iteration # 2757  loss is :  1.3226156\n",
      "Iteration # 2758  loss is :  1.3226143\n",
      "Iteration # 2759  loss is :  1.3226134\n",
      "Iteration # 2760  loss is :  1.322612\n",
      "Iteration # 2761  loss is :  1.3226111\n",
      "Iteration # 2762  loss is :  1.3226099\n",
      "Iteration # 2763  loss is :  1.3226087\n",
      "Iteration # 2764  loss is :  1.3226076\n",
      "Iteration # 2765  loss is :  1.3226066\n",
      "Iteration # 2766  loss is :  1.3226053\n",
      "Iteration # 2767  loss is :  1.3226042\n",
      "Iteration # 2768  loss is :  1.3226031\n",
      "Iteration # 2769  loss is :  1.322602\n",
      "Iteration # 2770  loss is :  1.3226007\n",
      "Iteration # 2771  loss is :  1.3225998\n",
      "Iteration # 2772  loss is :  1.3225986\n",
      "Iteration # 2773  loss is :  1.3225975\n",
      "Iteration # 2774  loss is :  1.3225963\n",
      "Iteration # 2775  loss is :  1.3225952\n",
      "Iteration # 2776  loss is :  1.3225942\n",
      "Iteration # 2777  loss is :  1.322593\n",
      "Iteration # 2778  loss is :  1.3225919\n",
      "Iteration # 2779  loss is :  1.3225908\n",
      "Iteration # 2780  loss is :  1.3225896\n",
      "Iteration # 2781  loss is :  1.3225884\n",
      "Iteration # 2782  loss is :  1.3225875\n",
      "Iteration # 2783  loss is :  1.3225862\n",
      "Iteration # 2784  loss is :  1.3225851\n",
      "Iteration # 2785  loss is :  1.322584\n",
      "Iteration # 2786  loss is :  1.322583\n",
      "Iteration # 2787  loss is :  1.3225816\n",
      "Iteration # 2788  loss is :  1.3225807\n",
      "Iteration # 2789  loss is :  1.3225795\n",
      "Iteration # 2790  loss is :  1.3225785\n",
      "Iteration # 2791  loss is :  1.3225775\n",
      "Iteration # 2792  loss is :  1.3225762\n",
      "Iteration # 2793  loss is :  1.3225752\n",
      "Iteration # 2794  loss is :  1.322574\n",
      "Iteration # 2795  loss is :  1.3225731\n",
      "Iteration # 2796  loss is :  1.3225718\n",
      "Iteration # 2797  loss is :  1.3225708\n",
      "Iteration # 2798  loss is :  1.3225696\n",
      "Iteration # 2799  loss is :  1.3225685\n",
      "Iteration # 2800  loss is :  1.3225676\n",
      "Iteration # 2801  loss is :  1.3225663\n",
      "Iteration # 2802  loss is :  1.3225653\n",
      "Iteration # 2803  loss is :  1.3225642\n",
      "Iteration # 2804  loss is :  1.3225632\n",
      "Iteration # 2805  loss is :  1.3225622\n",
      "Iteration # 2806  loss is :  1.3225609\n",
      "Iteration # 2807  loss is :  1.3225598\n",
      "Iteration # 2808  loss is :  1.3225586\n",
      "Iteration # 2809  loss is :  1.3225577\n",
      "Iteration # 2810  loss is :  1.3225565\n",
      "Iteration # 2811  loss is :  1.3225555\n",
      "Iteration # 2812  loss is :  1.3225543\n",
      "Iteration # 2813  loss is :  1.3225534\n",
      "Iteration # 2814  loss is :  1.3225522\n",
      "Iteration # 2815  loss is :  1.322551\n",
      "Iteration # 2816  loss is :  1.3225499\n",
      "Iteration # 2817  loss is :  1.3225489\n",
      "Iteration # 2818  loss is :  1.3225479\n",
      "Iteration # 2819  loss is :  1.3225468\n",
      "Iteration # 2820  loss is :  1.3225458\n",
      "Iteration # 2821  loss is :  1.3225447\n",
      "Iteration # 2822  loss is :  1.3225436\n",
      "Iteration # 2823  loss is :  1.3225425\n",
      "Iteration # 2824  loss is :  1.3225415\n",
      "Iteration # 2825  loss is :  1.3225404\n",
      "Iteration # 2826  loss is :  1.3225393\n",
      "Iteration # 2827  loss is :  1.322538\n",
      "Iteration # 2828  loss is :  1.3225373\n",
      "Iteration # 2829  loss is :  1.322536\n",
      "Iteration # 2830  loss is :  1.322535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 2831  loss is :  1.322534\n",
      "Iteration # 2832  loss is :  1.3225328\n",
      "Iteration # 2833  loss is :  1.3225318\n",
      "Iteration # 2834  loss is :  1.3225309\n",
      "Iteration # 2835  loss is :  1.3225297\n",
      "Iteration # 2836  loss is :  1.3225286\n",
      "Iteration # 2837  loss is :  1.3225275\n",
      "Iteration # 2838  loss is :  1.3225266\n",
      "Iteration # 2839  loss is :  1.3225255\n",
      "Iteration # 2840  loss is :  1.3225244\n",
      "Iteration # 2841  loss is :  1.3225234\n",
      "Iteration # 2842  loss is :  1.3225223\n",
      "Iteration # 2843  loss is :  1.3225213\n",
      "Iteration # 2844  loss is :  1.3225203\n",
      "Iteration # 2845  loss is :  1.3225192\n",
      "Iteration # 2846  loss is :  1.3225182\n",
      "Iteration # 2847  loss is :  1.3225172\n",
      "Iteration # 2848  loss is :  1.322516\n",
      "Iteration # 2849  loss is :  1.3225149\n",
      "Iteration # 2850  loss is :  1.3225139\n",
      "Iteration # 2851  loss is :  1.3225129\n",
      "Iteration # 2852  loss is :  1.3225119\n",
      "Iteration # 2853  loss is :  1.3225107\n",
      "Iteration # 2854  loss is :  1.3225098\n",
      "Iteration # 2855  loss is :  1.3225087\n",
      "Iteration # 2856  loss is :  1.3225076\n",
      "Iteration # 2857  loss is :  1.3225067\n",
      "Iteration # 2858  loss is :  1.3225055\n",
      "Iteration # 2859  loss is :  1.3225046\n",
      "Iteration # 2860  loss is :  1.3225033\n",
      "Iteration # 2861  loss is :  1.3225026\n",
      "Iteration # 2862  loss is :  1.3225014\n",
      "Iteration # 2863  loss is :  1.3225003\n",
      "Iteration # 2864  loss is :  1.3224994\n",
      "Iteration # 2865  loss is :  1.3224983\n",
      "Iteration # 2866  loss is :  1.3224974\n",
      "Iteration # 2867  loss is :  1.3224962\n",
      "Iteration # 2868  loss is :  1.3224952\n",
      "Iteration # 2869  loss is :  1.3224941\n",
      "Iteration # 2870  loss is :  1.3224931\n",
      "Iteration # 2871  loss is :  1.3224922\n",
      "Iteration # 2872  loss is :  1.3224913\n",
      "Iteration # 2873  loss is :  1.32249\n",
      "Iteration # 2874  loss is :  1.3224891\n",
      "Iteration # 2875  loss is :  1.322488\n",
      "Iteration # 2876  loss is :  1.322487\n",
      "Iteration # 2877  loss is :  1.322486\n",
      "Iteration # 2878  loss is :  1.3224849\n",
      "Iteration # 2879  loss is :  1.3224839\n",
      "Iteration # 2880  loss is :  1.322483\n",
      "Iteration # 2881  loss is :  1.322482\n",
      "Iteration # 2882  loss is :  1.3224809\n",
      "Iteration # 2883  loss is :  1.3224798\n",
      "Iteration # 2884  loss is :  1.3224788\n",
      "Iteration # 2885  loss is :  1.3224779\n",
      "Iteration # 2886  loss is :  1.3224769\n",
      "Iteration # 2887  loss is :  1.3224759\n",
      "Iteration # 2888  loss is :  1.3224747\n",
      "Iteration # 2889  loss is :  1.3224739\n",
      "Iteration # 2890  loss is :  1.3224727\n",
      "Iteration # 2891  loss is :  1.3224719\n",
      "Iteration # 2892  loss is :  1.3224708\n",
      "Iteration # 2893  loss is :  1.3224698\n",
      "Iteration # 2894  loss is :  1.3224688\n",
      "Iteration # 2895  loss is :  1.3224678\n",
      "Iteration # 2896  loss is :  1.3224667\n",
      "Iteration # 2897  loss is :  1.3224658\n",
      "Iteration # 2898  loss is :  1.3224648\n",
      "Iteration # 2899  loss is :  1.3224636\n",
      "Iteration # 2900  loss is :  1.3224628\n",
      "Iteration # 2901  loss is :  1.3224618\n",
      "Iteration # 2902  loss is :  1.3224608\n",
      "Iteration # 2903  loss is :  1.3224597\n",
      "Iteration # 2904  loss is :  1.3224589\n",
      "Iteration # 2905  loss is :  1.3224577\n",
      "Iteration # 2906  loss is :  1.3224568\n",
      "Iteration # 2907  loss is :  1.3224556\n",
      "Iteration # 2908  loss is :  1.3224547\n",
      "Iteration # 2909  loss is :  1.3224536\n",
      "Iteration # 2910  loss is :  1.3224527\n",
      "Iteration # 2911  loss is :  1.3224517\n",
      "Iteration # 2912  loss is :  1.3224509\n",
      "Iteration # 2913  loss is :  1.3224498\n",
      "Iteration # 2914  loss is :  1.3224488\n",
      "Iteration # 2915  loss is :  1.3224478\n",
      "Iteration # 2916  loss is :  1.3224468\n",
      "Iteration # 2917  loss is :  1.3224459\n",
      "Iteration # 2918  loss is :  1.3224449\n",
      "Iteration # 2919  loss is :  1.3224438\n",
      "Iteration # 2920  loss is :  1.3224429\n",
      "Iteration # 2921  loss is :  1.322442\n",
      "Iteration # 2922  loss is :  1.3224409\n",
      "Iteration # 2923  loss is :  1.3224401\n",
      "Iteration # 2924  loss is :  1.3224391\n",
      "Iteration # 2925  loss is :  1.322438\n",
      "Iteration # 2926  loss is :  1.322437\n",
      "Iteration # 2927  loss is :  1.3224361\n",
      "Iteration # 2928  loss is :  1.322435\n",
      "Iteration # 2929  loss is :  1.3224342\n",
      "Iteration # 2930  loss is :  1.3224331\n",
      "Iteration # 2931  loss is :  1.3224322\n",
      "Iteration # 2932  loss is :  1.3224311\n",
      "Iteration # 2933  loss is :  1.3224304\n",
      "Iteration # 2934  loss is :  1.3224292\n",
      "Iteration # 2935  loss is :  1.3224282\n",
      "Iteration # 2936  loss is :  1.3224274\n",
      "Iteration # 2937  loss is :  1.3224263\n",
      "Iteration # 2938  loss is :  1.3224254\n",
      "Iteration # 2939  loss is :  1.3224244\n",
      "Iteration # 2940  loss is :  1.3224233\n",
      "Iteration # 2941  loss is :  1.3224224\n",
      "Iteration # 2942  loss is :  1.3224216\n",
      "Iteration # 2943  loss is :  1.3224205\n",
      "Iteration # 2944  loss is :  1.3224195\n",
      "Iteration # 2945  loss is :  1.3224187\n",
      "Iteration # 2946  loss is :  1.3224176\n",
      "Iteration # 2947  loss is :  1.3224168\n",
      "Iteration # 2948  loss is :  1.3224157\n",
      "Iteration # 2949  loss is :  1.3224149\n",
      "Iteration # 2950  loss is :  1.3224137\n",
      "Iteration # 2951  loss is :  1.3224128\n",
      "Iteration # 2952  loss is :  1.3224119\n",
      "Iteration # 2953  loss is :  1.322411\n",
      "Iteration # 2954  loss is :  1.32241\n",
      "Iteration # 2955  loss is :  1.322409\n",
      "Iteration # 2956  loss is :  1.3224081\n",
      "Iteration # 2957  loss is :  1.3224072\n",
      "Iteration # 2958  loss is :  1.3224062\n",
      "Iteration # 2959  loss is :  1.3224053\n",
      "Iteration # 2960  loss is :  1.3224041\n",
      "Iteration # 2961  loss is :  1.3224033\n",
      "Iteration # 2962  loss is :  1.3224024\n",
      "Iteration # 2963  loss is :  1.3224014\n",
      "Iteration # 2964  loss is :  1.3224005\n",
      "Iteration # 2965  loss is :  1.3223996\n",
      "Iteration # 2966  loss is :  1.3223987\n",
      "Iteration # 2967  loss is :  1.3223977\n",
      "Iteration # 2968  loss is :  1.3223966\n",
      "Iteration # 2969  loss is :  1.3223959\n",
      "Iteration # 2970  loss is :  1.3223947\n",
      "Iteration # 2971  loss is :  1.322394\n",
      "Iteration # 2972  loss is :  1.322393\n",
      "Iteration # 2973  loss is :  1.3223921\n",
      "Iteration # 2974  loss is :  1.322391\n",
      "Iteration # 2975  loss is :  1.3223902\n",
      "Iteration # 2976  loss is :  1.3223892\n",
      "Iteration # 2977  loss is :  1.3223884\n",
      "Iteration # 2978  loss is :  1.3223873\n",
      "Iteration # 2979  loss is :  1.3223864\n",
      "Iteration # 2980  loss is :  1.3223855\n",
      "Iteration # 2981  loss is :  1.3223845\n",
      "Iteration # 2982  loss is :  1.3223835\n",
      "Iteration # 2983  loss is :  1.3223827\n",
      "Iteration # 2984  loss is :  1.3223817\n",
      "Iteration # 2985  loss is :  1.3223808\n",
      "Iteration # 2986  loss is :  1.32238\n",
      "Iteration # 2987  loss is :  1.3223791\n",
      "Iteration # 2988  loss is :  1.3223782\n",
      "Iteration # 2989  loss is :  1.3223771\n",
      "Iteration # 2990  loss is :  1.322376\n",
      "Iteration # 2991  loss is :  1.3223753\n",
      "Iteration # 2992  loss is :  1.3223745\n",
      "Iteration # 2993  loss is :  1.3223734\n",
      "Iteration # 2994  loss is :  1.3223724\n",
      "Iteration # 2995  loss is :  1.3223716\n",
      "Iteration # 2996  loss is :  1.3223706\n",
      "Iteration # 2997  loss is :  1.3223698\n",
      "Iteration # 2998  loss is :  1.3223689\n",
      "Iteration # 2999  loss is :  1.3223679\n",
      "Iteration # 3000  loss is :  1.3223671\n",
      "Iteration # 3001  loss is :  1.3223659\n",
      "Iteration # 3002  loss is :  1.3223652\n",
      "Iteration # 3003  loss is :  1.3223643\n",
      "Iteration # 3004  loss is :  1.3223635\n",
      "Iteration # 3005  loss is :  1.3223624\n",
      "Iteration # 3006  loss is :  1.3223616\n",
      "Iteration # 3007  loss is :  1.3223606\n",
      "Iteration # 3008  loss is :  1.3223599\n",
      "Iteration # 3009  loss is :  1.3223587\n",
      "Iteration # 3010  loss is :  1.322358\n",
      "Iteration # 3011  loss is :  1.3223569\n",
      "Iteration # 3012  loss is :  1.3223562\n",
      "Iteration # 3013  loss is :  1.322355\n",
      "Iteration # 3014  loss is :  1.3223543\n",
      "Iteration # 3015  loss is :  1.3223535\n",
      "Iteration # 3016  loss is :  1.3223524\n",
      "Iteration # 3017  loss is :  1.3223515\n",
      "Iteration # 3018  loss is :  1.3223505\n",
      "Iteration # 3019  loss is :  1.3223498\n",
      "Iteration # 3020  loss is :  1.3223488\n",
      "Iteration # 3021  loss is :  1.3223479\n",
      "Iteration # 3022  loss is :  1.322347\n",
      "Iteration # 3023  loss is :  1.3223461\n",
      "Iteration # 3024  loss is :  1.3223453\n",
      "Iteration # 3025  loss is :  1.3223444\n",
      "Iteration # 3026  loss is :  1.3223433\n",
      "Iteration # 3027  loss is :  1.3223426\n",
      "Iteration # 3028  loss is :  1.3223416\n",
      "Iteration # 3029  loss is :  1.3223408\n",
      "Iteration # 3030  loss is :  1.3223398\n",
      "Iteration # 3031  loss is :  1.3223389\n",
      "Iteration # 3032  loss is :  1.3223381\n",
      "Iteration # 3033  loss is :  1.3223372\n",
      "Iteration # 3034  loss is :  1.3223363\n",
      "Iteration # 3035  loss is :  1.3223354\n",
      "Iteration # 3036  loss is :  1.3223345\n",
      "Iteration # 3037  loss is :  1.3223336\n",
      "Iteration # 3038  loss is :  1.3223329\n",
      "Iteration # 3039  loss is :  1.3223318\n",
      "Iteration # 3040  loss is :  1.322331\n",
      "Iteration # 3041  loss is :  1.32233\n",
      "Iteration # 3042  loss is :  1.3223292\n",
      "Iteration # 3043  loss is :  1.3223283\n",
      "Iteration # 3044  loss is :  1.3223274\n",
      "Iteration # 3045  loss is :  1.3223264\n",
      "Iteration # 3046  loss is :  1.3223257\n",
      "Iteration # 3047  loss is :  1.3223249\n",
      "Iteration # 3048  loss is :  1.3223239\n",
      "Iteration # 3049  loss is :  1.322323\n",
      "Iteration # 3050  loss is :  1.3223221\n",
      "Iteration # 3051  loss is :  1.3223213\n",
      "Iteration # 3052  loss is :  1.3223205\n",
      "Iteration # 3053  loss is :  1.3223196\n",
      "Iteration # 3054  loss is :  1.3223186\n",
      "Iteration # 3055  loss is :  1.3223177\n",
      "Iteration # 3056  loss is :  1.3223169\n",
      "Iteration # 3057  loss is :  1.3223162\n",
      "Iteration # 3058  loss is :  1.3223152\n",
      "Iteration # 3059  loss is :  1.3223143\n",
      "Iteration # 3060  loss is :  1.3223133\n",
      "Iteration # 3061  loss is :  1.3223124\n",
      "Iteration # 3062  loss is :  1.3223116\n",
      "Iteration # 3063  loss is :  1.3223108\n",
      "Iteration # 3064  loss is :  1.3223099\n",
      "Iteration # 3065  loss is :  1.3223089\n",
      "Iteration # 3066  loss is :  1.3223082\n",
      "Iteration # 3067  loss is :  1.3223073\n",
      "Iteration # 3068  loss is :  1.3223065\n",
      "Iteration # 3069  loss is :  1.3223054\n",
      "Iteration # 3070  loss is :  1.3223047\n",
      "Iteration # 3071  loss is :  1.3223039\n",
      "Iteration # 3072  loss is :  1.3223029\n",
      "Iteration # 3073  loss is :  1.3223021\n",
      "Iteration # 3074  loss is :  1.3223013\n",
      "Iteration # 3075  loss is :  1.3223004\n",
      "Iteration # 3076  loss is :  1.3222995\n",
      "Iteration # 3077  loss is :  1.3222988\n",
      "Iteration # 3078  loss is :  1.3222978\n",
      "Iteration # 3079  loss is :  1.3222971\n",
      "Iteration # 3080  loss is :  1.3222961\n",
      "Iteration # 3081  loss is :  1.3222952\n",
      "Iteration # 3082  loss is :  1.3222944\n",
      "Iteration # 3083  loss is :  1.3222935\n",
      "Iteration # 3084  loss is :  1.3222927\n",
      "Iteration # 3085  loss is :  1.3222919\n",
      "Iteration # 3086  loss is :  1.3222909\n",
      "Iteration # 3087  loss is :  1.3222901\n",
      "Iteration # 3088  loss is :  1.3222893\n",
      "Iteration # 3089  loss is :  1.3222885\n",
      "Iteration # 3090  loss is :  1.3222876\n",
      "Iteration # 3091  loss is :  1.3222867\n",
      "Iteration # 3092  loss is :  1.3222858\n",
      "Iteration # 3093  loss is :  1.3222849\n",
      "Iteration # 3094  loss is :  1.3222841\n",
      "Iteration # 3095  loss is :  1.3222834\n",
      "Iteration # 3096  loss is :  1.3222824\n",
      "Iteration # 3097  loss is :  1.3222817\n",
      "Iteration # 3098  loss is :  1.3222806\n",
      "Iteration # 3099  loss is :  1.3222799\n",
      "Iteration # 3100  loss is :  1.3222792\n",
      "Iteration # 3101  loss is :  1.3222781\n",
      "Iteration # 3102  loss is :  1.3222774\n",
      "Iteration # 3103  loss is :  1.3222765\n",
      "Iteration # 3104  loss is :  1.3222759\n",
      "Iteration # 3105  loss is :  1.3222749\n",
      "Iteration # 3106  loss is :  1.3222741\n",
      "Iteration # 3107  loss is :  1.3222733\n",
      "Iteration # 3108  loss is :  1.3222723\n",
      "Iteration # 3109  loss is :  1.3222716\n",
      "Iteration # 3110  loss is :  1.3222708\n",
      "Iteration # 3111  loss is :  1.3222699\n",
      "Iteration # 3112  loss is :  1.3222691\n",
      "Iteration # 3113  loss is :  1.3222682\n",
      "Iteration # 3114  loss is :  1.3222673\n",
      "Iteration # 3115  loss is :  1.3222666\n",
      "Iteration # 3116  loss is :  1.3222657\n",
      "Iteration # 3117  loss is :  1.3222649\n",
      "Iteration # 3118  loss is :  1.3222641\n",
      "Iteration # 3119  loss is :  1.3222631\n",
      "Iteration # 3120  loss is :  1.3222623\n",
      "Iteration # 3121  loss is :  1.3222616\n",
      "Iteration # 3122  loss is :  1.3222607\n",
      "Iteration # 3123  loss is :  1.3222599\n",
      "Iteration # 3124  loss is :  1.3222591\n",
      "Iteration # 3125  loss is :  1.3222584\n",
      "Iteration # 3126  loss is :  1.3222575\n",
      "Iteration # 3127  loss is :  1.3222566\n",
      "Iteration # 3128  loss is :  1.3222556\n",
      "Iteration # 3129  loss is :  1.322255\n",
      "Iteration # 3130  loss is :  1.3222543\n",
      "Iteration # 3131  loss is :  1.3222532\n",
      "Iteration # 3132  loss is :  1.3222525\n",
      "Iteration # 3133  loss is :  1.3222517\n",
      "Iteration # 3134  loss is :  1.3222508\n",
      "Iteration # 3135  loss is :  1.32225\n",
      "Iteration # 3136  loss is :  1.3222493\n",
      "Iteration # 3137  loss is :  1.3222485\n",
      "Iteration # 3138  loss is :  1.3222476\n",
      "Iteration # 3139  loss is :  1.3222469\n",
      "Iteration # 3140  loss is :  1.3222461\n",
      "Iteration # 3141  loss is :  1.3222452\n",
      "Iteration # 3142  loss is :  1.3222443\n",
      "Iteration # 3143  loss is :  1.3222436\n",
      "Iteration # 3144  loss is :  1.3222426\n",
      "Iteration # 3145  loss is :  1.3222418\n",
      "Iteration # 3146  loss is :  1.3222411\n",
      "Iteration # 3147  loss is :  1.3222402\n",
      "Iteration # 3148  loss is :  1.3222395\n",
      "Iteration # 3149  loss is :  1.3222387\n",
      "Iteration # 3150  loss is :  1.322238\n",
      "Iteration # 3151  loss is :  1.322237\n",
      "Iteration # 3152  loss is :  1.3222362\n",
      "Iteration # 3153  loss is :  1.3222355\n",
      "Iteration # 3154  loss is :  1.3222346\n",
      "Iteration # 3155  loss is :  1.3222338\n",
      "Iteration # 3156  loss is :  1.3222331\n",
      "Iteration # 3157  loss is :  1.3222322\n",
      "Iteration # 3158  loss is :  1.3222314\n",
      "Iteration # 3159  loss is :  1.3222307\n",
      "Iteration # 3160  loss is :  1.32223\n",
      "Iteration # 3161  loss is :  1.3222289\n",
      "Iteration # 3162  loss is :  1.3222282\n",
      "Iteration # 3163  loss is :  1.3222274\n",
      "Iteration # 3164  loss is :  1.3222265\n",
      "Iteration # 3165  loss is :  1.3222259\n",
      "Iteration # 3166  loss is :  1.3222251\n",
      "Iteration # 3167  loss is :  1.3222243\n",
      "Iteration # 3168  loss is :  1.3222233\n",
      "Iteration # 3169  loss is :  1.3222227\n",
      "Iteration # 3170  loss is :  1.3222219\n",
      "Iteration # 3171  loss is :  1.3222209\n",
      "Iteration # 3172  loss is :  1.3222203\n",
      "Iteration # 3173  loss is :  1.3222193\n",
      "Iteration # 3174  loss is :  1.3222187\n",
      "Iteration # 3175  loss is :  1.3222178\n",
      "Iteration # 3176  loss is :  1.322217\n",
      "Iteration # 3177  loss is :  1.3222164\n",
      "Iteration # 3178  loss is :  1.3222156\n",
      "Iteration # 3179  loss is :  1.3222148\n",
      "Iteration # 3180  loss is :  1.322214\n",
      "Iteration # 3181  loss is :  1.322213\n",
      "Iteration # 3182  loss is :  1.3222122\n",
      "Iteration # 3183  loss is :  1.3222115\n",
      "Iteration # 3184  loss is :  1.3222108\n",
      "Iteration # 3185  loss is :  1.3222098\n",
      "Iteration # 3186  loss is :  1.3222092\n",
      "Iteration # 3187  loss is :  1.3222084\n",
      "Iteration # 3188  loss is :  1.3222076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 3189  loss is :  1.3222069\n",
      "Iteration # 3190  loss is :  1.322206\n",
      "Iteration # 3191  loss is :  1.3222053\n",
      "Iteration # 3192  loss is :  1.3222045\n",
      "Iteration # 3193  loss is :  1.3222036\n",
      "Iteration # 3194  loss is :  1.3222029\n",
      "Iteration # 3195  loss is :  1.3222021\n",
      "Iteration # 3196  loss is :  1.3222013\n",
      "Iteration # 3197  loss is :  1.3222005\n",
      "Iteration # 3198  loss is :  1.3221997\n",
      "Iteration # 3199  loss is :  1.3221989\n",
      "Iteration # 3200  loss is :  1.3221983\n",
      "Iteration # 3201  loss is :  1.3221974\n",
      "Iteration # 3202  loss is :  1.3221966\n",
      "Iteration # 3203  loss is :  1.322196\n",
      "Iteration # 3204  loss is :  1.3221952\n",
      "Iteration # 3205  loss is :  1.3221945\n",
      "Iteration # 3206  loss is :  1.3221936\n",
      "Iteration # 3207  loss is :  1.3221928\n",
      "Iteration # 3208  loss is :  1.3221922\n",
      "Iteration # 3209  loss is :  1.3221914\n",
      "Iteration # 3210  loss is :  1.3221906\n",
      "Iteration # 3211  loss is :  1.3221897\n",
      "Iteration # 3212  loss is :  1.3221889\n",
      "Iteration # 3213  loss is :  1.3221883\n",
      "Iteration # 3214  loss is :  1.3221874\n",
      "Iteration # 3215  loss is :  1.3221866\n",
      "Iteration # 3216  loss is :  1.3221859\n",
      "Iteration # 3217  loss is :  1.322185\n",
      "Iteration # 3218  loss is :  1.3221843\n",
      "Iteration # 3219  loss is :  1.3221836\n",
      "Iteration # 3220  loss is :  1.3221828\n",
      "Iteration # 3221  loss is :  1.3221822\n",
      "Iteration # 3222  loss is :  1.3221813\n",
      "Iteration # 3223  loss is :  1.3221806\n",
      "Iteration # 3224  loss is :  1.3221798\n",
      "Iteration # 3225  loss is :  1.3221791\n",
      "Iteration # 3226  loss is :  1.3221782\n",
      "Iteration # 3227  loss is :  1.3221774\n",
      "Iteration # 3228  loss is :  1.3221769\n",
      "Iteration # 3229  loss is :  1.3221757\n",
      "Iteration # 3230  loss is :  1.322175\n",
      "Iteration # 3231  loss is :  1.3221745\n",
      "Iteration # 3232  loss is :  1.3221736\n",
      "Iteration # 3233  loss is :  1.3221729\n",
      "Iteration # 3234  loss is :  1.3221722\n",
      "Iteration # 3235  loss is :  1.3221713\n",
      "Iteration # 3236  loss is :  1.3221707\n",
      "Iteration # 3237  loss is :  1.3221699\n",
      "Iteration # 3238  loss is :  1.3221692\n",
      "Iteration # 3239  loss is :  1.3221684\n",
      "Iteration # 3240  loss is :  1.3221676\n",
      "Iteration # 3241  loss is :  1.322167\n",
      "Iteration # 3242  loss is :  1.3221662\n",
      "Iteration # 3243  loss is :  1.3221654\n",
      "Iteration # 3244  loss is :  1.3221647\n",
      "Iteration # 3245  loss is :  1.3221638\n",
      "Iteration # 3246  loss is :  1.3221633\n",
      "Iteration # 3247  loss is :  1.3221625\n",
      "Iteration # 3248  loss is :  1.3221617\n",
      "Iteration # 3249  loss is :  1.322161\n",
      "Iteration # 3250  loss is :  1.3221601\n",
      "Iteration # 3251  loss is :  1.3221594\n",
      "Iteration # 3252  loss is :  1.3221587\n",
      "Iteration # 3253  loss is :  1.322158\n",
      "Iteration # 3254  loss is :  1.3221573\n",
      "Iteration # 3255  loss is :  1.3221564\n",
      "Iteration # 3256  loss is :  1.3221557\n",
      "Iteration # 3257  loss is :  1.322155\n",
      "Iteration # 3258  loss is :  1.3221542\n",
      "Iteration # 3259  loss is :  1.3221534\n",
      "Iteration # 3260  loss is :  1.3221526\n",
      "Iteration # 3261  loss is :  1.322152\n",
      "Iteration # 3262  loss is :  1.3221512\n",
      "Iteration # 3263  loss is :  1.3221505\n",
      "Iteration # 3264  loss is :  1.3221498\n",
      "Iteration # 3265  loss is :  1.3221489\n",
      "Iteration # 3266  loss is :  1.3221483\n",
      "Iteration # 3267  loss is :  1.3221475\n",
      "Iteration # 3268  loss is :  1.3221468\n",
      "Iteration # 3269  loss is :  1.322146\n",
      "Iteration # 3270  loss is :  1.3221453\n",
      "Iteration # 3271  loss is :  1.3221445\n",
      "Iteration # 3272  loss is :  1.3221439\n",
      "Iteration # 3273  loss is :  1.3221431\n",
      "Iteration # 3274  loss is :  1.3221424\n",
      "Iteration # 3275  loss is :  1.3221418\n",
      "Iteration # 3276  loss is :  1.3221408\n",
      "Iteration # 3277  loss is :  1.3221402\n",
      "Iteration # 3278  loss is :  1.3221395\n",
      "Iteration # 3279  loss is :  1.3221388\n",
      "Iteration # 3280  loss is :  1.3221381\n",
      "Iteration # 3281  loss is :  1.3221372\n",
      "Iteration # 3282  loss is :  1.3221366\n",
      "Iteration # 3283  loss is :  1.3221358\n",
      "Iteration # 3284  loss is :  1.3221351\n",
      "Iteration # 3285  loss is :  1.3221344\n",
      "Iteration # 3286  loss is :  1.3221335\n",
      "Iteration # 3287  loss is :  1.3221331\n",
      "Iteration # 3288  loss is :  1.3221322\n",
      "Iteration # 3289  loss is :  1.3221314\n",
      "Iteration # 3290  loss is :  1.3221308\n",
      "Iteration # 3291  loss is :  1.3221301\n",
      "Iteration # 3292  loss is :  1.3221292\n",
      "Iteration # 3293  loss is :  1.3221287\n",
      "Iteration # 3294  loss is :  1.3221278\n",
      "Iteration # 3295  loss is :  1.3221272\n",
      "Iteration # 3296  loss is :  1.3221264\n",
      "Iteration # 3297  loss is :  1.3221256\n",
      "Iteration # 3298  loss is :  1.322125\n",
      "Iteration # 3299  loss is :  1.3221244\n",
      "Iteration # 3300  loss is :  1.3221235\n",
      "Iteration # 3301  loss is :  1.3221228\n",
      "Iteration # 3302  loss is :  1.3221221\n",
      "Iteration # 3303  loss is :  1.3221214\n",
      "Iteration # 3304  loss is :  1.3221207\n",
      "Iteration # 3305  loss is :  1.32212\n",
      "Iteration # 3306  loss is :  1.3221194\n",
      "Iteration # 3307  loss is :  1.3221186\n",
      "Iteration # 3308  loss is :  1.3221178\n",
      "Iteration # 3309  loss is :  1.3221171\n",
      "Iteration # 3310  loss is :  1.3221163\n",
      "Iteration # 3311  loss is :  1.3221157\n",
      "Iteration # 3312  loss is :  1.322115\n",
      "Iteration # 3313  loss is :  1.3221141\n",
      "Iteration # 3314  loss is :  1.3221136\n",
      "Iteration # 3315  loss is :  1.3221128\n",
      "Iteration # 3316  loss is :  1.3221121\n",
      "Iteration # 3317  loss is :  1.3221115\n",
      "Iteration # 3318  loss is :  1.3221107\n",
      "Iteration # 3319  loss is :  1.3221102\n",
      "Iteration # 3320  loss is :  1.3221095\n",
      "Iteration # 3321  loss is :  1.3221085\n",
      "Iteration # 3322  loss is :  1.3221079\n",
      "Iteration # 3323  loss is :  1.3221072\n",
      "Iteration # 3324  loss is :  1.3221065\n",
      "Iteration # 3325  loss is :  1.3221059\n",
      "Iteration # 3326  loss is :  1.3221052\n",
      "Iteration # 3327  loss is :  1.3221045\n",
      "Iteration # 3328  loss is :  1.3221039\n",
      "Iteration # 3329  loss is :  1.322103\n",
      "Iteration # 3330  loss is :  1.3221023\n",
      "Iteration # 3331  loss is :  1.3221017\n",
      "Iteration # 3332  loss is :  1.3221009\n",
      "Iteration # 3333  loss is :  1.3221002\n",
      "Iteration # 3334  loss is :  1.3220996\n",
      "Iteration # 3335  loss is :  1.3220987\n",
      "Iteration # 3336  loss is :  1.322098\n",
      "Iteration # 3337  loss is :  1.3220973\n",
      "Iteration # 3338  loss is :  1.3220967\n",
      "Iteration # 3339  loss is :  1.322096\n",
      "Iteration # 3340  loss is :  1.3220953\n",
      "Iteration # 3341  loss is :  1.3220946\n",
      "Iteration # 3342  loss is :  1.3220938\n",
      "Iteration # 3343  loss is :  1.3220931\n",
      "Iteration # 3344  loss is :  1.3220924\n",
      "Iteration # 3345  loss is :  1.3220917\n",
      "Iteration # 3346  loss is :  1.3220911\n",
      "Iteration # 3347  loss is :  1.3220904\n",
      "Iteration # 3348  loss is :  1.3220896\n",
      "Iteration # 3349  loss is :  1.3220891\n",
      "Iteration # 3350  loss is :  1.3220884\n",
      "Iteration # 3351  loss is :  1.3220876\n",
      "Iteration # 3352  loss is :  1.322087\n",
      "Iteration # 3353  loss is :  1.3220863\n",
      "Iteration # 3354  loss is :  1.3220856\n",
      "Iteration # 3355  loss is :  1.322085\n",
      "Iteration # 3356  loss is :  1.3220842\n",
      "Iteration # 3357  loss is :  1.3220836\n",
      "Iteration # 3358  loss is :  1.3220829\n",
      "Iteration # 3359  loss is :  1.322082\n",
      "Iteration # 3360  loss is :  1.3220816\n",
      "Iteration # 3361  loss is :  1.3220807\n",
      "Iteration # 3362  loss is :  1.3220801\n",
      "Iteration # 3363  loss is :  1.3220794\n",
      "Iteration # 3364  loss is :  1.3220786\n",
      "Iteration # 3365  loss is :  1.322078\n",
      "Iteration # 3366  loss is :  1.3220773\n",
      "Iteration # 3367  loss is :  1.3220767\n",
      "Iteration # 3368  loss is :  1.3220758\n",
      "Iteration # 3369  loss is :  1.3220754\n",
      "Iteration # 3370  loss is :  1.3220747\n",
      "Iteration # 3371  loss is :  1.322074\n",
      "Iteration # 3372  loss is :  1.3220733\n",
      "Iteration # 3373  loss is :  1.3220725\n",
      "Iteration # 3374  loss is :  1.322072\n",
      "Iteration # 3375  loss is :  1.3220713\n",
      "Iteration # 3376  loss is :  1.3220705\n",
      "Iteration # 3377  loss is :  1.3220699\n",
      "Iteration # 3378  loss is :  1.3220693\n",
      "Iteration # 3379  loss is :  1.3220685\n",
      "Iteration # 3380  loss is :  1.3220677\n",
      "Iteration # 3381  loss is :  1.3220671\n",
      "Iteration # 3382  loss is :  1.3220664\n",
      "Iteration # 3383  loss is :  1.3220658\n",
      "Iteration # 3384  loss is :  1.322065\n",
      "Iteration # 3385  loss is :  1.3220644\n",
      "Iteration # 3386  loss is :  1.3220639\n",
      "Iteration # 3387  loss is :  1.3220632\n",
      "Iteration # 3388  loss is :  1.3220625\n",
      "Iteration # 3389  loss is :  1.3220618\n",
      "Iteration # 3390  loss is :  1.3220611\n",
      "Iteration # 3391  loss is :  1.3220606\n",
      "Iteration # 3392  loss is :  1.3220598\n",
      "Iteration # 3393  loss is :  1.322059\n",
      "Iteration # 3394  loss is :  1.3220584\n",
      "Iteration # 3395  loss is :  1.3220578\n",
      "Iteration # 3396  loss is :  1.322057\n",
      "Iteration # 3397  loss is :  1.3220563\n",
      "Iteration # 3398  loss is :  1.3220557\n",
      "Iteration # 3399  loss is :  1.3220549\n",
      "Iteration # 3400  loss is :  1.3220545\n",
      "Iteration # 3401  loss is :  1.3220538\n",
      "Iteration # 3402  loss is :  1.3220531\n",
      "Iteration # 3403  loss is :  1.3220525\n",
      "Iteration # 3404  loss is :  1.3220518\n",
      "Iteration # 3405  loss is :  1.322051\n",
      "Iteration # 3406  loss is :  1.3220505\n",
      "Iteration # 3407  loss is :  1.3220497\n",
      "Iteration # 3408  loss is :  1.3220491\n",
      "Iteration # 3409  loss is :  1.3220484\n",
      "Iteration # 3410  loss is :  1.3220477\n",
      "Iteration # 3411  loss is :  1.3220472\n",
      "Iteration # 3412  loss is :  1.3220466\n",
      "Iteration # 3413  loss is :  1.3220459\n",
      "Iteration # 3414  loss is :  1.3220452\n",
      "Iteration # 3415  loss is :  1.3220446\n",
      "Iteration # 3416  loss is :  1.3220438\n",
      "Iteration # 3417  loss is :  1.3220433\n",
      "Iteration # 3418  loss is :  1.3220425\n",
      "Iteration # 3419  loss is :  1.3220419\n",
      "Iteration # 3420  loss is :  1.3220412\n",
      "Iteration # 3421  loss is :  1.3220404\n",
      "Iteration # 3422  loss is :  1.3220398\n",
      "Iteration # 3423  loss is :  1.3220394\n",
      "Iteration # 3424  loss is :  1.3220388\n",
      "Iteration # 3425  loss is :  1.3220379\n",
      "Iteration # 3426  loss is :  1.3220373\n",
      "Iteration # 3427  loss is :  1.3220366\n",
      "Iteration # 3428  loss is :  1.3220359\n",
      "Iteration # 3429  loss is :  1.3220353\n",
      "Iteration # 3430  loss is :  1.3220346\n",
      "Iteration # 3431  loss is :  1.322034\n",
      "Iteration # 3432  loss is :  1.3220334\n",
      "Iteration # 3433  loss is :  1.3220327\n",
      "Iteration # 3434  loss is :  1.3220322\n",
      "Iteration # 3435  loss is :  1.3220316\n",
      "Iteration # 3436  loss is :  1.3220308\n",
      "Iteration # 3437  loss is :  1.3220301\n",
      "Iteration # 3438  loss is :  1.3220295\n",
      "Iteration # 3439  loss is :  1.3220289\n",
      "Iteration # 3440  loss is :  1.3220282\n",
      "Iteration # 3441  loss is :  1.3220276\n",
      "Iteration # 3442  loss is :  1.3220268\n",
      "Iteration # 3443  loss is :  1.3220263\n",
      "Iteration # 3444  loss is :  1.3220258\n",
      "Iteration # 3445  loss is :  1.322025\n",
      "Iteration # 3446  loss is :  1.3220243\n",
      "Iteration # 3447  loss is :  1.3220237\n",
      "Iteration # 3448  loss is :  1.322023\n",
      "Iteration # 3449  loss is :  1.3220223\n",
      "Iteration # 3450  loss is :  1.3220218\n",
      "Iteration # 3451  loss is :  1.322021\n",
      "Iteration # 3452  loss is :  1.3220205\n",
      "Iteration # 3453  loss is :  1.3220199\n",
      "Iteration # 3454  loss is :  1.3220191\n",
      "Iteration # 3455  loss is :  1.3220186\n",
      "Iteration # 3456  loss is :  1.322018\n",
      "Iteration # 3457  loss is :  1.3220172\n",
      "Iteration # 3458  loss is :  1.3220167\n",
      "Iteration # 3459  loss is :  1.3220161\n",
      "Iteration # 3460  loss is :  1.3220153\n",
      "Iteration # 3461  loss is :  1.3220147\n",
      "Iteration # 3462  loss is :  1.3220141\n",
      "Iteration # 3463  loss is :  1.3220135\n",
      "Iteration # 3464  loss is :  1.3220129\n",
      "Iteration # 3465  loss is :  1.3220122\n",
      "Iteration # 3466  loss is :  1.3220115\n",
      "Iteration # 3467  loss is :  1.322011\n",
      "Iteration # 3468  loss is :  1.3220103\n",
      "Iteration # 3469  loss is :  1.3220097\n",
      "Iteration # 3470  loss is :  1.3220091\n",
      "Iteration # 3471  loss is :  1.3220085\n",
      "Iteration # 3472  loss is :  1.3220077\n",
      "Iteration # 3473  loss is :  1.3220072\n",
      "Iteration # 3474  loss is :  1.3220066\n",
      "Iteration # 3475  loss is :  1.322006\n",
      "Iteration # 3476  loss is :  1.3220053\n",
      "Iteration # 3477  loss is :  1.3220046\n",
      "Iteration # 3478  loss is :  1.322004\n",
      "Iteration # 3479  loss is :  1.3220034\n",
      "Iteration # 3480  loss is :  1.3220028\n",
      "Iteration # 3481  loss is :  1.322002\n",
      "Iteration # 3482  loss is :  1.3220015\n",
      "Iteration # 3483  loss is :  1.3220007\n",
      "Iteration # 3484  loss is :  1.3220001\n",
      "Iteration # 3485  loss is :  1.3219995\n",
      "Iteration # 3486  loss is :  1.3219991\n",
      "Iteration # 3487  loss is :  1.3219984\n",
      "Iteration # 3488  loss is :  1.3219978\n",
      "Iteration # 3489  loss is :  1.3219972\n",
      "Iteration # 3490  loss is :  1.3219965\n",
      "Iteration # 3491  loss is :  1.3219957\n",
      "Iteration # 3492  loss is :  1.3219951\n",
      "Iteration # 3493  loss is :  1.3219945\n",
      "Iteration # 3494  loss is :  1.3219941\n",
      "Iteration # 3495  loss is :  1.3219934\n",
      "Iteration # 3496  loss is :  1.3219928\n",
      "Iteration # 3497  loss is :  1.321992\n",
      "Iteration # 3498  loss is :  1.3219914\n",
      "Iteration # 3499  loss is :  1.3219908\n",
      "Iteration # 3500  loss is :  1.3219903\n",
      "Iteration # 3501  loss is :  1.3219898\n",
      "Iteration # 3502  loss is :  1.321989\n",
      "Iteration # 3503  loss is :  1.3219885\n",
      "Iteration # 3504  loss is :  1.3219879\n",
      "Iteration # 3505  loss is :  1.3219872\n",
      "Iteration # 3506  loss is :  1.3219864\n",
      "Iteration # 3507  loss is :  1.3219861\n",
      "Iteration # 3508  loss is :  1.3219852\n",
      "Iteration # 3509  loss is :  1.3219848\n",
      "Iteration # 3510  loss is :  1.321984\n",
      "Iteration # 3511  loss is :  1.3219836\n",
      "Iteration # 3512  loss is :  1.3219829\n",
      "Iteration # 3513  loss is :  1.3219823\n",
      "Iteration # 3514  loss is :  1.3219817\n",
      "Iteration # 3515  loss is :  1.3219811\n",
      "Iteration # 3516  loss is :  1.3219805\n",
      "Iteration # 3517  loss is :  1.3219799\n",
      "Iteration # 3518  loss is :  1.3219792\n",
      "Iteration # 3519  loss is :  1.3219786\n",
      "Iteration # 3520  loss is :  1.3219779\n",
      "Iteration # 3521  loss is :  1.3219774\n",
      "Iteration # 3522  loss is :  1.3219768\n",
      "Iteration # 3523  loss is :  1.3219762\n",
      "Iteration # 3524  loss is :  1.3219755\n",
      "Iteration # 3525  loss is :  1.3219749\n",
      "Iteration # 3526  loss is :  1.3219743\n",
      "Iteration # 3527  loss is :  1.3219738\n",
      "Iteration # 3528  loss is :  1.3219731\n",
      "Iteration # 3529  loss is :  1.3219726\n",
      "Iteration # 3530  loss is :  1.3219719\n",
      "Iteration # 3531  loss is :  1.3219712\n",
      "Iteration # 3532  loss is :  1.3219707\n",
      "Iteration # 3533  loss is :  1.32197\n",
      "Iteration # 3534  loss is :  1.3219695\n",
      "Iteration # 3535  loss is :  1.321969\n",
      "Iteration # 3536  loss is :  1.3219683\n",
      "Iteration # 3537  loss is :  1.3219677\n",
      "Iteration # 3538  loss is :  1.3219671\n",
      "Iteration # 3539  loss is :  1.3219664\n",
      "Iteration # 3540  loss is :  1.3219658\n",
      "Iteration # 3541  loss is :  1.3219652\n",
      "Iteration # 3542  loss is :  1.3219646\n",
      "Iteration # 3543  loss is :  1.321964\n",
      "Iteration # 3544  loss is :  1.3219634\n",
      "Iteration # 3545  loss is :  1.3219628\n",
      "Iteration # 3546  loss is :  1.3219622\n",
      "Iteration # 3547  loss is :  1.3219616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 3548  loss is :  1.3219612\n",
      "Iteration # 3549  loss is :  1.3219604\n",
      "Iteration # 3550  loss is :  1.3219599\n",
      "Iteration # 3551  loss is :  1.3219593\n",
      "Iteration # 3552  loss is :  1.3219588\n",
      "Iteration # 3553  loss is :  1.3219581\n",
      "Iteration # 3554  loss is :  1.3219575\n",
      "Iteration # 3555  loss is :  1.321957\n",
      "Iteration # 3556  loss is :  1.3219563\n",
      "Iteration # 3557  loss is :  1.3219557\n",
      "Iteration # 3558  loss is :  1.3219552\n",
      "Iteration # 3559  loss is :  1.3219545\n",
      "Iteration # 3560  loss is :  1.3219539\n",
      "Iteration # 3561  loss is :  1.3219534\n",
      "Iteration # 3562  loss is :  1.3219527\n",
      "Iteration # 3563  loss is :  1.3219521\n",
      "Iteration # 3564  loss is :  1.3219516\n",
      "Iteration # 3565  loss is :  1.321951\n",
      "Iteration # 3566  loss is :  1.3219504\n",
      "Iteration # 3567  loss is :  1.3219498\n",
      "Iteration # 3568  loss is :  1.3219491\n",
      "Iteration # 3569  loss is :  1.3219486\n",
      "Iteration # 3570  loss is :  1.321948\n",
      "Iteration # 3571  loss is :  1.3219473\n",
      "Iteration # 3572  loss is :  1.3219469\n",
      "Iteration # 3573  loss is :  1.3219461\n",
      "Iteration # 3574  loss is :  1.3219457\n",
      "Iteration # 3575  loss is :  1.3219452\n",
      "Iteration # 3576  loss is :  1.3219446\n",
      "Iteration # 3577  loss is :  1.3219439\n",
      "Iteration # 3578  loss is :  1.3219433\n",
      "Iteration # 3579  loss is :  1.3219427\n",
      "Iteration # 3580  loss is :  1.3219423\n",
      "Iteration # 3581  loss is :  1.3219416\n",
      "Iteration # 3582  loss is :  1.321941\n",
      "Iteration # 3583  loss is :  1.3219404\n",
      "Iteration # 3584  loss is :  1.3219398\n",
      "Iteration # 3585  loss is :  1.3219393\n",
      "Iteration # 3586  loss is :  1.3219388\n",
      "Iteration # 3587  loss is :  1.3219382\n",
      "Iteration # 3588  loss is :  1.3219374\n",
      "Iteration # 3589  loss is :  1.3219368\n",
      "Iteration # 3590  loss is :  1.3219364\n",
      "Iteration # 3591  loss is :  1.3219359\n",
      "Iteration # 3592  loss is :  1.3219353\n",
      "Iteration # 3593  loss is :  1.3219346\n",
      "Iteration # 3594  loss is :  1.321934\n",
      "Iteration # 3595  loss is :  1.3219334\n",
      "Iteration # 3596  loss is :  1.3219329\n",
      "Iteration # 3597  loss is :  1.3219324\n",
      "Iteration # 3598  loss is :  1.3219316\n",
      "Iteration # 3599  loss is :  1.3219311\n",
      "Iteration # 3600  loss is :  1.3219305\n",
      "Iteration # 3601  loss is :  1.32193\n",
      "Iteration # 3602  loss is :  1.3219295\n",
      "Iteration # 3603  loss is :  1.3219289\n",
      "Iteration # 3604  loss is :  1.3219281\n",
      "Iteration # 3605  loss is :  1.3219275\n",
      "Iteration # 3606  loss is :  1.3219271\n",
      "Iteration # 3607  loss is :  1.3219266\n",
      "Iteration # 3608  loss is :  1.321926\n",
      "Iteration # 3609  loss is :  1.3219254\n",
      "Iteration # 3610  loss is :  1.3219248\n",
      "Iteration # 3611  loss is :  1.3219242\n",
      "Iteration # 3612  loss is :  1.3219236\n",
      "Iteration # 3613  loss is :  1.3219231\n",
      "Iteration # 3614  loss is :  1.3219225\n",
      "Iteration # 3615  loss is :  1.3219218\n",
      "Iteration # 3616  loss is :  1.3219213\n",
      "Iteration # 3617  loss is :  1.3219208\n",
      "Iteration # 3618  loss is :  1.3219203\n",
      "Iteration # 3619  loss is :  1.3219196\n",
      "Iteration # 3620  loss is :  1.3219191\n",
      "Iteration # 3621  loss is :  1.3219186\n",
      "Iteration # 3622  loss is :  1.321918\n",
      "Iteration # 3623  loss is :  1.3219174\n",
      "Iteration # 3624  loss is :  1.3219168\n",
      "Iteration # 3625  loss is :  1.3219162\n",
      "Iteration # 3626  loss is :  1.3219157\n",
      "Iteration # 3627  loss is :  1.3219151\n",
      "Iteration # 3628  loss is :  1.3219146\n",
      "Iteration # 3629  loss is :  1.321914\n",
      "Iteration # 3630  loss is :  1.3219135\n",
      "Iteration # 3631  loss is :  1.3219129\n",
      "Iteration # 3632  loss is :  1.3219123\n",
      "Iteration # 3633  loss is :  1.3219118\n",
      "Iteration # 3634  loss is :  1.3219112\n",
      "Iteration # 3635  loss is :  1.3219106\n",
      "Iteration # 3636  loss is :  1.32191\n",
      "Iteration # 3637  loss is :  1.3219095\n",
      "Iteration # 3638  loss is :  1.321909\n",
      "Iteration # 3639  loss is :  1.3219084\n",
      "Iteration # 3640  loss is :  1.321908\n",
      "Iteration # 3641  loss is :  1.3219073\n",
      "Iteration # 3642  loss is :  1.3219067\n",
      "Iteration # 3643  loss is :  1.3219061\n",
      "Iteration # 3644  loss is :  1.3219056\n",
      "Iteration # 3645  loss is :  1.321905\n",
      "Iteration # 3646  loss is :  1.3219044\n",
      "Iteration # 3647  loss is :  1.321904\n",
      "Iteration # 3648  loss is :  1.3219032\n",
      "Iteration # 3649  loss is :  1.3219029\n",
      "Iteration # 3650  loss is :  1.3219023\n",
      "Iteration # 3651  loss is :  1.3219017\n",
      "Iteration # 3652  loss is :  1.3219012\n",
      "Iteration # 3653  loss is :  1.3219006\n",
      "Iteration # 3654  loss is :  1.3219001\n",
      "Iteration # 3655  loss is :  1.3218994\n",
      "Iteration # 3656  loss is :  1.3218988\n",
      "Iteration # 3657  loss is :  1.3218983\n",
      "Iteration # 3658  loss is :  1.3218979\n",
      "Iteration # 3659  loss is :  1.3218973\n",
      "Iteration # 3660  loss is :  1.3218967\n",
      "Iteration # 3661  loss is :  1.3218961\n",
      "Iteration # 3662  loss is :  1.3218957\n",
      "Iteration # 3663  loss is :  1.321895\n",
      "Iteration # 3664  loss is :  1.3218945\n",
      "Iteration # 3665  loss is :  1.3218939\n",
      "Iteration # 3666  loss is :  1.3218933\n",
      "Iteration # 3667  loss is :  1.3218927\n",
      "Iteration # 3668  loss is :  1.3218924\n",
      "Iteration # 3669  loss is :  1.3218917\n",
      "Iteration # 3670  loss is :  1.3218911\n",
      "Iteration # 3671  loss is :  1.3218907\n",
      "Iteration # 3672  loss is :  1.3218901\n",
      "Iteration # 3673  loss is :  1.3218894\n",
      "Iteration # 3674  loss is :  1.3218889\n",
      "Iteration # 3675  loss is :  1.3218886\n",
      "Iteration # 3676  loss is :  1.321888\n",
      "Iteration # 3677  loss is :  1.3218874\n",
      "Iteration # 3678  loss is :  1.3218868\n",
      "Iteration # 3679  loss is :  1.3218864\n",
      "Iteration # 3680  loss is :  1.3218856\n",
      "Iteration # 3681  loss is :  1.3218852\n",
      "Iteration # 3682  loss is :  1.3218845\n",
      "Iteration # 3683  loss is :  1.3218839\n",
      "Iteration # 3684  loss is :  1.3218834\n",
      "Iteration # 3685  loss is :  1.3218831\n",
      "Iteration # 3686  loss is :  1.3218824\n",
      "Iteration # 3687  loss is :  1.3218819\n",
      "Iteration # 3688  loss is :  1.3218813\n",
      "Iteration # 3689  loss is :  1.3218808\n",
      "Iteration # 3690  loss is :  1.3218802\n",
      "Iteration # 3691  loss is :  1.3218796\n",
      "Iteration # 3692  loss is :  1.3218793\n",
      "Iteration # 3693  loss is :  1.3218787\n",
      "Iteration # 3694  loss is :  1.3218781\n",
      "Iteration # 3695  loss is :  1.3218775\n",
      "Iteration # 3696  loss is :  1.3218771\n",
      "Iteration # 3697  loss is :  1.3218764\n",
      "Iteration # 3698  loss is :  1.3218759\n",
      "Iteration # 3699  loss is :  1.3218755\n",
      "Iteration # 3700  loss is :  1.321875\n",
      "Iteration # 3701  loss is :  1.3218744\n",
      "Iteration # 3702  loss is :  1.3218738\n",
      "Iteration # 3703  loss is :  1.3218733\n",
      "Iteration # 3704  loss is :  1.3218728\n",
      "Iteration # 3705  loss is :  1.3218722\n",
      "Iteration # 3706  loss is :  1.3218718\n",
      "Iteration # 3707  loss is :  1.321871\n",
      "Iteration # 3708  loss is :  1.3218707\n",
      "Iteration # 3709  loss is :  1.3218701\n",
      "Iteration # 3710  loss is :  1.3218696\n",
      "Iteration # 3711  loss is :  1.3218689\n",
      "Iteration # 3712  loss is :  1.3218685\n",
      "Iteration # 3713  loss is :  1.321868\n",
      "Iteration # 3714  loss is :  1.3218675\n",
      "Iteration # 3715  loss is :  1.3218668\n",
      "Iteration # 3716  loss is :  1.3218664\n",
      "Iteration # 3717  loss is :  1.3218658\n",
      "Iteration # 3718  loss is :  1.3218652\n",
      "Iteration # 3719  loss is :  1.3218648\n",
      "Iteration # 3720  loss is :  1.3218642\n",
      "Iteration # 3721  loss is :  1.3218638\n",
      "Iteration # 3722  loss is :  1.3218632\n",
      "Iteration # 3723  loss is :  1.3218626\n",
      "Iteration # 3724  loss is :  1.3218622\n",
      "Iteration # 3725  loss is :  1.3218615\n",
      "Iteration # 3726  loss is :  1.321861\n",
      "Iteration # 3727  loss is :  1.3218606\n",
      "Iteration # 3728  loss is :  1.3218601\n",
      "Iteration # 3729  loss is :  1.3218595\n",
      "Iteration # 3730  loss is :  1.3218589\n",
      "Iteration # 3731  loss is :  1.3218584\n",
      "Iteration # 3732  loss is :  1.3218579\n",
      "Iteration # 3733  loss is :  1.3218572\n",
      "Iteration # 3734  loss is :  1.3218567\n",
      "Iteration # 3735  loss is :  1.3218564\n",
      "Iteration # 3736  loss is :  1.3218558\n",
      "Iteration # 3737  loss is :  1.3218552\n",
      "Iteration # 3738  loss is :  1.3218546\n",
      "Iteration # 3739  loss is :  1.3218542\n",
      "Iteration # 3740  loss is :  1.3218536\n",
      "Iteration # 3741  loss is :  1.321853\n",
      "Iteration # 3742  loss is :  1.3218526\n",
      "Iteration # 3743  loss is :  1.3218521\n",
      "Iteration # 3744  loss is :  1.3218515\n",
      "Iteration # 3745  loss is :  1.3218509\n",
      "Iteration # 3746  loss is :  1.3218505\n",
      "Iteration # 3747  loss is :  1.32185\n",
      "Iteration # 3748  loss is :  1.3218495\n",
      "Iteration # 3749  loss is :  1.3218489\n",
      "Iteration # 3750  loss is :  1.3218485\n",
      "Iteration # 3751  loss is :  1.3218479\n",
      "Iteration # 3752  loss is :  1.3218474\n",
      "Iteration # 3753  loss is :  1.3218468\n",
      "Iteration # 3754  loss is :  1.3218464\n",
      "Iteration # 3755  loss is :  1.3218459\n",
      "Iteration # 3756  loss is :  1.3218453\n",
      "Iteration # 3757  loss is :  1.3218449\n",
      "Iteration # 3758  loss is :  1.3218443\n",
      "Iteration # 3759  loss is :  1.3218437\n",
      "Iteration # 3760  loss is :  1.3218431\n",
      "Iteration # 3761  loss is :  1.3218428\n",
      "Iteration # 3762  loss is :  1.3218422\n",
      "Iteration # 3763  loss is :  1.3218416\n",
      "Iteration # 3764  loss is :  1.3218412\n",
      "Iteration # 3765  loss is :  1.3218408\n",
      "Iteration # 3766  loss is :  1.32184\n",
      "Iteration # 3767  loss is :  1.3218396\n",
      "Iteration # 3768  loss is :  1.3218391\n",
      "Iteration # 3769  loss is :  1.3218387\n",
      "Iteration # 3770  loss is :  1.3218381\n",
      "Iteration # 3771  loss is :  1.3218375\n",
      "Iteration # 3772  loss is :  1.3218372\n",
      "Iteration # 3773  loss is :  1.3218366\n",
      "Iteration # 3774  loss is :  1.321836\n",
      "Iteration # 3775  loss is :  1.3218355\n",
      "Iteration # 3776  loss is :  1.321835\n",
      "Iteration # 3777  loss is :  1.3218346\n",
      "Iteration # 3778  loss is :  1.3218338\n",
      "Iteration # 3779  loss is :  1.3218336\n",
      "Iteration # 3780  loss is :  1.321833\n",
      "Iteration # 3781  loss is :  1.3218323\n",
      "Iteration # 3782  loss is :  1.3218321\n",
      "Iteration # 3783  loss is :  1.3218316\n",
      "Iteration # 3784  loss is :  1.321831\n",
      "Iteration # 3785  loss is :  1.3218304\n",
      "Iteration # 3786  loss is :  1.3218299\n",
      "Iteration # 3787  loss is :  1.3218294\n",
      "Iteration # 3788  loss is :  1.3218288\n",
      "Iteration # 3789  loss is :  1.3218284\n",
      "Iteration # 3790  loss is :  1.3218279\n",
      "Iteration # 3791  loss is :  1.3218274\n",
      "Iteration # 3792  loss is :  1.3218269\n",
      "Iteration # 3793  loss is :  1.3218262\n",
      "Iteration # 3794  loss is :  1.321826\n",
      "Iteration # 3795  loss is :  1.3218254\n",
      "Iteration # 3796  loss is :  1.3218249\n",
      "Iteration # 3797  loss is :  1.3218243\n",
      "Iteration # 3798  loss is :  1.321824\n",
      "Iteration # 3799  loss is :  1.3218234\n",
      "Iteration # 3800  loss is :  1.321823\n",
      "Iteration # 3801  loss is :  1.3218223\n",
      "Iteration # 3802  loss is :  1.3218219\n",
      "Iteration # 3803  loss is :  1.3218213\n",
      "Iteration # 3804  loss is :  1.3218209\n",
      "Iteration # 3805  loss is :  1.3218203\n",
      "Iteration # 3806  loss is :  1.3218199\n",
      "Iteration # 3807  loss is :  1.3218194\n",
      "Iteration # 3808  loss is :  1.3218188\n",
      "Iteration # 3809  loss is :  1.3218182\n",
      "Iteration # 3810  loss is :  1.3218178\n",
      "Iteration # 3811  loss is :  1.3218174\n",
      "Iteration # 3812  loss is :  1.3218168\n",
      "Iteration # 3813  loss is :  1.3218163\n",
      "Iteration # 3814  loss is :  1.3218158\n",
      "Iteration # 3815  loss is :  1.3218153\n",
      "Iteration # 3816  loss is :  1.3218148\n",
      "Iteration # 3817  loss is :  1.3218144\n",
      "Iteration # 3818  loss is :  1.3218138\n",
      "Iteration # 3819  loss is :  1.3218135\n",
      "Iteration # 3820  loss is :  1.3218129\n",
      "Iteration # 3821  loss is :  1.3218124\n",
      "Iteration # 3822  loss is :  1.3218118\n",
      "Iteration # 3823  loss is :  1.3218113\n",
      "Iteration # 3824  loss is :  1.3218107\n",
      "Iteration # 3825  loss is :  1.3218102\n",
      "Iteration # 3826  loss is :  1.32181\n",
      "Iteration # 3827  loss is :  1.3218094\n",
      "Iteration # 3828  loss is :  1.3218088\n",
      "Iteration # 3829  loss is :  1.3218085\n",
      "Iteration # 3830  loss is :  1.321808\n",
      "Iteration # 3831  loss is :  1.3218074\n",
      "Iteration # 3832  loss is :  1.3218069\n",
      "Iteration # 3833  loss is :  1.3218066\n",
      "Iteration # 3834  loss is :  1.321806\n",
      "Iteration # 3835  loss is :  1.3218054\n",
      "Iteration # 3836  loss is :  1.321805\n",
      "Iteration # 3837  loss is :  1.3218045\n",
      "Iteration # 3838  loss is :  1.321804\n",
      "Iteration # 3839  loss is :  1.3218035\n",
      "Iteration # 3840  loss is :  1.3218031\n",
      "Iteration # 3841  loss is :  1.3218025\n",
      "Iteration # 3842  loss is :  1.321802\n",
      "Iteration # 3843  loss is :  1.3218015\n",
      "Iteration # 3844  loss is :  1.321801\n",
      "Iteration # 3845  loss is :  1.3218007\n",
      "Iteration # 3846  loss is :  1.3218001\n",
      "Iteration # 3847  loss is :  1.3217995\n",
      "Iteration # 3848  loss is :  1.321799\n",
      "Iteration # 3849  loss is :  1.3217986\n",
      "Iteration # 3850  loss is :  1.3217981\n",
      "Iteration # 3851  loss is :  1.3217976\n",
      "Iteration # 3852  loss is :  1.3217973\n",
      "Iteration # 3853  loss is :  1.3217967\n",
      "Iteration # 3854  loss is :  1.3217961\n",
      "Iteration # 3855  loss is :  1.3217957\n",
      "Iteration # 3856  loss is :  1.3217952\n",
      "Iteration # 3857  loss is :  1.3217946\n",
      "Iteration # 3858  loss is :  1.3217943\n",
      "Iteration # 3859  loss is :  1.3217938\n",
      "Iteration # 3860  loss is :  1.3217932\n",
      "Iteration # 3861  loss is :  1.3217928\n",
      "Iteration # 3862  loss is :  1.3217924\n",
      "Iteration # 3863  loss is :  1.3217918\n",
      "Iteration # 3864  loss is :  1.3217914\n",
      "Iteration # 3865  loss is :  1.3217908\n",
      "Iteration # 3866  loss is :  1.3217903\n",
      "Iteration # 3867  loss is :  1.3217899\n",
      "Iteration # 3868  loss is :  1.3217894\n",
      "Iteration # 3869  loss is :  1.3217889\n",
      "Iteration # 3870  loss is :  1.3217884\n",
      "Iteration # 3871  loss is :  1.3217878\n",
      "Iteration # 3872  loss is :  1.3217875\n",
      "Iteration # 3873  loss is :  1.3217871\n",
      "Iteration # 3874  loss is :  1.3217865\n",
      "Iteration # 3875  loss is :  1.3217859\n",
      "Iteration # 3876  loss is :  1.3217856\n",
      "Iteration # 3877  loss is :  1.321785\n",
      "Iteration # 3878  loss is :  1.3217846\n",
      "Iteration # 3879  loss is :  1.3217841\n",
      "Iteration # 3880  loss is :  1.3217837\n",
      "Iteration # 3881  loss is :  1.3217831\n",
      "Iteration # 3882  loss is :  1.3217826\n",
      "Iteration # 3883  loss is :  1.3217821\n",
      "Iteration # 3884  loss is :  1.3217818\n",
      "Iteration # 3885  loss is :  1.3217813\n",
      "Iteration # 3886  loss is :  1.3217807\n",
      "Iteration # 3887  loss is :  1.3217803\n",
      "Iteration # 3888  loss is :  1.3217798\n",
      "Iteration # 3889  loss is :  1.3217794\n",
      "Iteration # 3890  loss is :  1.3217788\n",
      "Iteration # 3891  loss is :  1.3217784\n",
      "Iteration # 3892  loss is :  1.3217779\n",
      "Iteration # 3893  loss is :  1.3217775\n",
      "Iteration # 3894  loss is :  1.3217769\n",
      "Iteration # 3895  loss is :  1.3217764\n",
      "Iteration # 3896  loss is :  1.3217762\n",
      "Iteration # 3897  loss is :  1.3217756\n",
      "Iteration # 3898  loss is :  1.3217751\n",
      "Iteration # 3899  loss is :  1.3217746\n",
      "Iteration # 3900  loss is :  1.3217742\n",
      "Iteration # 3901  loss is :  1.3217738\n",
      "Iteration # 3902  loss is :  1.3217732\n",
      "Iteration # 3903  loss is :  1.3217728\n",
      "Iteration # 3904  loss is :  1.3217722\n",
      "Iteration # 3905  loss is :  1.3217717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 3906  loss is :  1.3217714\n",
      "Iteration # 3907  loss is :  1.3217709\n",
      "Iteration # 3908  loss is :  1.3217704\n",
      "Iteration # 3909  loss is :  1.32177\n",
      "Iteration # 3910  loss is :  1.3217695\n",
      "Iteration # 3911  loss is :  1.3217689\n",
      "Iteration # 3912  loss is :  1.3217686\n",
      "Iteration # 3913  loss is :  1.321768\n",
      "Iteration # 3914  loss is :  1.3217676\n",
      "Iteration # 3915  loss is :  1.3217671\n",
      "Iteration # 3916  loss is :  1.3217667\n",
      "Iteration # 3917  loss is :  1.3217661\n",
      "Iteration # 3918  loss is :  1.3217658\n",
      "Iteration # 3919  loss is :  1.3217652\n",
      "Iteration # 3920  loss is :  1.321765\n",
      "Iteration # 3921  loss is :  1.3217642\n",
      "Iteration # 3922  loss is :  1.3217639\n",
      "Iteration # 3923  loss is :  1.3217633\n",
      "Iteration # 3924  loss is :  1.321763\n",
      "Iteration # 3925  loss is :  1.3217623\n",
      "Iteration # 3926  loss is :  1.321762\n",
      "Iteration # 3927  loss is :  1.3217615\n",
      "Iteration # 3928  loss is :  1.321761\n",
      "Iteration # 3929  loss is :  1.3217607\n",
      "Iteration # 3930  loss is :  1.3217602\n",
      "Iteration # 3931  loss is :  1.3217596\n",
      "Iteration # 3932  loss is :  1.3217592\n",
      "Iteration # 3933  loss is :  1.3217589\n",
      "Iteration # 3934  loss is :  1.3217583\n",
      "Iteration # 3935  loss is :  1.3217578\n",
      "Iteration # 3936  loss is :  1.3217574\n",
      "Iteration # 3937  loss is :  1.321757\n",
      "Iteration # 3938  loss is :  1.3217565\n",
      "Iteration # 3939  loss is :  1.3217559\n",
      "Iteration # 3940  loss is :  1.3217554\n",
      "Iteration # 3941  loss is :  1.3217552\n",
      "Iteration # 3942  loss is :  1.3217546\n",
      "Iteration # 3943  loss is :  1.321754\n",
      "Iteration # 3944  loss is :  1.3217536\n",
      "Iteration # 3945  loss is :  1.3217533\n",
      "Iteration # 3946  loss is :  1.3217528\n",
      "Iteration # 3947  loss is :  1.3217523\n",
      "Iteration # 3948  loss is :  1.3217518\n",
      "Iteration # 3949  loss is :  1.3217514\n",
      "Iteration # 3950  loss is :  1.3217509\n",
      "Iteration # 3951  loss is :  1.3217505\n",
      "Iteration # 3952  loss is :  1.32175\n",
      "Iteration # 3953  loss is :  1.3217496\n",
      "Iteration # 3954  loss is :  1.3217492\n",
      "Iteration # 3955  loss is :  1.3217486\n",
      "Iteration # 3956  loss is :  1.3217481\n",
      "Iteration # 3957  loss is :  1.3217478\n",
      "Iteration # 3958  loss is :  1.3217474\n",
      "Iteration # 3959  loss is :  1.3217468\n",
      "Iteration # 3960  loss is :  1.3217462\n",
      "Iteration # 3961  loss is :  1.321746\n",
      "Iteration # 3962  loss is :  1.3217454\n",
      "Iteration # 3963  loss is :  1.321745\n",
      "Iteration # 3964  loss is :  1.3217444\n",
      "Iteration # 3965  loss is :  1.3217441\n",
      "Iteration # 3966  loss is :  1.3217437\n",
      "Iteration # 3967  loss is :  1.3217432\n",
      "Iteration # 3968  loss is :  1.3217427\n",
      "Iteration # 3969  loss is :  1.3217423\n",
      "Iteration # 3970  loss is :  1.3217418\n",
      "Iteration # 3971  loss is :  1.3217415\n",
      "Iteration # 3972  loss is :  1.321741\n",
      "Iteration # 3973  loss is :  1.3217404\n",
      "Iteration # 3974  loss is :  1.32174\n",
      "Iteration # 3975  loss is :  1.3217396\n",
      "Iteration # 3976  loss is :  1.3217391\n",
      "Iteration # 3977  loss is :  1.3217387\n",
      "Iteration # 3978  loss is :  1.3217382\n",
      "Iteration # 3979  loss is :  1.3217378\n",
      "Iteration # 3980  loss is :  1.3217374\n",
      "Iteration # 3981  loss is :  1.3217369\n",
      "Iteration # 3982  loss is :  1.3217363\n",
      "Iteration # 3983  loss is :  1.3217361\n",
      "Iteration # 3984  loss is :  1.3217356\n",
      "Iteration # 3985  loss is :  1.3217351\n",
      "Iteration # 3986  loss is :  1.3217347\n",
      "Iteration # 3987  loss is :  1.3217341\n",
      "Iteration # 3988  loss is :  1.3217338\n",
      "Iteration # 3989  loss is :  1.3217332\n",
      "Iteration # 3990  loss is :  1.3217329\n",
      "Iteration # 3991  loss is :  1.3217325\n",
      "Iteration # 3992  loss is :  1.3217319\n",
      "Iteration # 3993  loss is :  1.3217316\n",
      "Iteration # 3994  loss is :  1.3217311\n",
      "Iteration # 3995  loss is :  1.3217306\n",
      "Iteration # 3996  loss is :  1.3217303\n",
      "Iteration # 3997  loss is :  1.3217298\n",
      "Iteration # 3998  loss is :  1.3217293\n",
      "Iteration # 3999  loss is :  1.321729\n",
      "Iteration # 4000  loss is :  1.3217285\n",
      "Iteration # 4001  loss is :  1.321728\n",
      "Iteration # 4002  loss is :  1.3217275\n",
      "Iteration # 4003  loss is :  1.321727\n",
      "Iteration # 4004  loss is :  1.3217267\n",
      "Iteration # 4005  loss is :  1.3217263\n",
      "Iteration # 4006  loss is :  1.3217257\n",
      "Iteration # 4007  loss is :  1.3217254\n",
      "Iteration # 4008  loss is :  1.3217249\n",
      "Iteration # 4009  loss is :  1.3217245\n",
      "Iteration # 4010  loss is :  1.321724\n",
      "Iteration # 4011  loss is :  1.3217236\n",
      "Iteration # 4012  loss is :  1.3217231\n",
      "Iteration # 4013  loss is :  1.3217226\n",
      "Iteration # 4014  loss is :  1.3217224\n",
      "Iteration # 4015  loss is :  1.3217218\n",
      "Iteration # 4016  loss is :  1.3217213\n",
      "Iteration # 4017  loss is :  1.3217211\n",
      "Iteration # 4018  loss is :  1.3217205\n",
      "Iteration # 4019  loss is :  1.3217199\n",
      "Iteration # 4020  loss is :  1.3217196\n",
      "Iteration # 4021  loss is :  1.3217192\n",
      "Iteration # 4022  loss is :  1.3217188\n",
      "Iteration # 4023  loss is :  1.3217182\n",
      "Iteration # 4024  loss is :  1.3217179\n",
      "Iteration # 4025  loss is :  1.3217175\n",
      "Iteration # 4026  loss is :  1.321717\n",
      "Iteration # 4027  loss is :  1.3217167\n",
      "Iteration # 4028  loss is :  1.3217162\n",
      "Iteration # 4029  loss is :  1.3217156\n",
      "Iteration # 4030  loss is :  1.3217154\n",
      "Iteration # 4031  loss is :  1.3217149\n",
      "Iteration # 4032  loss is :  1.3217143\n",
      "Iteration # 4033  loss is :  1.3217139\n",
      "Iteration # 4034  loss is :  1.3217134\n",
      "Iteration # 4035  loss is :  1.3217131\n",
      "Iteration # 4036  loss is :  1.3217127\n",
      "Iteration # 4037  loss is :  1.3217121\n",
      "Iteration # 4038  loss is :  1.3217118\n",
      "Iteration # 4039  loss is :  1.3217114\n",
      "Iteration # 4040  loss is :  1.321711\n",
      "Iteration # 4041  loss is :  1.3217106\n",
      "Iteration # 4042  loss is :  1.32171\n",
      "Iteration # 4043  loss is :  1.3217096\n",
      "Iteration # 4044  loss is :  1.3217093\n",
      "Iteration # 4045  loss is :  1.3217087\n",
      "Iteration # 4046  loss is :  1.3217084\n",
      "Iteration # 4047  loss is :  1.3217078\n",
      "Iteration # 4048  loss is :  1.3217075\n",
      "Iteration # 4049  loss is :  1.3217071\n",
      "Iteration # 4050  loss is :  1.3217067\n",
      "Iteration # 4051  loss is :  1.321706\n",
      "Iteration # 4052  loss is :  1.3217058\n",
      "Iteration # 4053  loss is :  1.3217053\n",
      "Iteration # 4054  loss is :  1.321705\n",
      "Iteration # 4055  loss is :  1.3217044\n",
      "Iteration # 4056  loss is :  1.321704\n",
      "Iteration # 4057  loss is :  1.3217037\n",
      "Iteration # 4058  loss is :  1.3217034\n",
      "Iteration # 4059  loss is :  1.3217028\n",
      "Iteration # 4060  loss is :  1.3217022\n",
      "Iteration # 4061  loss is :  1.321702\n",
      "Iteration # 4062  loss is :  1.3217015\n",
      "Iteration # 4063  loss is :  1.3217012\n",
      "Iteration # 4064  loss is :  1.3217007\n",
      "Iteration # 4065  loss is :  1.3217001\n",
      "Iteration # 4066  loss is :  1.3216999\n",
      "Iteration # 4067  loss is :  1.3216993\n",
      "Iteration # 4068  loss is :  1.3216989\n",
      "Iteration # 4069  loss is :  1.3216985\n",
      "Iteration # 4070  loss is :  1.3216982\n",
      "Iteration # 4071  loss is :  1.3216977\n",
      "Iteration # 4072  loss is :  1.3216972\n",
      "Iteration # 4073  loss is :  1.3216968\n",
      "Iteration # 4074  loss is :  1.3216963\n",
      "Iteration # 4075  loss is :  1.3216959\n",
      "Iteration # 4076  loss is :  1.3216954\n",
      "Iteration # 4077  loss is :  1.3216951\n",
      "Iteration # 4078  loss is :  1.3216946\n",
      "Iteration # 4079  loss is :  1.3216944\n",
      "Iteration # 4080  loss is :  1.3216938\n",
      "Iteration # 4081  loss is :  1.3216934\n",
      "Iteration # 4082  loss is :  1.321693\n",
      "Iteration # 4083  loss is :  1.3216926\n",
      "Iteration # 4084  loss is :  1.3216922\n",
      "Iteration # 4085  loss is :  1.3216919\n",
      "Iteration # 4086  loss is :  1.3216914\n",
      "Iteration # 4087  loss is :  1.3216908\n",
      "Iteration # 4088  loss is :  1.3216906\n",
      "Iteration # 4089  loss is :  1.3216901\n",
      "Iteration # 4090  loss is :  1.3216897\n",
      "Iteration # 4091  loss is :  1.3216892\n",
      "Iteration # 4092  loss is :  1.3216888\n",
      "Iteration # 4093  loss is :  1.3216885\n",
      "Iteration # 4094  loss is :  1.3216879\n",
      "Iteration # 4095  loss is :  1.3216876\n",
      "Iteration # 4096  loss is :  1.3216871\n",
      "Iteration # 4097  loss is :  1.3216867\n",
      "Iteration # 4098  loss is :  1.3216864\n",
      "Iteration # 4099  loss is :  1.3216859\n",
      "Iteration # 4100  loss is :  1.3216856\n",
      "Iteration # 4101  loss is :  1.321685\n",
      "Iteration # 4102  loss is :  1.3216846\n",
      "Iteration # 4103  loss is :  1.3216842\n",
      "Iteration # 4104  loss is :  1.3216838\n",
      "Iteration # 4105  loss is :  1.3216834\n",
      "Iteration # 4106  loss is :  1.3216829\n",
      "Iteration # 4107  loss is :  1.3216826\n",
      "Iteration # 4108  loss is :  1.3216821\n",
      "Iteration # 4109  loss is :  1.3216816\n",
      "Iteration # 4110  loss is :  1.3216813\n",
      "Iteration # 4111  loss is :  1.3216808\n",
      "Iteration # 4112  loss is :  1.3216805\n",
      "Iteration # 4113  loss is :  1.32168\n",
      "Iteration # 4114  loss is :  1.3216796\n",
      "Iteration # 4115  loss is :  1.3216792\n",
      "Iteration # 4116  loss is :  1.3216789\n",
      "Iteration # 4117  loss is :  1.3216784\n",
      "Iteration # 4118  loss is :  1.321678\n",
      "Iteration # 4119  loss is :  1.3216777\n",
      "Iteration # 4120  loss is :  1.3216772\n",
      "Iteration # 4121  loss is :  1.3216767\n",
      "Iteration # 4122  loss is :  1.3216765\n",
      "Iteration # 4123  loss is :  1.3216759\n",
      "Iteration # 4124  loss is :  1.3216755\n",
      "Iteration # 4125  loss is :  1.3216752\n",
      "Iteration # 4126  loss is :  1.3216747\n",
      "Iteration # 4127  loss is :  1.3216743\n",
      "Iteration # 4128  loss is :  1.3216738\n",
      "Iteration # 4129  loss is :  1.3216735\n",
      "Iteration # 4130  loss is :  1.321673\n",
      "Iteration # 4131  loss is :  1.3216728\n",
      "Iteration # 4132  loss is :  1.3216722\n",
      "Iteration # 4133  loss is :  1.3216717\n",
      "Iteration # 4134  loss is :  1.3216715\n",
      "Iteration # 4135  loss is :  1.3216711\n",
      "Iteration # 4136  loss is :  1.3216707\n",
      "Iteration # 4137  loss is :  1.3216703\n",
      "Iteration # 4138  loss is :  1.3216698\n",
      "Iteration # 4139  loss is :  1.3216696\n",
      "Iteration # 4140  loss is :  1.321669\n",
      "Iteration # 4141  loss is :  1.3216686\n",
      "Iteration # 4142  loss is :  1.3216681\n",
      "Iteration # 4143  loss is :  1.3216678\n",
      "Iteration # 4144  loss is :  1.3216674\n",
      "Iteration # 4145  loss is :  1.321667\n",
      "Iteration # 4146  loss is :  1.3216666\n",
      "Iteration # 4147  loss is :  1.321666\n",
      "Iteration # 4148  loss is :  1.3216658\n",
      "Iteration # 4149  loss is :  1.3216653\n",
      "Iteration # 4150  loss is :  1.321665\n",
      "Iteration # 4151  loss is :  1.3216645\n",
      "Iteration # 4152  loss is :  1.3216642\n",
      "Iteration # 4153  loss is :  1.3216637\n",
      "Iteration # 4154  loss is :  1.3216633\n",
      "Iteration # 4155  loss is :  1.3216629\n",
      "Iteration # 4156  loss is :  1.3216625\n",
      "Iteration # 4157  loss is :  1.3216621\n",
      "Iteration # 4158  loss is :  1.3216618\n",
      "Iteration # 4159  loss is :  1.3216614\n",
      "Iteration # 4160  loss is :  1.321661\n",
      "Iteration # 4161  loss is :  1.3216604\n",
      "Iteration # 4162  loss is :  1.3216602\n",
      "Iteration # 4163  loss is :  1.3216597\n",
      "Iteration # 4164  loss is :  1.3216593\n",
      "Iteration # 4165  loss is :  1.3216588\n",
      "Iteration # 4166  loss is :  1.3216586\n",
      "Iteration # 4167  loss is :  1.3216581\n",
      "Iteration # 4168  loss is :  1.3216578\n",
      "Iteration # 4169  loss is :  1.3216573\n",
      "Iteration # 4170  loss is :  1.3216568\n",
      "Iteration # 4171  loss is :  1.3216565\n",
      "Iteration # 4172  loss is :  1.3216561\n",
      "Iteration # 4173  loss is :  1.3216558\n",
      "Iteration # 4174  loss is :  1.3216553\n",
      "Iteration # 4175  loss is :  1.3216548\n",
      "Iteration # 4176  loss is :  1.3216546\n",
      "Iteration # 4177  loss is :  1.3216542\n",
      "Iteration # 4178  loss is :  1.3216538\n",
      "Iteration # 4179  loss is :  1.3216532\n",
      "Iteration # 4180  loss is :  1.321653\n",
      "Iteration # 4181  loss is :  1.3216524\n",
      "Iteration # 4182  loss is :  1.3216523\n",
      "Iteration # 4183  loss is :  1.3216517\n",
      "Iteration # 4184  loss is :  1.3216513\n",
      "Iteration # 4185  loss is :  1.321651\n",
      "Iteration # 4186  loss is :  1.3216505\n",
      "Iteration # 4187  loss is :  1.3216501\n",
      "Iteration # 4188  loss is :  1.3216498\n",
      "Iteration # 4189  loss is :  1.3216494\n",
      "Iteration # 4190  loss is :  1.321649\n",
      "Iteration # 4191  loss is :  1.3216486\n",
      "Iteration # 4192  loss is :  1.3216482\n",
      "Iteration # 4193  loss is :  1.3216478\n",
      "Iteration # 4194  loss is :  1.3216474\n",
      "Iteration # 4195  loss is :  1.3216468\n",
      "Iteration # 4196  loss is :  1.3216467\n",
      "Iteration # 4197  loss is :  1.3216462\n",
      "Iteration # 4198  loss is :  1.3216459\n",
      "Iteration # 4199  loss is :  1.3216454\n",
      "Iteration # 4200  loss is :  1.3216449\n",
      "Iteration # 4201  loss is :  1.3216447\n",
      "Iteration # 4202  loss is :  1.3216442\n",
      "Iteration # 4203  loss is :  1.321644\n",
      "Iteration # 4204  loss is :  1.3216434\n",
      "Iteration # 4205  loss is :  1.3216431\n",
      "Iteration # 4206  loss is :  1.3216426\n",
      "Iteration # 4207  loss is :  1.3216424\n",
      "Iteration # 4208  loss is :  1.3216419\n",
      "Iteration # 4209  loss is :  1.3216416\n",
      "Iteration # 4210  loss is :  1.321641\n",
      "Iteration # 4211  loss is :  1.3216407\n",
      "Iteration # 4212  loss is :  1.3216404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 4213  loss is :  1.3216399\n",
      "Iteration # 4214  loss is :  1.3216397\n",
      "Iteration # 4215  loss is :  1.3216391\n",
      "Iteration # 4216  loss is :  1.321639\n",
      "Iteration # 4217  loss is :  1.3216383\n",
      "Iteration # 4218  loss is :  1.321638\n",
      "Iteration # 4219  loss is :  1.3216376\n",
      "Iteration # 4220  loss is :  1.3216373\n",
      "Iteration # 4221  loss is :  1.3216368\n",
      "Iteration # 4222  loss is :  1.3216364\n",
      "Iteration # 4223  loss is :  1.3216362\n",
      "Iteration # 4224  loss is :  1.3216356\n",
      "Iteration # 4225  loss is :  1.3216354\n",
      "Iteration # 4226  loss is :  1.3216349\n",
      "Iteration # 4227  loss is :  1.3216345\n",
      "Iteration # 4228  loss is :  1.3216342\n",
      "Iteration # 4229  loss is :  1.3216337\n",
      "Iteration # 4230  loss is :  1.3216333\n",
      "Iteration # 4231  loss is :  1.321633\n",
      "Iteration # 4232  loss is :  1.3216325\n",
      "Iteration # 4233  loss is :  1.3216321\n",
      "Iteration # 4234  loss is :  1.3216318\n",
      "Iteration # 4235  loss is :  1.3216314\n",
      "Iteration # 4236  loss is :  1.3216312\n",
      "Iteration # 4237  loss is :  1.3216306\n",
      "Iteration # 4238  loss is :  1.3216304\n",
      "Iteration # 4239  loss is :  1.3216298\n",
      "Iteration # 4240  loss is :  1.3216295\n",
      "Iteration # 4241  loss is :  1.3216292\n",
      "Iteration # 4242  loss is :  1.3216287\n",
      "Iteration # 4243  loss is :  1.3216283\n",
      "Iteration # 4244  loss is :  1.3216279\n",
      "Iteration # 4245  loss is :  1.3216276\n",
      "Iteration # 4246  loss is :  1.3216271\n",
      "Iteration # 4247  loss is :  1.3216269\n",
      "Iteration # 4248  loss is :  1.3216264\n",
      "Iteration # 4249  loss is :  1.3216262\n",
      "Iteration # 4250  loss is :  1.3216256\n",
      "Iteration # 4251  loss is :  1.3216254\n",
      "Iteration # 4252  loss is :  1.321625\n",
      "Iteration # 4253  loss is :  1.3216245\n",
      "Iteration # 4254  loss is :  1.3216242\n",
      "Iteration # 4255  loss is :  1.3216238\n",
      "Iteration # 4256  loss is :  1.3216234\n",
      "Iteration # 4257  loss is :  1.3216228\n",
      "Iteration # 4258  loss is :  1.3216226\n",
      "Iteration # 4259  loss is :  1.3216221\n",
      "Iteration # 4260  loss is :  1.3216219\n",
      "Iteration # 4261  loss is :  1.3216215\n",
      "Iteration # 4262  loss is :  1.3216212\n",
      "Iteration # 4263  loss is :  1.3216207\n",
      "Iteration # 4264  loss is :  1.3216203\n",
      "Iteration # 4265  loss is :  1.32162\n",
      "Iteration # 4266  loss is :  1.3216196\n",
      "Iteration # 4267  loss is :  1.3216192\n",
      "Iteration # 4268  loss is :  1.3216189\n",
      "Iteration # 4269  loss is :  1.3216183\n",
      "Iteration # 4270  loss is :  1.3216181\n",
      "Iteration # 4271  loss is :  1.3216177\n",
      "Iteration # 4272  loss is :  1.3216174\n",
      "Iteration # 4273  loss is :  1.3216169\n",
      "Iteration # 4274  loss is :  1.3216166\n",
      "Iteration # 4275  loss is :  1.3216162\n",
      "Iteration # 4276  loss is :  1.3216157\n",
      "Iteration # 4277  loss is :  1.3216155\n",
      "Iteration # 4278  loss is :  1.3216151\n",
      "Iteration # 4279  loss is :  1.3216146\n",
      "Iteration # 4280  loss is :  1.3216143\n",
      "Iteration # 4281  loss is :  1.321614\n",
      "Iteration # 4282  loss is :  1.3216136\n",
      "Iteration # 4283  loss is :  1.3216133\n",
      "Iteration # 4284  loss is :  1.3216128\n",
      "Iteration # 4285  loss is :  1.3216124\n",
      "Iteration # 4286  loss is :  1.321612\n",
      "Iteration # 4287  loss is :  1.3216118\n",
      "Iteration # 4288  loss is :  1.3216114\n",
      "Iteration # 4289  loss is :  1.3216109\n",
      "Iteration # 4290  loss is :  1.3216106\n",
      "Iteration # 4291  loss is :  1.3216102\n",
      "Iteration # 4292  loss is :  1.32161\n",
      "Iteration # 4293  loss is :  1.3216095\n",
      "Iteration # 4294  loss is :  1.3216091\n",
      "Iteration # 4295  loss is :  1.3216087\n",
      "Iteration # 4296  loss is :  1.3216085\n",
      "Iteration # 4297  loss is :  1.321608\n",
      "Iteration # 4298  loss is :  1.3216077\n",
      "Iteration # 4299  loss is :  1.3216072\n",
      "Iteration # 4300  loss is :  1.3216068\n",
      "Iteration # 4301  loss is :  1.3216066\n",
      "Iteration # 4302  loss is :  1.3216063\n",
      "Iteration # 4303  loss is :  1.3216058\n",
      "Iteration # 4304  loss is :  1.3216054\n",
      "Iteration # 4305  loss is :  1.321605\n",
      "Iteration # 4306  loss is :  1.3216047\n",
      "Iteration # 4307  loss is :  1.3216044\n",
      "Iteration # 4308  loss is :  1.3216039\n",
      "Iteration # 4309  loss is :  1.3216037\n",
      "Iteration # 4310  loss is :  1.3216031\n",
      "Iteration # 4311  loss is :  1.3216028\n",
      "Iteration # 4312  loss is :  1.3216025\n",
      "Iteration # 4313  loss is :  1.3216022\n",
      "Iteration # 4314  loss is :  1.3216016\n",
      "Iteration # 4315  loss is :  1.3216013\n",
      "Iteration # 4316  loss is :  1.3216009\n",
      "Iteration # 4317  loss is :  1.3216007\n",
      "Iteration # 4318  loss is :  1.3216002\n",
      "Iteration # 4319  loss is :  1.3216\n",
      "Iteration # 4320  loss is :  1.3215996\n",
      "Iteration # 4321  loss is :  1.3215991\n",
      "Iteration # 4322  loss is :  1.3215989\n",
      "Iteration # 4323  loss is :  1.3215985\n",
      "Iteration # 4324  loss is :  1.321598\n",
      "Iteration # 4325  loss is :  1.3215976\n",
      "Iteration # 4326  loss is :  1.3215972\n",
      "Iteration # 4327  loss is :  1.321597\n",
      "Iteration # 4328  loss is :  1.3215966\n",
      "Iteration # 4329  loss is :  1.3215961\n",
      "Iteration # 4330  loss is :  1.3215959\n",
      "Iteration # 4331  loss is :  1.3215954\n",
      "Iteration # 4332  loss is :  1.3215952\n",
      "Iteration # 4333  loss is :  1.3215948\n",
      "Iteration # 4334  loss is :  1.3215945\n",
      "Iteration # 4335  loss is :  1.321594\n",
      "Iteration # 4336  loss is :  1.3215936\n",
      "Iteration # 4337  loss is :  1.3215933\n",
      "Iteration # 4338  loss is :  1.321593\n",
      "Iteration # 4339  loss is :  1.3215926\n",
      "Iteration # 4340  loss is :  1.3215922\n",
      "Iteration # 4341  loss is :  1.3215919\n",
      "Iteration # 4342  loss is :  1.3215916\n",
      "Iteration # 4343  loss is :  1.3215911\n",
      "Iteration # 4344  loss is :  1.3215909\n",
      "Iteration # 4345  loss is :  1.3215905\n",
      "Iteration # 4346  loss is :  1.3215901\n",
      "Iteration # 4347  loss is :  1.3215896\n",
      "Iteration # 4348  loss is :  1.3215895\n",
      "Iteration # 4349  loss is :  1.321589\n",
      "Iteration # 4350  loss is :  1.3215888\n",
      "Iteration # 4351  loss is :  1.3215882\n",
      "Iteration # 4352  loss is :  1.3215879\n",
      "Iteration # 4353  loss is :  1.3215876\n",
      "Iteration # 4354  loss is :  1.3215874\n",
      "Iteration # 4355  loss is :  1.3215868\n",
      "Iteration # 4356  loss is :  1.3215864\n",
      "Iteration # 4357  loss is :  1.321586\n",
      "Iteration # 4358  loss is :  1.3215857\n",
      "Iteration # 4359  loss is :  1.3215854\n",
      "Iteration # 4360  loss is :  1.321585\n",
      "Iteration # 4361  loss is :  1.3215847\n",
      "Iteration # 4362  loss is :  1.3215843\n",
      "Iteration # 4363  loss is :  1.321584\n",
      "Iteration # 4364  loss is :  1.3215837\n",
      "Iteration # 4365  loss is :  1.3215833\n",
      "Iteration # 4366  loss is :  1.3215828\n",
      "Iteration # 4367  loss is :  1.3215826\n",
      "Iteration # 4368  loss is :  1.3215821\n",
      "Iteration # 4369  loss is :  1.3215818\n",
      "Iteration # 4370  loss is :  1.3215815\n",
      "Iteration # 4371  loss is :  1.3215811\n",
      "Iteration # 4372  loss is :  1.3215808\n",
      "Iteration # 4373  loss is :  1.3215803\n",
      "Iteration # 4374  loss is :  1.3215799\n",
      "Iteration # 4375  loss is :  1.3215797\n",
      "Iteration # 4376  loss is :  1.3215792\n",
      "Iteration # 4377  loss is :  1.3215789\n",
      "Iteration # 4378  loss is :  1.3215785\n",
      "Iteration # 4379  loss is :  1.3215783\n",
      "Iteration # 4380  loss is :  1.3215778\n",
      "Iteration # 4381  loss is :  1.3215775\n",
      "Iteration # 4382  loss is :  1.3215772\n",
      "Iteration # 4383  loss is :  1.3215768\n",
      "Iteration # 4384  loss is :  1.3215764\n",
      "Iteration # 4385  loss is :  1.3215761\n",
      "Iteration # 4386  loss is :  1.3215759\n",
      "Iteration # 4387  loss is :  1.3215755\n",
      "Iteration # 4388  loss is :  1.321575\n",
      "Iteration # 4389  loss is :  1.3215748\n",
      "Iteration # 4390  loss is :  1.3215742\n",
      "Iteration # 4391  loss is :  1.3215741\n",
      "Iteration # 4392  loss is :  1.3215737\n",
      "Iteration # 4393  loss is :  1.3215733\n",
      "Iteration # 4394  loss is :  1.321573\n",
      "Iteration # 4395  loss is :  1.3215727\n",
      "Iteration # 4396  loss is :  1.3215723\n",
      "Iteration # 4397  loss is :  1.321572\n",
      "Iteration # 4398  loss is :  1.3215716\n",
      "Iteration # 4399  loss is :  1.3215711\n",
      "Iteration # 4400  loss is :  1.3215708\n",
      "Iteration # 4401  loss is :  1.3215705\n",
      "Iteration # 4402  loss is :  1.3215703\n",
      "Iteration # 4403  loss is :  1.3215698\n",
      "Iteration # 4404  loss is :  1.3215694\n",
      "Iteration # 4405  loss is :  1.3215691\n",
      "Iteration # 4406  loss is :  1.3215686\n",
      "Iteration # 4407  loss is :  1.3215685\n",
      "Iteration # 4408  loss is :  1.3215681\n",
      "Iteration # 4409  loss is :  1.3215678\n",
      "Iteration # 4410  loss is :  1.3215673\n",
      "Iteration # 4411  loss is :  1.321567\n",
      "Iteration # 4412  loss is :  1.3215666\n",
      "Iteration # 4413  loss is :  1.3215663\n",
      "Iteration # 4414  loss is :  1.321566\n",
      "Iteration # 4415  loss is :  1.3215656\n",
      "Iteration # 4416  loss is :  1.3215655\n",
      "Iteration # 4417  loss is :  1.321565\n",
      "Iteration # 4418  loss is :  1.3215644\n",
      "Iteration # 4419  loss is :  1.3215643\n",
      "Iteration # 4420  loss is :  1.321564\n",
      "Iteration # 4421  loss is :  1.3215636\n",
      "Iteration # 4422  loss is :  1.3215631\n",
      "Iteration # 4423  loss is :  1.3215628\n",
      "Iteration # 4424  loss is :  1.3215626\n",
      "Iteration # 4425  loss is :  1.3215622\n",
      "Iteration # 4426  loss is :  1.3215619\n",
      "Iteration # 4427  loss is :  1.3215615\n",
      "Iteration # 4428  loss is :  1.3215612\n",
      "Iteration # 4429  loss is :  1.3215609\n",
      "Iteration # 4430  loss is :  1.3215605\n",
      "Iteration # 4431  loss is :  1.32156\n",
      "Iteration # 4432  loss is :  1.3215599\n",
      "Iteration # 4433  loss is :  1.3215594\n",
      "Iteration # 4434  loss is :  1.3215591\n",
      "Iteration # 4435  loss is :  1.3215588\n",
      "Iteration # 4436  loss is :  1.3215586\n",
      "Iteration # 4437  loss is :  1.321558\n",
      "Iteration # 4438  loss is :  1.3215578\n",
      "Iteration # 4439  loss is :  1.3215572\n",
      "Iteration # 4440  loss is :  1.321557\n",
      "Iteration # 4441  loss is :  1.3215567\n",
      "Iteration # 4442  loss is :  1.3215563\n",
      "Iteration # 4443  loss is :  1.3215559\n",
      "Iteration # 4444  loss is :  1.3215556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 4445  loss is :  1.3215553\n",
      "Iteration # 4446  loss is :  1.3215551\n",
      "Iteration # 4447  loss is :  1.3215547\n",
      "Iteration # 4448  loss is :  1.3215543\n",
      "Iteration # 4449  loss is :  1.321554\n",
      "Iteration # 4450  loss is :  1.3215537\n",
      "Iteration # 4451  loss is :  1.3215532\n",
      "Iteration # 4452  loss is :  1.321553\n",
      "Iteration # 4453  loss is :  1.3215528\n",
      "Iteration # 4454  loss is :  1.3215522\n",
      "Iteration # 4455  loss is :  1.3215518\n",
      "Iteration # 4456  loss is :  1.3215516\n",
      "Iteration # 4457  loss is :  1.3215513\n",
      "Iteration # 4458  loss is :  1.321551\n",
      "Iteration # 4459  loss is :  1.3215506\n",
      "Iteration # 4460  loss is :  1.3215503\n",
      "Iteration # 4461  loss is :  1.3215498\n",
      "Iteration # 4462  loss is :  1.3215494\n",
      "Iteration # 4463  loss is :  1.3215493\n",
      "Iteration # 4464  loss is :  1.3215488\n",
      "Iteration # 4465  loss is :  1.3215486\n",
      "Iteration # 4466  loss is :  1.3215481\n",
      "Iteration # 4467  loss is :  1.3215479\n",
      "Iteration # 4468  loss is :  1.3215475\n",
      "Iteration # 4469  loss is :  1.3215473\n",
      "Iteration # 4470  loss is :  1.3215468\n",
      "Iteration # 4471  loss is :  1.3215466\n",
      "Iteration # 4472  loss is :  1.3215461\n",
      "Iteration # 4473  loss is :  1.3215458\n",
      "Iteration # 4474  loss is :  1.3215455\n",
      "Iteration # 4475  loss is :  1.3215452\n",
      "Iteration # 4476  loss is :  1.3215446\n",
      "Iteration # 4477  loss is :  1.3215444\n",
      "Iteration # 4478  loss is :  1.3215443\n",
      "Iteration # 4479  loss is :  1.3215438\n",
      "Iteration # 4480  loss is :  1.3215436\n",
      "Iteration # 4481  loss is :  1.3215431\n",
      "Iteration # 4482  loss is :  1.3215429\n",
      "Iteration # 4483  loss is :  1.3215425\n",
      "Iteration # 4484  loss is :  1.321542\n",
      "Iteration # 4485  loss is :  1.3215418\n",
      "Iteration # 4486  loss is :  1.3215415\n",
      "Iteration # 4487  loss is :  1.3215412\n",
      "Iteration # 4488  loss is :  1.3215408\n",
      "Iteration # 4489  loss is :  1.3215404\n",
      "Iteration # 4490  loss is :  1.3215401\n",
      "Iteration # 4491  loss is :  1.3215399\n",
      "Iteration # 4492  loss is :  1.3215395\n",
      "Iteration # 4493  loss is :  1.321539\n",
      "Iteration # 4494  loss is :  1.3215388\n",
      "Iteration # 4495  loss is :  1.3215384\n",
      "Iteration # 4496  loss is :  1.3215381\n",
      "Iteration # 4497  loss is :  1.3215379\n",
      "Iteration # 4498  loss is :  1.3215375\n",
      "Iteration # 4499  loss is :  1.3215373\n",
      "Iteration # 4500  loss is :  1.3215368\n",
      "Iteration # 4501  loss is :  1.3215364\n",
      "Iteration # 4502  loss is :  1.3215361\n",
      "Iteration # 4503  loss is :  1.321536\n",
      "Iteration # 4504  loss is :  1.3215356\n",
      "Iteration # 4505  loss is :  1.3215351\n",
      "Iteration # 4506  loss is :  1.3215348\n",
      "Iteration # 4507  loss is :  1.3215345\n",
      "Iteration # 4508  loss is :  1.3215342\n",
      "Iteration # 4509  loss is :  1.3215338\n",
      "Iteration # 4510  loss is :  1.3215334\n",
      "Iteration # 4511  loss is :  1.3215332\n",
      "Iteration # 4512  loss is :  1.3215328\n",
      "Iteration # 4513  loss is :  1.3215325\n",
      "Iteration # 4514  loss is :  1.3215322\n",
      "Iteration # 4515  loss is :  1.3215318\n",
      "Iteration # 4516  loss is :  1.3215317\n",
      "Iteration # 4517  loss is :  1.3215312\n",
      "Iteration # 4518  loss is :  1.3215309\n",
      "Iteration # 4519  loss is :  1.3215306\n",
      "Iteration # 4520  loss is :  1.3215303\n",
      "Iteration # 4521  loss is :  1.3215299\n",
      "Iteration # 4522  loss is :  1.3215297\n",
      "Iteration # 4523  loss is :  1.3215292\n",
      "Iteration # 4524  loss is :  1.3215289\n",
      "Iteration # 4525  loss is :  1.3215286\n",
      "Iteration # 4526  loss is :  1.3215283\n",
      "Iteration # 4527  loss is :  1.321528\n",
      "Iteration # 4528  loss is :  1.3215276\n",
      "Iteration # 4529  loss is :  1.3215274\n",
      "Iteration # 4530  loss is :  1.3215269\n",
      "Iteration # 4531  loss is :  1.3215265\n",
      "Iteration # 4532  loss is :  1.3215263\n",
      "Iteration # 4533  loss is :  1.321526\n",
      "Iteration # 4534  loss is :  1.3215257\n",
      "Iteration # 4535  loss is :  1.3215253\n",
      "Iteration # 4536  loss is :  1.321525\n",
      "Iteration # 4537  loss is :  1.3215246\n",
      "Iteration # 4538  loss is :  1.3215244\n",
      "Iteration # 4539  loss is :  1.3215241\n",
      "Iteration # 4540  loss is :  1.3215237\n",
      "Iteration # 4541  loss is :  1.3215233\n",
      "Iteration # 4542  loss is :  1.321523\n",
      "Iteration # 4543  loss is :  1.3215227\n",
      "Iteration # 4544  loss is :  1.3215226\n",
      "Iteration # 4545  loss is :  1.3215221\n",
      "Iteration # 4546  loss is :  1.3215218\n",
      "Iteration # 4547  loss is :  1.3215214\n",
      "Iteration # 4548  loss is :  1.3215212\n",
      "Iteration # 4549  loss is :  1.3215208\n",
      "Iteration # 4550  loss is :  1.3215207\n",
      "Iteration # 4551  loss is :  1.3215201\n",
      "Iteration # 4552  loss is :  1.3215199\n",
      "Iteration # 4553  loss is :  1.3215195\n",
      "Iteration # 4554  loss is :  1.3215191\n",
      "Iteration # 4555  loss is :  1.3215189\n",
      "Iteration # 4556  loss is :  1.3215185\n",
      "Iteration # 4557  loss is :  1.3215183\n",
      "Iteration # 4558  loss is :  1.321518\n",
      "Iteration # 4559  loss is :  1.3215176\n",
      "Iteration # 4560  loss is :  1.3215172\n",
      "Iteration # 4561  loss is :  1.321517\n",
      "Iteration # 4562  loss is :  1.3215165\n",
      "Iteration # 4563  loss is :  1.3215164\n",
      "Iteration # 4564  loss is :  1.3215159\n",
      "Iteration # 4565  loss is :  1.3215156\n",
      "Iteration # 4566  loss is :  1.3215152\n",
      "Iteration # 4567  loss is :  1.3215151\n",
      "Iteration # 4568  loss is :  1.3215148\n",
      "Iteration # 4569  loss is :  1.3215144\n",
      "Iteration # 4570  loss is :  1.321514\n",
      "Iteration # 4571  loss is :  1.3215137\n",
      "Iteration # 4572  loss is :  1.3215134\n",
      "Iteration # 4573  loss is :  1.3215132\n",
      "Iteration # 4574  loss is :  1.3215128\n",
      "Iteration # 4575  loss is :  1.3215123\n",
      "Iteration # 4576  loss is :  1.3215121\n",
      "Iteration # 4577  loss is :  1.3215119\n",
      "Iteration # 4578  loss is :  1.3215115\n",
      "Iteration # 4579  loss is :  1.3215113\n",
      "Iteration # 4580  loss is :  1.3215109\n",
      "Iteration # 4581  loss is :  1.3215106\n",
      "Iteration # 4582  loss is :  1.3215102\n",
      "Iteration # 4583  loss is :  1.32151\n",
      "Iteration # 4584  loss is :  1.3215095\n",
      "Iteration # 4585  loss is :  1.3215094\n",
      "Iteration # 4586  loss is :  1.3215089\n",
      "Iteration # 4587  loss is :  1.3215086\n",
      "Iteration # 4588  loss is :  1.3215084\n",
      "Iteration # 4589  loss is :  1.321508\n",
      "Iteration # 4590  loss is :  1.3215076\n",
      "Iteration # 4591  loss is :  1.3215075\n",
      "Iteration # 4592  loss is :  1.3215071\n",
      "Iteration # 4593  loss is :  1.3215067\n",
      "Iteration # 4594  loss is :  1.3215065\n",
      "Iteration # 4595  loss is :  1.3215059\n",
      "Iteration # 4596  loss is :  1.3215058\n",
      "Iteration # 4597  loss is :  1.3215055\n",
      "Iteration # 4598  loss is :  1.3215052\n",
      "Iteration # 4599  loss is :  1.321505\n",
      "Iteration # 4600  loss is :  1.3215046\n",
      "Iteration # 4601  loss is :  1.3215044\n",
      "Iteration # 4602  loss is :  1.3215039\n",
      "Iteration # 4603  loss is :  1.3215038\n",
      "Iteration # 4604  loss is :  1.3215033\n",
      "Iteration # 4605  loss is :  1.321503\n",
      "Iteration # 4606  loss is :  1.3215027\n",
      "Iteration # 4607  loss is :  1.3215024\n",
      "Iteration # 4608  loss is :  1.321502\n",
      "Iteration # 4609  loss is :  1.3215017\n",
      "Iteration # 4610  loss is :  1.3215015\n",
      "Iteration # 4611  loss is :  1.3215011\n",
      "Iteration # 4612  loss is :  1.3215009\n",
      "Iteration # 4613  loss is :  1.3215004\n",
      "Iteration # 4614  loss is :  1.3215002\n",
      "Iteration # 4615  loss is :  1.3214998\n",
      "Iteration # 4616  loss is :  1.3214996\n",
      "Iteration # 4617  loss is :  1.3214993\n",
      "Iteration # 4618  loss is :  1.321499\n",
      "Iteration # 4619  loss is :  1.3214986\n",
      "Iteration # 4620  loss is :  1.3214982\n",
      "Iteration # 4621  loss is :  1.3214979\n",
      "Iteration # 4622  loss is :  1.3214977\n",
      "Iteration # 4623  loss is :  1.3214974\n",
      "Iteration # 4624  loss is :  1.3214971\n",
      "Iteration # 4625  loss is :  1.3214967\n",
      "Iteration # 4626  loss is :  1.3214965\n",
      "Iteration # 4627  loss is :  1.321496\n",
      "Iteration # 4628  loss is :  1.3214959\n",
      "Iteration # 4629  loss is :  1.3214955\n",
      "Iteration # 4630  loss is :  1.3214953\n",
      "Iteration # 4631  loss is :  1.3214948\n",
      "Iteration # 4632  loss is :  1.3214945\n",
      "Iteration # 4633  loss is :  1.3214942\n",
      "Iteration # 4634  loss is :  1.321494\n",
      "Iteration # 4635  loss is :  1.3214937\n",
      "Iteration # 4636  loss is :  1.3214934\n",
      "Iteration # 4637  loss is :  1.321493\n",
      "Iteration # 4638  loss is :  1.3214926\n",
      "Iteration # 4639  loss is :  1.3214923\n",
      "Iteration # 4640  loss is :  1.3214921\n",
      "Iteration # 4641  loss is :  1.3214918\n",
      "Iteration # 4642  loss is :  1.3214916\n",
      "Iteration # 4643  loss is :  1.3214912\n",
      "Iteration # 4644  loss is :  1.321491\n",
      "Iteration # 4645  loss is :  1.3214905\n",
      "Iteration # 4646  loss is :  1.3214903\n",
      "Iteration # 4647  loss is :  1.3214899\n",
      "Iteration # 4648  loss is :  1.3214897\n",
      "Iteration # 4649  loss is :  1.3214895\n",
      "Iteration # 4650  loss is :  1.3214891\n",
      "Iteration # 4651  loss is :  1.3214887\n",
      "Iteration # 4652  loss is :  1.3214884\n",
      "Iteration # 4653  loss is :  1.3214881\n",
      "Iteration # 4654  loss is :  1.3214879\n",
      "Iteration # 4655  loss is :  1.3214875\n",
      "Iteration # 4656  loss is :  1.3214871\n",
      "Iteration # 4657  loss is :  1.321487\n",
      "Iteration # 4658  loss is :  1.3214867\n",
      "Iteration # 4659  loss is :  1.3214862\n",
      "Iteration # 4660  loss is :  1.321486\n",
      "Iteration # 4661  loss is :  1.3214856\n",
      "Iteration # 4662  loss is :  1.3214854\n",
      "Iteration # 4663  loss is :  1.321485\n",
      "Iteration # 4664  loss is :  1.3214847\n",
      "Iteration # 4665  loss is :  1.3214846\n",
      "Iteration # 4666  loss is :  1.3214843\n",
      "Iteration # 4667  loss is :  1.321484\n",
      "Iteration # 4668  loss is :  1.3214836\n",
      "Iteration # 4669  loss is :  1.3214833\n",
      "Iteration # 4670  loss is :  1.3214829\n",
      "Iteration # 4671  loss is :  1.3214827\n",
      "Iteration # 4672  loss is :  1.3214825\n",
      "Iteration # 4673  loss is :  1.321482\n",
      "Iteration # 4674  loss is :  1.3214818\n",
      "Iteration # 4675  loss is :  1.3214813\n",
      "Iteration # 4676  loss is :  1.3214811\n",
      "Iteration # 4677  loss is :  1.3214808\n",
      "Iteration # 4678  loss is :  1.3214806\n",
      "Iteration # 4679  loss is :  1.3214803\n",
      "Iteration # 4680  loss is :  1.32148\n",
      "Iteration # 4681  loss is :  1.3214797\n",
      "Iteration # 4682  loss is :  1.3214793\n",
      "Iteration # 4683  loss is :  1.321479\n",
      "Iteration # 4684  loss is :  1.3214787\n",
      "Iteration # 4685  loss is :  1.3214785\n",
      "Iteration # 4686  loss is :  1.3214782\n",
      "Iteration # 4687  loss is :  1.3214779\n",
      "Iteration # 4688  loss is :  1.3214777\n",
      "Iteration # 4689  loss is :  1.321477\n",
      "Iteration # 4690  loss is :  1.3214769\n",
      "Iteration # 4691  loss is :  1.3214766\n",
      "Iteration # 4692  loss is :  1.3214765\n",
      "Iteration # 4693  loss is :  1.321476\n",
      "Iteration # 4694  loss is :  1.3214757\n",
      "Iteration # 4695  loss is :  1.3214755\n",
      "Iteration # 4696  loss is :  1.3214751\n",
      "Iteration # 4697  loss is :  1.3214749\n",
      "Iteration # 4698  loss is :  1.3214744\n",
      "Iteration # 4699  loss is :  1.3214742\n",
      "Iteration # 4700  loss is :  1.321474\n",
      "Iteration # 4701  loss is :  1.3214737\n",
      "Iteration # 4702  loss is :  1.3214735\n",
      "Iteration # 4703  loss is :  1.3214731\n",
      "Iteration # 4704  loss is :  1.3214729\n",
      "Iteration # 4705  loss is :  1.3214725\n",
      "Iteration # 4706  loss is :  1.3214722\n",
      "Iteration # 4707  loss is :  1.3214719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 4708  loss is :  1.3214715\n",
      "Iteration # 4709  loss is :  1.3214712\n",
      "Iteration # 4710  loss is :  1.321471\n",
      "Iteration # 4711  loss is :  1.3214707\n",
      "Iteration # 4712  loss is :  1.3214704\n",
      "Iteration # 4713  loss is :  1.3214701\n",
      "Iteration # 4714  loss is :  1.3214699\n",
      "Iteration # 4715  loss is :  1.3214693\n",
      "Iteration # 4716  loss is :  1.3214693\n",
      "Iteration # 4717  loss is :  1.321469\n",
      "Iteration # 4718  loss is :  1.3214686\n",
      "Iteration # 4719  loss is :  1.3214684\n",
      "Iteration # 4720  loss is :  1.321468\n",
      "Iteration # 4721  loss is :  1.3214678\n",
      "Iteration # 4722  loss is :  1.3214674\n",
      "Iteration # 4723  loss is :  1.3214672\n",
      "Iteration # 4724  loss is :  1.3214668\n",
      "Iteration # 4725  loss is :  1.3214666\n",
      "Iteration # 4726  loss is :  1.3214663\n",
      "Iteration # 4727  loss is :  1.321466\n",
      "Iteration # 4728  loss is :  1.3214656\n",
      "Iteration # 4729  loss is :  1.3214654\n",
      "Iteration # 4730  loss is :  1.3214651\n",
      "Iteration # 4731  loss is :  1.3214648\n",
      "Iteration # 4732  loss is :  1.3214645\n",
      "Iteration # 4733  loss is :  1.3214643\n",
      "Iteration # 4734  loss is :  1.3214637\n",
      "Iteration # 4735  loss is :  1.3214637\n",
      "Iteration # 4736  loss is :  1.3214633\n",
      "Iteration # 4737  loss is :  1.321463\n",
      "Iteration # 4738  loss is :  1.3214629\n",
      "Iteration # 4739  loss is :  1.3214624\n",
      "Iteration # 4740  loss is :  1.3214622\n",
      "Iteration # 4741  loss is :  1.3214618\n",
      "Iteration # 4742  loss is :  1.3214617\n",
      "Iteration # 4743  loss is :  1.3214613\n",
      "Iteration # 4744  loss is :  1.3214611\n",
      "Iteration # 4745  loss is :  1.3214608\n",
      "Iteration # 4746  loss is :  1.3214604\n",
      "Iteration # 4747  loss is :  1.3214601\n",
      "Iteration # 4748  loss is :  1.3214598\n",
      "Iteration # 4749  loss is :  1.3214597\n",
      "Iteration # 4750  loss is :  1.3214593\n",
      "Iteration # 4751  loss is :  1.3214589\n",
      "Iteration # 4752  loss is :  1.3214587\n",
      "Iteration # 4753  loss is :  1.3214583\n",
      "Iteration # 4754  loss is :  1.3214581\n",
      "Iteration # 4755  loss is :  1.3214579\n",
      "Iteration # 4756  loss is :  1.3214576\n",
      "Iteration # 4757  loss is :  1.3214573\n",
      "Iteration # 4758  loss is :  1.321457\n",
      "Iteration # 4759  loss is :  1.3214566\n",
      "Iteration # 4760  loss is :  1.3214562\n",
      "Iteration # 4761  loss is :  1.3214561\n",
      "Iteration # 4762  loss is :  1.3214558\n",
      "Iteration # 4763  loss is :  1.3214555\n",
      "Iteration # 4764  loss is :  1.3214552\n",
      "Iteration # 4765  loss is :  1.3214549\n",
      "Iteration # 4766  loss is :  1.3214545\n",
      "Iteration # 4767  loss is :  1.3214544\n",
      "Iteration # 4768  loss is :  1.321454\n",
      "Iteration # 4769  loss is :  1.3214538\n",
      "Iteration # 4770  loss is :  1.3214533\n",
      "Iteration # 4771  loss is :  1.3214532\n",
      "Iteration # 4772  loss is :  1.3214529\n",
      "Iteration # 4773  loss is :  1.3214525\n",
      "Iteration # 4774  loss is :  1.3214523\n",
      "Iteration # 4775  loss is :  1.321452\n",
      "Iteration # 4776  loss is :  1.3214518\n",
      "Iteration # 4777  loss is :  1.3214514\n",
      "Iteration # 4778  loss is :  1.3214512\n",
      "Iteration # 4779  loss is :  1.321451\n",
      "Iteration # 4780  loss is :  1.3214506\n",
      "Iteration # 4781  loss is :  1.3214502\n",
      "Iteration # 4782  loss is :  1.32145\n",
      "Iteration # 4783  loss is :  1.3214498\n",
      "Iteration # 4784  loss is :  1.3214496\n",
      "Iteration # 4785  loss is :  1.3214493\n",
      "Iteration # 4786  loss is :  1.3214489\n",
      "Iteration # 4787  loss is :  1.3214484\n",
      "Iteration # 4788  loss is :  1.3214483\n",
      "Iteration # 4789  loss is :  1.3214481\n",
      "Iteration # 4790  loss is :  1.3214477\n",
      "Iteration # 4791  loss is :  1.3214475\n",
      "Iteration # 4792  loss is :  1.3214471\n",
      "Iteration # 4793  loss is :  1.3214469\n",
      "Iteration # 4794  loss is :  1.3214467\n",
      "Iteration # 4795  loss is :  1.3214464\n",
      "Iteration # 4796  loss is :  1.3214462\n",
      "Iteration # 4797  loss is :  1.3214457\n",
      "Iteration # 4798  loss is :  1.3214455\n",
      "Iteration # 4799  loss is :  1.321445\n",
      "Iteration # 4800  loss is :  1.3214449\n",
      "Iteration # 4801  loss is :  1.3214447\n",
      "Iteration # 4802  loss is :  1.3214443\n",
      "Iteration # 4803  loss is :  1.3214442\n",
      "Iteration # 4804  loss is :  1.3214437\n",
      "Iteration # 4805  loss is :  1.3214434\n",
      "Iteration # 4806  loss is :  1.3214432\n",
      "Iteration # 4807  loss is :  1.3214428\n",
      "Iteration # 4808  loss is :  1.3214426\n",
      "Iteration # 4809  loss is :  1.3214424\n",
      "Iteration # 4810  loss is :  1.3214421\n",
      "Iteration # 4811  loss is :  1.3214418\n",
      "Iteration # 4812  loss is :  1.3214415\n",
      "Iteration # 4813  loss is :  1.3214413\n",
      "Iteration # 4814  loss is :  1.3214409\n",
      "Iteration # 4815  loss is :  1.3214407\n",
      "Iteration # 4816  loss is :  1.3214406\n",
      "Iteration # 4817  loss is :  1.3214402\n",
      "Iteration # 4818  loss is :  1.32144\n",
      "Iteration # 4819  loss is :  1.3214395\n",
      "Iteration # 4820  loss is :  1.3214393\n",
      "Iteration # 4821  loss is :  1.321439\n",
      "Iteration # 4822  loss is :  1.3214388\n",
      "Iteration # 4823  loss is :  1.3214386\n",
      "Iteration # 4824  loss is :  1.3214382\n",
      "Iteration # 4825  loss is :  1.321438\n",
      "Iteration # 4826  loss is :  1.3214376\n",
      "Iteration # 4827  loss is :  1.3214372\n",
      "Iteration # 4828  loss is :  1.321437\n",
      "Iteration # 4829  loss is :  1.3214368\n",
      "Iteration # 4830  loss is :  1.3214365\n",
      "Iteration # 4831  loss is :  1.3214363\n",
      "Iteration # 4832  loss is :  1.3214359\n",
      "Iteration # 4833  loss is :  1.3214357\n",
      "Iteration # 4834  loss is :  1.3214353\n",
      "Iteration # 4835  loss is :  1.3214351\n",
      "Iteration # 4836  loss is :  1.321435\n",
      "Iteration # 4837  loss is :  1.3214346\n",
      "Iteration # 4838  loss is :  1.3214344\n",
      "Iteration # 4839  loss is :  1.3214339\n",
      "Iteration # 4840  loss is :  1.3214337\n",
      "Iteration # 4841  loss is :  1.3214333\n",
      "Iteration # 4842  loss is :  1.3214332\n",
      "Iteration # 4843  loss is :  1.321433\n",
      "Iteration # 4844  loss is :  1.3214325\n",
      "Iteration # 4845  loss is :  1.3214322\n",
      "Iteration # 4846  loss is :  1.321432\n",
      "Iteration # 4847  loss is :  1.3214318\n",
      "Iteration # 4848  loss is :  1.3214315\n",
      "Iteration # 4849  loss is :  1.3214313\n",
      "Iteration # 4850  loss is :  1.3214309\n",
      "Iteration # 4851  loss is :  1.3214308\n",
      "Iteration # 4852  loss is :  1.3214303\n",
      "Iteration # 4853  loss is :  1.3214302\n",
      "Iteration # 4854  loss is :  1.3214297\n",
      "Iteration # 4855  loss is :  1.3214295\n",
      "Iteration # 4856  loss is :  1.3214293\n",
      "Iteration # 4857  loss is :  1.321429\n",
      "Iteration # 4858  loss is :  1.3214288\n",
      "Iteration # 4859  loss is :  1.3214285\n",
      "Iteration # 4860  loss is :  1.3214282\n",
      "Iteration # 4861  loss is :  1.321428\n",
      "Iteration # 4862  loss is :  1.3214276\n",
      "Iteration # 4863  loss is :  1.3214273\n",
      "Iteration # 4864  loss is :  1.3214272\n",
      "Iteration # 4865  loss is :  1.3214269\n",
      "Iteration # 4866  loss is :  1.3214266\n",
      "Iteration # 4867  loss is :  1.3214263\n",
      "Iteration # 4868  loss is :  1.3214259\n",
      "Iteration # 4869  loss is :  1.3214258\n",
      "Iteration # 4870  loss is :  1.3214254\n",
      "Iteration # 4871  loss is :  1.3214253\n",
      "Iteration # 4872  loss is :  1.321425\n",
      "Iteration # 4873  loss is :  1.3214245\n",
      "Iteration # 4874  loss is :  1.3214245\n",
      "Iteration # 4875  loss is :  1.321424\n",
      "Iteration # 4876  loss is :  1.3214239\n",
      "Iteration # 4877  loss is :  1.3214235\n",
      "Iteration # 4878  loss is :  1.3214233\n",
      "Iteration # 4879  loss is :  1.321423\n",
      "Iteration # 4880  loss is :  1.3214226\n",
      "Iteration # 4881  loss is :  1.3214225\n",
      "Iteration # 4882  loss is :  1.3214221\n",
      "Iteration # 4883  loss is :  1.3214217\n",
      "Iteration # 4884  loss is :  1.3214216\n",
      "Iteration # 4885  loss is :  1.3214214\n",
      "Iteration # 4886  loss is :  1.3214211\n",
      "Iteration # 4887  loss is :  1.3214208\n",
      "Iteration # 4888  loss is :  1.3214206\n",
      "Iteration # 4889  loss is :  1.3214202\n",
      "Iteration # 4890  loss is :  1.3214201\n",
      "Iteration # 4891  loss is :  1.3214197\n",
      "Iteration # 4892  loss is :  1.3214195\n",
      "Iteration # 4893  loss is :  1.3214191\n",
      "Iteration # 4894  loss is :  1.3214189\n",
      "Iteration # 4895  loss is :  1.3214188\n",
      "Iteration # 4896  loss is :  1.3214183\n",
      "Iteration # 4897  loss is :  1.321418\n",
      "Iteration # 4898  loss is :  1.3214178\n",
      "Iteration # 4899  loss is :  1.3214176\n",
      "Iteration # 4900  loss is :  1.3214173\n",
      "Iteration # 4901  loss is :  1.321417\n",
      "Iteration # 4902  loss is :  1.3214169\n",
      "Iteration # 4903  loss is :  1.3214164\n",
      "Iteration # 4904  loss is :  1.3214163\n",
      "Iteration # 4905  loss is :  1.3214159\n",
      "Iteration # 4906  loss is :  1.3214157\n",
      "Iteration # 4907  loss is :  1.3214155\n",
      "Iteration # 4908  loss is :  1.3214152\n",
      "Iteration # 4909  loss is :  1.3214148\n",
      "Iteration # 4910  loss is :  1.3214147\n",
      "Iteration # 4911  loss is :  1.3214142\n",
      "Iteration # 4912  loss is :  1.321414\n",
      "Iteration # 4913  loss is :  1.3214139\n",
      "Iteration # 4914  loss is :  1.3214135\n",
      "Iteration # 4915  loss is :  1.3214133\n",
      "Iteration # 4916  loss is :  1.3214129\n",
      "Iteration # 4917  loss is :  1.3214127\n",
      "Iteration # 4918  loss is :  1.3214124\n",
      "Iteration # 4919  loss is :  1.3214121\n",
      "Iteration # 4920  loss is :  1.321412\n",
      "Iteration # 4921  loss is :  1.3214117\n",
      "Iteration # 4922  loss is :  1.3214114\n",
      "Iteration # 4923  loss is :  1.3214113\n",
      "Iteration # 4924  loss is :  1.3214108\n",
      "Iteration # 4925  loss is :  1.3214107\n",
      "Iteration # 4926  loss is :  1.3214104\n",
      "Iteration # 4927  loss is :  1.3214101\n",
      "Iteration # 4928  loss is :  1.3214098\n",
      "Iteration # 4929  loss is :  1.3214096\n",
      "Iteration # 4930  loss is :  1.3214092\n",
      "Iteration # 4931  loss is :  1.321409\n",
      "Iteration # 4932  loss is :  1.3214086\n",
      "Iteration # 4933  loss is :  1.3214084\n",
      "Iteration # 4934  loss is :  1.3214083\n",
      "Iteration # 4935  loss is :  1.321408\n",
      "Iteration # 4936  loss is :  1.3214077\n",
      "Iteration # 4937  loss is :  1.3214074\n",
      "Iteration # 4938  loss is :  1.3214071\n",
      "Iteration # 4939  loss is :  1.3214068\n",
      "Iteration # 4940  loss is :  1.3214065\n",
      "Iteration # 4941  loss is :  1.3214064\n",
      "Iteration # 4942  loss is :  1.3214062\n",
      "Iteration # 4943  loss is :  1.321406\n",
      "Iteration # 4944  loss is :  1.3214055\n",
      "Iteration # 4945  loss is :  1.3214053\n",
      "Iteration # 4946  loss is :  1.321405\n",
      "Iteration # 4947  loss is :  1.3214047\n",
      "Iteration # 4948  loss is :  1.3214045\n",
      "Iteration # 4949  loss is :  1.3214043\n",
      "Iteration # 4950  loss is :  1.3214042\n",
      "Iteration # 4951  loss is :  1.3214037\n",
      "Iteration # 4952  loss is :  1.3214036\n",
      "Iteration # 4953  loss is :  1.3214033\n",
      "Iteration # 4954  loss is :  1.321403\n",
      "Iteration # 4955  loss is :  1.3214028\n",
      "Iteration # 4956  loss is :  1.3214025\n",
      "Iteration # 4957  loss is :  1.3214022\n",
      "Iteration # 4958  loss is :  1.321402\n",
      "Iteration # 4959  loss is :  1.3214016\n",
      "Iteration # 4960  loss is :  1.3214014\n",
      "Iteration # 4961  loss is :  1.321401\n",
      "Iteration # 4962  loss is :  1.3214009\n",
      "Iteration # 4963  loss is :  1.3214006\n",
      "Iteration # 4964  loss is :  1.3214004\n",
      "Iteration # 4965  loss is :  1.3214002\n",
      "Iteration # 4966  loss is :  1.3213999\n",
      "Iteration # 4967  loss is :  1.3213996\n",
      "Iteration # 4968  loss is :  1.3213992\n",
      "Iteration # 4969  loss is :  1.321399\n",
      "Iteration # 4970  loss is :  1.3213987\n",
      "Iteration # 4971  loss is :  1.3213986\n",
      "Iteration # 4972  loss is :  1.3213983\n",
      "Iteration # 4973  loss is :  1.321398\n",
      "Iteration # 4974  loss is :  1.3213978\n",
      "Iteration # 4975  loss is :  1.3213974\n",
      "Iteration # 4976  loss is :  1.3213972\n",
      "Iteration # 4977  loss is :  1.321397\n",
      "Iteration # 4978  loss is :  1.3213967\n",
      "Iteration # 4979  loss is :  1.3213965\n",
      "Iteration # 4980  loss is :  1.3213961\n",
      "Iteration # 4981  loss is :  1.3213959\n",
      "Iteration # 4982  loss is :  1.3213958\n",
      "Iteration # 4983  loss is :  1.3213954\n",
      "Iteration # 4984  loss is :  1.321395\n",
      "Iteration # 4985  loss is :  1.3213949\n",
      "Iteration # 4986  loss is :  1.3213947\n",
      "Iteration # 4987  loss is :  1.3213944\n",
      "Iteration # 4988  loss is :  1.3213942\n",
      "Iteration # 4989  loss is :  1.3213938\n",
      "Iteration # 4990  loss is :  1.3213936\n",
      "Iteration # 4991  loss is :  1.3213934\n",
      "Iteration # 4992  loss is :  1.321393\n",
      "Iteration # 4993  loss is :  1.321393\n",
      "Iteration # 4994  loss is :  1.3213927\n",
      "Iteration # 4995  loss is :  1.3213924\n",
      "Iteration # 4996  loss is :  1.321392\n",
      "Iteration # 4997  loss is :  1.3213918\n",
      "Iteration # 4998  loss is :  1.3213915\n",
      "Iteration # 4999  loss is :  1.3213912\n",
      "Iteration # 5000  loss is :  1.3213911\n",
      "Iteration # 5001  loss is :  1.3213909\n",
      "Iteration # 5002  loss is :  1.3213905\n",
      "Iteration # 5003  loss is :  1.3213903\n",
      "Iteration # 5004  loss is :  1.32139\n",
      "Iteration # 5005  loss is :  1.3213897\n",
      "Iteration # 5006  loss is :  1.3213894\n",
      "Iteration # 5007  loss is :  1.3213892\n",
      "Iteration # 5008  loss is :  1.3213891\n",
      "Iteration # 5009  loss is :  1.3213887\n",
      "Iteration # 5010  loss is :  1.3213884\n",
      "Iteration # 5011  loss is :  1.3213882\n",
      "Iteration # 5012  loss is :  1.3213879\n",
      "Iteration # 5013  loss is :  1.3213878\n",
      "Iteration # 5014  loss is :  1.3213874\n",
      "Iteration # 5015  loss is :  1.3213873\n",
      "Iteration # 5016  loss is :  1.321387\n",
      "Iteration # 5017  loss is :  1.3213868\n",
      "Iteration # 5018  loss is :  1.3213865\n",
      "Iteration # 5019  loss is :  1.3213861\n",
      "Iteration # 5020  loss is :  1.321386\n",
      "Iteration # 5021  loss is :  1.3213856\n",
      "Iteration # 5022  loss is :  1.3213854\n",
      "Iteration # 5023  loss is :  1.3213853\n",
      "Iteration # 5024  loss is :  1.3213849\n",
      "Iteration # 5025  loss is :  1.3213847\n",
      "Iteration # 5026  loss is :  1.3213844\n",
      "Iteration # 5027  loss is :  1.3213841\n",
      "Iteration # 5028  loss is :  1.3213838\n",
      "Iteration # 5029  loss is :  1.3213836\n",
      "Iteration # 5030  loss is :  1.3213835\n",
      "Iteration # 5031  loss is :  1.3213831\n",
      "Iteration # 5032  loss is :  1.3213828\n",
      "Iteration # 5033  loss is :  1.3213828\n",
      "Iteration # 5034  loss is :  1.3213824\n",
      "Iteration # 5035  loss is :  1.3213822\n",
      "Iteration # 5036  loss is :  1.3213818\n",
      "Iteration # 5037  loss is :  1.3213816\n",
      "Iteration # 5038  loss is :  1.3213814\n",
      "Iteration # 5039  loss is :  1.3213812\n",
      "Iteration # 5040  loss is :  1.3213809\n",
      "Iteration # 5041  loss is :  1.3213806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 5042  loss is :  1.3213804\n",
      "Iteration # 5043  loss is :  1.32138\n",
      "Iteration # 5044  loss is :  1.3213798\n",
      "Iteration # 5045  loss is :  1.3213797\n",
      "Iteration # 5046  loss is :  1.3213794\n",
      "Iteration # 5047  loss is :  1.3213791\n",
      "Iteration # 5048  loss is :  1.321379\n",
      "Iteration # 5049  loss is :  1.3213787\n",
      "Iteration # 5050  loss is :  1.3213785\n",
      "Iteration # 5051  loss is :  1.321378\n",
      "Iteration # 5052  loss is :  1.3213778\n",
      "Iteration # 5053  loss is :  1.3213778\n",
      "Iteration # 5054  loss is :  1.3213774\n",
      "Iteration # 5055  loss is :  1.3213772\n",
      "Iteration # 5056  loss is :  1.3213769\n",
      "Iteration # 5057  loss is :  1.3213766\n",
      "Iteration # 5058  loss is :  1.3213763\n",
      "Iteration # 5059  loss is :  1.3213762\n",
      "Iteration # 5060  loss is :  1.3213758\n",
      "Iteration # 5061  loss is :  1.3213757\n",
      "Iteration # 5062  loss is :  1.3213755\n",
      "Iteration # 5063  loss is :  1.3213753\n",
      "Iteration # 5064  loss is :  1.3213748\n",
      "Iteration # 5065  loss is :  1.3213747\n",
      "Iteration # 5066  loss is :  1.3213743\n",
      "Iteration # 5067  loss is :  1.3213742\n",
      "Iteration # 5068  loss is :  1.3213739\n",
      "Iteration # 5069  loss is :  1.3213737\n",
      "Iteration # 5070  loss is :  1.3213735\n",
      "Iteration # 5071  loss is :  1.3213731\n",
      "Iteration # 5072  loss is :  1.3213729\n",
      "Iteration # 5073  loss is :  1.3213727\n",
      "Iteration # 5074  loss is :  1.3213723\n",
      "Iteration # 5075  loss is :  1.3213722\n",
      "Iteration # 5076  loss is :  1.321372\n",
      "Iteration # 5077  loss is :  1.3213717\n",
      "Iteration # 5078  loss is :  1.3213716\n",
      "Iteration # 5079  loss is :  1.3213712\n",
      "Iteration # 5080  loss is :  1.321371\n",
      "Iteration # 5081  loss is :  1.3213707\n",
      "Iteration # 5082  loss is :  1.3213704\n",
      "Iteration # 5083  loss is :  1.3213701\n",
      "Iteration # 5084  loss is :  1.32137\n",
      "Iteration # 5085  loss is :  1.3213696\n",
      "Iteration # 5086  loss is :  1.3213694\n",
      "Iteration # 5087  loss is :  1.3213692\n",
      "Iteration # 5088  loss is :  1.3213688\n",
      "Iteration # 5089  loss is :  1.3213687\n",
      "Iteration # 5090  loss is :  1.3213686\n",
      "Iteration # 5091  loss is :  1.3213682\n",
      "Iteration # 5092  loss is :  1.3213681\n",
      "Iteration # 5093  loss is :  1.3213679\n",
      "Iteration # 5094  loss is :  1.3213675\n",
      "Iteration # 5095  loss is :  1.3213671\n",
      "Iteration # 5096  loss is :  1.3213669\n",
      "Iteration # 5097  loss is :  1.3213667\n",
      "Iteration # 5098  loss is :  1.3213664\n",
      "Iteration # 5099  loss is :  1.3213662\n",
      "Iteration # 5100  loss is :  1.3213662\n",
      "Iteration # 5101  loss is :  1.3213658\n",
      "Iteration # 5102  loss is :  1.3213655\n",
      "Iteration # 5103  loss is :  1.3213652\n",
      "Iteration # 5104  loss is :  1.3213651\n",
      "Iteration # 5105  loss is :  1.3213646\n",
      "Iteration # 5106  loss is :  1.3213645\n",
      "Iteration # 5107  loss is :  1.3213644\n",
      "Iteration # 5108  loss is :  1.321364\n",
      "Iteration # 5109  loss is :  1.3213638\n",
      "Iteration # 5110  loss is :  1.3213636\n",
      "Iteration # 5111  loss is :  1.3213633\n",
      "Iteration # 5112  loss is :  1.321363\n",
      "Iteration # 5113  loss is :  1.321363\n",
      "Iteration # 5114  loss is :  1.3213625\n",
      "Iteration # 5115  loss is :  1.3213625\n",
      "Iteration # 5116  loss is :  1.3213621\n",
      "Iteration # 5117  loss is :  1.3213619\n",
      "Iteration # 5118  loss is :  1.3213617\n",
      "Iteration # 5119  loss is :  1.3213613\n",
      "Iteration # 5120  loss is :  1.3213611\n",
      "Iteration # 5121  loss is :  1.3213608\n",
      "Iteration # 5122  loss is :  1.3213606\n",
      "Iteration # 5123  loss is :  1.3213605\n",
      "Iteration # 5124  loss is :  1.3213602\n",
      "Iteration # 5125  loss is :  1.3213599\n",
      "Iteration # 5126  loss is :  1.3213598\n",
      "Iteration # 5127  loss is :  1.3213594\n",
      "Iteration # 5128  loss is :  1.3213592\n",
      "Iteration # 5129  loss is :  1.3213589\n",
      "Iteration # 5130  loss is :  1.3213587\n",
      "Iteration # 5131  loss is :  1.3213584\n",
      "Iteration # 5132  loss is :  1.3213582\n",
      "Iteration # 5133  loss is :  1.321358\n",
      "Iteration # 5134  loss is :  1.3213577\n",
      "Iteration # 5135  loss is :  1.3213576\n",
      "Iteration # 5136  loss is :  1.3213574\n",
      "Iteration # 5137  loss is :  1.321357\n",
      "Iteration # 5138  loss is :  1.3213568\n",
      "Iteration # 5139  loss is :  1.3213567\n",
      "Iteration # 5140  loss is :  1.3213564\n",
      "Iteration # 5141  loss is :  1.321356\n",
      "Iteration # 5142  loss is :  1.3213557\n",
      "Iteration # 5143  loss is :  1.3213555\n",
      "Iteration # 5144  loss is :  1.3213553\n",
      "Iteration # 5145  loss is :  1.3213551\n",
      "Iteration # 5146  loss is :  1.3213549\n",
      "Iteration # 5147  loss is :  1.3213547\n",
      "Iteration # 5148  loss is :  1.3213544\n",
      "Iteration # 5149  loss is :  1.3213542\n",
      "Iteration # 5150  loss is :  1.321354\n",
      "Iteration # 5151  loss is :  1.3213536\n",
      "Iteration # 5152  loss is :  1.3213536\n",
      "Iteration # 5153  loss is :  1.3213532\n",
      "Iteration # 5154  loss is :  1.321353\n",
      "Iteration # 5155  loss is :  1.3213528\n",
      "Iteration # 5156  loss is :  1.3213526\n",
      "Iteration # 5157  loss is :  1.3213524\n",
      "Iteration # 5158  loss is :  1.321352\n",
      "Iteration # 5159  loss is :  1.3213518\n",
      "Iteration # 5160  loss is :  1.3213515\n",
      "Iteration # 5161  loss is :  1.3213514\n",
      "Iteration # 5162  loss is :  1.321351\n",
      "Iteration # 5163  loss is :  1.3213507\n",
      "Iteration # 5164  loss is :  1.3213506\n",
      "Iteration # 5165  loss is :  1.3213505\n",
      "Iteration # 5166  loss is :  1.3213501\n",
      "Iteration # 5167  loss is :  1.3213499\n",
      "Iteration # 5168  loss is :  1.3213497\n",
      "Iteration # 5169  loss is :  1.3213493\n",
      "Iteration # 5170  loss is :  1.3213493\n",
      "Iteration # 5171  loss is :  1.3213491\n",
      "Iteration # 5172  loss is :  1.3213488\n",
      "Iteration # 5173  loss is :  1.3213485\n",
      "Iteration # 5174  loss is :  1.3213483\n",
      "Iteration # 5175  loss is :  1.321348\n",
      "Iteration # 5176  loss is :  1.3213477\n",
      "Iteration # 5177  loss is :  1.3213475\n",
      "Iteration # 5178  loss is :  1.3213472\n",
      "Iteration # 5179  loss is :  1.321347\n",
      "Iteration # 5180  loss is :  1.321347\n",
      "Iteration # 5181  loss is :  1.3213465\n",
      "Iteration # 5182  loss is :  1.3213464\n",
      "Iteration # 5183  loss is :  1.321346\n",
      "Iteration # 5184  loss is :  1.3213458\n",
      "Iteration # 5185  loss is :  1.3213456\n",
      "Iteration # 5186  loss is :  1.3213453\n",
      "Iteration # 5187  loss is :  1.3213451\n",
      "Iteration # 5188  loss is :  1.3213451\n",
      "Iteration # 5189  loss is :  1.3213447\n",
      "Iteration # 5190  loss is :  1.3213445\n",
      "Iteration # 5191  loss is :  1.3213443\n",
      "Iteration # 5192  loss is :  1.3213439\n",
      "Iteration # 5193  loss is :  1.3213437\n",
      "Iteration # 5194  loss is :  1.3213434\n",
      "Iteration # 5195  loss is :  1.3213432\n",
      "Iteration # 5196  loss is :  1.3213432\n",
      "Iteration # 5197  loss is :  1.321343\n",
      "Iteration # 5198  loss is :  1.3213426\n",
      "Iteration # 5199  loss is :  1.3213423\n",
      "Iteration # 5200  loss is :  1.3213421\n",
      "Iteration # 5201  loss is :  1.3213418\n",
      "Iteration # 5202  loss is :  1.3213416\n",
      "Iteration # 5203  loss is :  1.3213415\n",
      "Iteration # 5204  loss is :  1.3213413\n",
      "Iteration # 5205  loss is :  1.321341\n",
      "Iteration # 5206  loss is :  1.3213408\n",
      "Iteration # 5207  loss is :  1.3213404\n",
      "Iteration # 5208  loss is :  1.3213402\n",
      "Iteration # 5209  loss is :  1.3213401\n",
      "Iteration # 5210  loss is :  1.3213396\n",
      "Iteration # 5211  loss is :  1.3213396\n",
      "Iteration # 5212  loss is :  1.3213395\n",
      "Iteration # 5213  loss is :  1.3213391\n",
      "Iteration # 5214  loss is :  1.3213389\n",
      "Iteration # 5215  loss is :  1.3213387\n",
      "Iteration # 5216  loss is :  1.3213384\n",
      "Iteration # 5217  loss is :  1.321338\n",
      "Iteration # 5218  loss is :  1.3213379\n",
      "Iteration # 5219  loss is :  1.3213377\n",
      "Iteration # 5220  loss is :  1.3213376\n",
      "Iteration # 5221  loss is :  1.3213373\n",
      "Iteration # 5222  loss is :  1.321337\n",
      "Iteration # 5223  loss is :  1.3213367\n",
      "Iteration # 5224  loss is :  1.3213365\n",
      "Iteration # 5225  loss is :  1.3213363\n",
      "Iteration # 5226  loss is :  1.321336\n",
      "Iteration # 5227  loss is :  1.3213359\n",
      "Iteration # 5228  loss is :  1.3213356\n",
      "Iteration # 5229  loss is :  1.3213354\n",
      "Iteration # 5230  loss is :  1.3213352\n",
      "Iteration # 5231  loss is :  1.3213348\n",
      "Iteration # 5232  loss is :  1.3213347\n",
      "Iteration # 5233  loss is :  1.3213346\n",
      "Iteration # 5234  loss is :  1.3213341\n",
      "Iteration # 5235  loss is :  1.321334\n",
      "Iteration # 5236  loss is :  1.3213339\n",
      "Iteration # 5237  loss is :  1.3213338\n",
      "Iteration # 5238  loss is :  1.3213333\n",
      "Iteration # 5239  loss is :  1.3213333\n",
      "Iteration # 5240  loss is :  1.3213328\n",
      "Iteration # 5241  loss is :  1.3213327\n",
      "Iteration # 5242  loss is :  1.3213325\n",
      "Iteration # 5243  loss is :  1.3213322\n",
      "Iteration # 5244  loss is :  1.321332\n",
      "Iteration # 5245  loss is :  1.3213317\n",
      "Iteration # 5246  loss is :  1.3213315\n",
      "Iteration # 5247  loss is :  1.3213311\n",
      "Iteration # 5248  loss is :  1.321331\n",
      "Iteration # 5249  loss is :  1.3213309\n",
      "Iteration # 5250  loss is :  1.3213305\n",
      "Iteration # 5251  loss is :  1.3213303\n",
      "Iteration # 5252  loss is :  1.3213301\n",
      "Iteration # 5253  loss is :  1.3213301\n",
      "Iteration # 5254  loss is :  1.3213298\n",
      "Iteration # 5255  loss is :  1.3213296\n",
      "Iteration # 5256  loss is :  1.3213292\n",
      "Iteration # 5257  loss is :  1.321329\n",
      "Iteration # 5258  loss is :  1.3213288\n",
      "Iteration # 5259  loss is :  1.3213285\n",
      "Iteration # 5260  loss is :  1.3213284\n",
      "Iteration # 5261  loss is :  1.3213282\n",
      "Iteration # 5262  loss is :  1.3213279\n",
      "Iteration # 5263  loss is :  1.3213277\n",
      "Iteration # 5264  loss is :  1.3213274\n",
      "Iteration # 5265  loss is :  1.3213271\n",
      "Iteration # 5266  loss is :  1.321327\n",
      "Iteration # 5267  loss is :  1.3213267\n",
      "Iteration # 5268  loss is :  1.3213265\n",
      "Iteration # 5269  loss is :  1.3213263\n",
      "Iteration # 5270  loss is :  1.3213261\n",
      "Iteration # 5271  loss is :  1.3213258\n",
      "Iteration # 5272  loss is :  1.3213258\n",
      "Iteration # 5273  loss is :  1.3213254\n",
      "Iteration # 5274  loss is :  1.3213251\n",
      "Iteration # 5275  loss is :  1.3213248\n",
      "Iteration # 5276  loss is :  1.3213247\n",
      "Iteration # 5277  loss is :  1.3213243\n",
      "Iteration # 5278  loss is :  1.3213243\n",
      "Iteration # 5279  loss is :  1.3213241\n",
      "Iteration # 5280  loss is :  1.3213239\n",
      "Iteration # 5281  loss is :  1.3213236\n",
      "Iteration # 5282  loss is :  1.3213234\n",
      "Iteration # 5283  loss is :  1.3213232\n",
      "Iteration # 5284  loss is :  1.3213228\n",
      "Iteration # 5285  loss is :  1.3213227\n",
      "Iteration # 5286  loss is :  1.3213226\n",
      "Iteration # 5287  loss is :  1.3213223\n",
      "Iteration # 5288  loss is :  1.3213221\n",
      "Iteration # 5289  loss is :  1.3213218\n",
      "Iteration # 5290  loss is :  1.3213215\n",
      "Iteration # 5291  loss is :  1.3213214\n",
      "Iteration # 5292  loss is :  1.3213212\n",
      "Iteration # 5293  loss is :  1.3213208\n",
      "Iteration # 5294  loss is :  1.3213207\n",
      "Iteration # 5295  loss is :  1.3213205\n",
      "Iteration # 5296  loss is :  1.3213202\n",
      "Iteration # 5297  loss is :  1.32132\n",
      "Iteration # 5298  loss is :  1.3213199\n",
      "Iteration # 5299  loss is :  1.3213196\n",
      "Iteration # 5300  loss is :  1.3213193\n",
      "Iteration # 5301  loss is :  1.3213191\n",
      "Iteration # 5302  loss is :  1.3213187\n",
      "Iteration # 5303  loss is :  1.3213186\n",
      "Iteration # 5304  loss is :  1.3213184\n",
      "Iteration # 5305  loss is :  1.3213183\n",
      "Iteration # 5306  loss is :  1.321318\n",
      "Iteration # 5307  loss is :  1.3213178\n",
      "Iteration # 5308  loss is :  1.3213174\n",
      "Iteration # 5309  loss is :  1.3213173\n",
      "Iteration # 5310  loss is :  1.3213171\n",
      "Iteration # 5311  loss is :  1.321317\n",
      "Iteration # 5312  loss is :  1.3213167\n",
      "Iteration # 5313  loss is :  1.3213165\n",
      "Iteration # 5314  loss is :  1.3213162\n",
      "Iteration # 5315  loss is :  1.321316\n",
      "Iteration # 5316  loss is :  1.3213159\n",
      "Iteration # 5317  loss is :  1.3213156\n",
      "Iteration # 5318  loss is :  1.3213153\n",
      "Iteration # 5319  loss is :  1.321315\n",
      "Iteration # 5320  loss is :  1.3213149\n",
      "Iteration # 5321  loss is :  1.3213148\n",
      "Iteration # 5322  loss is :  1.3213145\n",
      "Iteration # 5323  loss is :  1.3213143\n",
      "Iteration # 5324  loss is :  1.321314\n",
      "Iteration # 5325  loss is :  1.3213139\n",
      "Iteration # 5326  loss is :  1.3213135\n",
      "Iteration # 5327  loss is :  1.3213133\n",
      "Iteration # 5328  loss is :  1.3213131\n",
      "Iteration # 5329  loss is :  1.3213129\n",
      "Iteration # 5330  loss is :  1.3213127\n",
      "Iteration # 5331  loss is :  1.3213125\n",
      "Iteration # 5332  loss is :  1.3213122\n",
      "Iteration # 5333  loss is :  1.321312\n",
      "Iteration # 5334  loss is :  1.3213118\n",
      "Iteration # 5335  loss is :  1.3213116\n",
      "Iteration # 5336  loss is :  1.3213115\n",
      "Iteration # 5337  loss is :  1.3213111\n",
      "Iteration # 5338  loss is :  1.321311\n",
      "Iteration # 5339  loss is :  1.3213108\n",
      "Iteration # 5340  loss is :  1.3213105\n",
      "Iteration # 5341  loss is :  1.3213103\n",
      "Iteration # 5342  loss is :  1.32131\n",
      "Iteration # 5343  loss is :  1.3213098\n",
      "Iteration # 5344  loss is :  1.3213097\n",
      "Iteration # 5345  loss is :  1.3213094\n",
      "Iteration # 5346  loss is :  1.3213091\n",
      "Iteration # 5347  loss is :  1.321309\n",
      "Iteration # 5348  loss is :  1.3213087\n",
      "Iteration # 5349  loss is :  1.3213087\n",
      "Iteration # 5350  loss is :  1.3213084\n",
      "Iteration # 5351  loss is :  1.3213081\n",
      "Iteration # 5352  loss is :  1.3213079\n",
      "Iteration # 5353  loss is :  1.3213077\n",
      "Iteration # 5354  loss is :  1.3213074\n",
      "Iteration # 5355  loss is :  1.3213073\n",
      "Iteration # 5356  loss is :  1.3213071\n",
      "Iteration # 5357  loss is :  1.3213068\n",
      "Iteration # 5358  loss is :  1.3213066\n",
      "Iteration # 5359  loss is :  1.3213065\n",
      "Iteration # 5360  loss is :  1.3213062\n",
      "Iteration # 5361  loss is :  1.321306\n",
      "Iteration # 5362  loss is :  1.3213056\n",
      "Iteration # 5363  loss is :  1.3213055\n",
      "Iteration # 5364  loss is :  1.3213053\n",
      "Iteration # 5365  loss is :  1.3213053\n",
      "Iteration # 5366  loss is :  1.3213049\n",
      "Iteration # 5367  loss is :  1.3213047\n",
      "Iteration # 5368  loss is :  1.3213044\n",
      "Iteration # 5369  loss is :  1.3213042\n",
      "Iteration # 5370  loss is :  1.3213041\n",
      "Iteration # 5371  loss is :  1.3213038\n",
      "Iteration # 5372  loss is :  1.3213035\n",
      "Iteration # 5373  loss is :  1.3213034\n",
      "Iteration # 5374  loss is :  1.3213032\n",
      "Iteration # 5375  loss is :  1.321303\n",
      "Iteration # 5376  loss is :  1.3213028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 5377  loss is :  1.3213025\n",
      "Iteration # 5378  loss is :  1.3213022\n",
      "Iteration # 5379  loss is :  1.321302\n",
      "Iteration # 5380  loss is :  1.3213018\n",
      "Iteration # 5381  loss is :  1.3213017\n",
      "Iteration # 5382  loss is :  1.3213015\n",
      "Iteration # 5383  loss is :  1.3213012\n",
      "Iteration # 5384  loss is :  1.3213012\n",
      "Iteration # 5385  loss is :  1.3213007\n",
      "Iteration # 5386  loss is :  1.3213006\n",
      "Iteration # 5387  loss is :  1.3213004\n",
      "Iteration # 5388  loss is :  1.3213\n",
      "Iteration # 5389  loss is :  1.3212998\n",
      "Iteration # 5390  loss is :  1.3212997\n",
      "Iteration # 5391  loss is :  1.3212996\n",
      "Iteration # 5392  loss is :  1.3212993\n",
      "Iteration # 5393  loss is :  1.3212991\n",
      "Iteration # 5394  loss is :  1.3212988\n",
      "Iteration # 5395  loss is :  1.3212986\n",
      "Iteration # 5396  loss is :  1.3212985\n",
      "Iteration # 5397  loss is :  1.3212982\n",
      "Iteration # 5398  loss is :  1.3212979\n",
      "Iteration # 5399  loss is :  1.3212978\n",
      "Iteration # 5400  loss is :  1.3212975\n",
      "Iteration # 5401  loss is :  1.3212974\n",
      "Iteration # 5402  loss is :  1.3212972\n",
      "Iteration # 5403  loss is :  1.3212969\n",
      "Iteration # 5404  loss is :  1.3212966\n",
      "Iteration # 5405  loss is :  1.3212966\n",
      "Iteration # 5406  loss is :  1.3212963\n",
      "Iteration # 5407  loss is :  1.3212961\n",
      "Iteration # 5408  loss is :  1.3212957\n",
      "Iteration # 5409  loss is :  1.3212957\n",
      "Iteration # 5410  loss is :  1.3212955\n",
      "Iteration # 5411  loss is :  1.3212951\n",
      "Iteration # 5412  loss is :  1.3212951\n",
      "Iteration # 5413  loss is :  1.3212948\n",
      "Iteration # 5414  loss is :  1.3212944\n",
      "Iteration # 5415  loss is :  1.3212943\n",
      "Iteration # 5416  loss is :  1.3212942\n",
      "Iteration # 5417  loss is :  1.321294\n",
      "Iteration # 5418  loss is :  1.3212937\n",
      "Iteration # 5419  loss is :  1.3212936\n",
      "Iteration # 5420  loss is :  1.3212934\n",
      "Iteration # 5421  loss is :  1.3212931\n",
      "Iteration # 5422  loss is :  1.3212929\n",
      "Iteration # 5423  loss is :  1.3212926\n",
      "Iteration # 5424  loss is :  1.3212925\n",
      "Iteration # 5425  loss is :  1.3212922\n",
      "Iteration # 5426  loss is :  1.321292\n",
      "Iteration # 5427  loss is :  1.3212918\n",
      "Iteration # 5428  loss is :  1.3212917\n",
      "Iteration # 5429  loss is :  1.3212914\n",
      "Iteration # 5430  loss is :  1.3212913\n",
      "Iteration # 5431  loss is :  1.321291\n",
      "Iteration # 5432  loss is :  1.3212909\n",
      "Iteration # 5433  loss is :  1.3212905\n",
      "Iteration # 5434  loss is :  1.3212904\n",
      "Iteration # 5435  loss is :  1.3212901\n",
      "Iteration # 5436  loss is :  1.32129\n",
      "Iteration # 5437  loss is :  1.3212898\n",
      "Iteration # 5438  loss is :  1.3212895\n",
      "Iteration # 5439  loss is :  1.3212894\n",
      "Iteration # 5440  loss is :  1.3212892\n",
      "Iteration # 5441  loss is :  1.3212888\n",
      "Iteration # 5442  loss is :  1.3212887\n",
      "Iteration # 5443  loss is :  1.3212885\n",
      "Iteration # 5444  loss is :  1.3212881\n",
      "Iteration # 5445  loss is :  1.3212881\n",
      "Iteration # 5446  loss is :  1.3212879\n",
      "Iteration # 5447  loss is :  1.3212875\n",
      "Iteration # 5448  loss is :  1.3212875\n",
      "Iteration # 5449  loss is :  1.3212873\n",
      "Iteration # 5450  loss is :  1.3212872\n",
      "Iteration # 5451  loss is :  1.3212868\n",
      "Iteration # 5452  loss is :  1.3212867\n",
      "Iteration # 5453  loss is :  1.3212863\n",
      "Iteration # 5454  loss is :  1.3212862\n",
      "Iteration # 5455  loss is :  1.321286\n",
      "Iteration # 5456  loss is :  1.3212857\n",
      "Iteration # 5457  loss is :  1.3212856\n",
      "Iteration # 5458  loss is :  1.3212854\n",
      "Iteration # 5459  loss is :  1.3212851\n",
      "Iteration # 5460  loss is :  1.321285\n",
      "Iteration # 5461  loss is :  1.3212847\n",
      "Iteration # 5462  loss is :  1.3212845\n",
      "Iteration # 5463  loss is :  1.3212843\n",
      "Iteration # 5464  loss is :  1.321284\n",
      "Iteration # 5465  loss is :  1.321284\n",
      "Iteration # 5466  loss is :  1.3212838\n",
      "Iteration # 5467  loss is :  1.3212835\n",
      "Iteration # 5468  loss is :  1.3212832\n",
      "Iteration # 5469  loss is :  1.321283\n",
      "Iteration # 5470  loss is :  1.3212827\n",
      "Iteration # 5471  loss is :  1.3212826\n",
      "Iteration # 5472  loss is :  1.3212825\n",
      "Iteration # 5473  loss is :  1.3212824\n",
      "Iteration # 5474  loss is :  1.321282\n",
      "Iteration # 5475  loss is :  1.3212819\n",
      "Iteration # 5476  loss is :  1.3212817\n",
      "Iteration # 5477  loss is :  1.3212813\n",
      "Iteration # 5478  loss is :  1.3212813\n",
      "Iteration # 5479  loss is :  1.3212811\n",
      "Iteration # 5480  loss is :  1.3212808\n",
      "Iteration # 5481  loss is :  1.3212805\n",
      "Iteration # 5482  loss is :  1.3212804\n",
      "Iteration # 5483  loss is :  1.3212802\n",
      "Iteration # 5484  loss is :  1.32128\n",
      "Iteration # 5485  loss is :  1.3212798\n",
      "Iteration # 5486  loss is :  1.3212795\n",
      "Iteration # 5487  loss is :  1.3212792\n",
      "Iteration # 5488  loss is :  1.3212792\n",
      "Iteration # 5489  loss is :  1.3212789\n",
      "Iteration # 5490  loss is :  1.3212787\n",
      "Iteration # 5491  loss is :  1.3212786\n",
      "Iteration # 5492  loss is :  1.3212783\n",
      "Iteration # 5493  loss is :  1.3212783\n",
      "Iteration # 5494  loss is :  1.321278\n",
      "Iteration # 5495  loss is :  1.3212779\n",
      "Iteration # 5496  loss is :  1.3212776\n",
      "Iteration # 5497  loss is :  1.3212773\n",
      "Iteration # 5498  loss is :  1.321277\n",
      "Iteration # 5499  loss is :  1.3212769\n",
      "Iteration # 5500  loss is :  1.3212767\n",
      "Iteration # 5501  loss is :  1.3212765\n",
      "Iteration # 5502  loss is :  1.3212763\n",
      "Iteration # 5503  loss is :  1.3212761\n",
      "Iteration # 5504  loss is :  1.321276\n",
      "Iteration # 5505  loss is :  1.3212757\n",
      "Iteration # 5506  loss is :  1.3212755\n",
      "Iteration # 5507  loss is :  1.3212754\n",
      "Iteration # 5508  loss is :  1.3212751\n",
      "Iteration # 5509  loss is :  1.3212749\n",
      "Iteration # 5510  loss is :  1.3212748\n",
      "Iteration # 5511  loss is :  1.3212744\n",
      "Iteration # 5512  loss is :  1.3212744\n",
      "Iteration # 5513  loss is :  1.3212742\n",
      "Iteration # 5514  loss is :  1.3212739\n",
      "Iteration # 5515  loss is :  1.3212736\n",
      "Iteration # 5516  loss is :  1.3212734\n",
      "Iteration # 5517  loss is :  1.3212733\n",
      "Iteration # 5518  loss is :  1.321273\n",
      "Iteration # 5519  loss is :  1.3212729\n",
      "Iteration # 5520  loss is :  1.3212729\n",
      "Iteration # 5521  loss is :  1.3212726\n",
      "Iteration # 5522  loss is :  1.3212723\n",
      "Iteration # 5523  loss is :  1.3212721\n",
      "Iteration # 5524  loss is :  1.3212719\n",
      "Iteration # 5525  loss is :  1.3212715\n",
      "Iteration # 5526  loss is :  1.3212714\n",
      "Iteration # 5527  loss is :  1.3212712\n",
      "Iteration # 5528  loss is :  1.321271\n",
      "Iteration # 5529  loss is :  1.3212708\n",
      "Iteration # 5530  loss is :  1.3212707\n",
      "Iteration # 5531  loss is :  1.3212706\n",
      "Iteration # 5532  loss is :  1.3212701\n",
      "Iteration # 5533  loss is :  1.3212701\n",
      "Iteration # 5534  loss is :  1.3212699\n",
      "Iteration # 5535  loss is :  1.3212696\n",
      "Iteration # 5536  loss is :  1.3212694\n",
      "Iteration # 5537  loss is :  1.3212692\n",
      "Iteration # 5538  loss is :  1.321269\n",
      "Iteration # 5539  loss is :  1.3212688\n",
      "Iteration # 5540  loss is :  1.3212687\n",
      "Iteration # 5541  loss is :  1.3212684\n",
      "Iteration # 5542  loss is :  1.3212682\n",
      "Iteration # 5543  loss is :  1.321268\n",
      "Iteration # 5544  loss is :  1.3212678\n",
      "Iteration # 5545  loss is :  1.3212677\n",
      "Iteration # 5546  loss is :  1.3212674\n",
      "Iteration # 5547  loss is :  1.3212672\n",
      "Iteration # 5548  loss is :  1.3212671\n",
      "Iteration # 5549  loss is :  1.3212669\n",
      "Iteration # 5550  loss is :  1.3212667\n",
      "Iteration # 5551  loss is :  1.3212665\n",
      "Iteration # 5552  loss is :  1.3212664\n",
      "Iteration # 5553  loss is :  1.321266\n",
      "Iteration # 5554  loss is :  1.3212658\n",
      "Iteration # 5555  loss is :  1.3212658\n",
      "Iteration # 5556  loss is :  1.3212655\n",
      "Iteration # 5557  loss is :  1.3212652\n",
      "Iteration # 5558  loss is :  1.3212651\n",
      "Iteration # 5559  loss is :  1.321265\n",
      "Iteration # 5560  loss is :  1.3212646\n",
      "Iteration # 5561  loss is :  1.3212645\n",
      "Iteration # 5562  loss is :  1.3212643\n",
      "Iteration # 5563  loss is :  1.3212639\n",
      "Iteration # 5564  loss is :  1.3212639\n",
      "Iteration # 5565  loss is :  1.3212637\n",
      "Iteration # 5566  loss is :  1.3212636\n",
      "Iteration # 5567  loss is :  1.3212633\n",
      "Iteration # 5568  loss is :  1.3212631\n",
      "Iteration # 5569  loss is :  1.321263\n",
      "Iteration # 5570  loss is :  1.3212626\n",
      "Iteration # 5571  loss is :  1.3212626\n",
      "Iteration # 5572  loss is :  1.3212624\n",
      "Iteration # 5573  loss is :  1.3212621\n",
      "Iteration # 5574  loss is :  1.3212619\n",
      "Iteration # 5575  loss is :  1.3212616\n",
      "Iteration # 5576  loss is :  1.3212615\n",
      "Iteration # 5577  loss is :  1.3212613\n",
      "Iteration # 5578  loss is :  1.321261\n",
      "Iteration # 5579  loss is :  1.3212608\n",
      "Iteration # 5580  loss is :  1.3212607\n",
      "Iteration # 5581  loss is :  1.3212606\n",
      "Iteration # 5582  loss is :  1.3212603\n",
      "Iteration # 5583  loss is :  1.3212602\n",
      "Iteration # 5584  loss is :  1.3212599\n",
      "Iteration # 5585  loss is :  1.3212596\n",
      "Iteration # 5586  loss is :  1.3212595\n",
      "Iteration # 5587  loss is :  1.3212595\n",
      "Iteration # 5588  loss is :  1.3212591\n",
      "Iteration # 5589  loss is :  1.3212589\n",
      "Iteration # 5590  loss is :  1.3212587\n",
      "Iteration # 5591  loss is :  1.3212585\n",
      "Iteration # 5592  loss is :  1.3212583\n",
      "Iteration # 5593  loss is :  1.3212582\n",
      "Iteration # 5594  loss is :  1.3212581\n",
      "Iteration # 5595  loss is :  1.3212577\n",
      "Iteration # 5596  loss is :  1.3212576\n",
      "Iteration # 5597  loss is :  1.3212574\n",
      "Iteration # 5598  loss is :  1.3212572\n",
      "Iteration # 5599  loss is :  1.321257\n",
      "Iteration # 5600  loss is :  1.3212568\n",
      "Iteration # 5601  loss is :  1.3212566\n",
      "Iteration # 5602  loss is :  1.3212564\n",
      "Iteration # 5603  loss is :  1.3212562\n",
      "Iteration # 5604  loss is :  1.3212559\n",
      "Iteration # 5605  loss is :  1.3212557\n",
      "Iteration # 5606  loss is :  1.3212556\n",
      "Iteration # 5607  loss is :  1.3212554\n",
      "Iteration # 5608  loss is :  1.3212553\n",
      "Iteration # 5609  loss is :  1.3212551\n",
      "Iteration # 5610  loss is :  1.3212548\n",
      "Iteration # 5611  loss is :  1.3212546\n",
      "Iteration # 5612  loss is :  1.3212545\n",
      "Iteration # 5613  loss is :  1.3212541\n",
      "Iteration # 5614  loss is :  1.321254\n",
      "Iteration # 5615  loss is :  1.3212538\n",
      "Iteration # 5616  loss is :  1.3212538\n",
      "Iteration # 5617  loss is :  1.3212535\n",
      "Iteration # 5618  loss is :  1.3212533\n",
      "Iteration # 5619  loss is :  1.3212532\n",
      "Iteration # 5620  loss is :  1.3212528\n",
      "Iteration # 5621  loss is :  1.3212527\n",
      "Iteration # 5622  loss is :  1.3212525\n",
      "Iteration # 5623  loss is :  1.3212522\n",
      "Iteration # 5624  loss is :  1.3212521\n",
      "Iteration # 5625  loss is :  1.3212519\n",
      "Iteration # 5626  loss is :  1.3212519\n",
      "Iteration # 5627  loss is :  1.3212515\n",
      "Iteration # 5628  loss is :  1.3212514\n",
      "Iteration # 5629  loss is :  1.3212512\n",
      "Iteration # 5630  loss is :  1.3212509\n",
      "Iteration # 5631  loss is :  1.3212508\n",
      "Iteration # 5632  loss is :  1.3212506\n",
      "Iteration # 5633  loss is :  1.3212503\n",
      "Iteration # 5634  loss is :  1.3212502\n",
      "Iteration # 5635  loss is :  1.32125\n",
      "Iteration # 5636  loss is :  1.3212498\n",
      "Iteration # 5637  loss is :  1.3212496\n",
      "Iteration # 5638  loss is :  1.3212495\n",
      "Iteration # 5639  loss is :  1.3212492\n",
      "Iteration # 5640  loss is :  1.321249\n",
      "Iteration # 5641  loss is :  1.3212489\n",
      "Iteration # 5642  loss is :  1.3212487\n",
      "Iteration # 5643  loss is :  1.3212484\n",
      "Iteration # 5644  loss is :  1.3212483\n",
      "Iteration # 5645  loss is :  1.321248\n",
      "Iteration # 5646  loss is :  1.3212479\n",
      "Iteration # 5647  loss is :  1.3212478\n",
      "Iteration # 5648  loss is :  1.3212476\n",
      "Iteration # 5649  loss is :  1.3212473\n",
      "Iteration # 5650  loss is :  1.3212471\n",
      "Iteration # 5651  loss is :  1.321247\n",
      "Iteration # 5652  loss is :  1.3212466\n",
      "Iteration # 5653  loss is :  1.3212465\n",
      "Iteration # 5654  loss is :  1.3212463\n",
      "Iteration # 5655  loss is :  1.3212461\n",
      "Iteration # 5656  loss is :  1.321246\n",
      "Iteration # 5657  loss is :  1.3212458\n",
      "Iteration # 5658  loss is :  1.3212457\n",
      "Iteration # 5659  loss is :  1.3212456\n",
      "Iteration # 5660  loss is :  1.3212452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 5661  loss is :  1.3212451\n",
      "Iteration # 5662  loss is :  1.3212448\n",
      "Iteration # 5663  loss is :  1.3212446\n",
      "Iteration # 5664  loss is :  1.3212444\n",
      "Iteration # 5665  loss is :  1.3212442\n",
      "Iteration # 5666  loss is :  1.3212442\n",
      "Iteration # 5667  loss is :  1.3212439\n",
      "Iteration # 5668  loss is :  1.3212436\n",
      "Iteration # 5669  loss is :  1.3212436\n",
      "Iteration # 5670  loss is :  1.3212433\n",
      "Iteration # 5671  loss is :  1.3212432\n",
      "Iteration # 5672  loss is :  1.321243\n",
      "Iteration # 5673  loss is :  1.3212428\n",
      "Iteration # 5674  loss is :  1.3212425\n",
      "Iteration # 5675  loss is :  1.3212423\n",
      "Iteration # 5676  loss is :  1.3212422\n",
      "Iteration # 5677  loss is :  1.3212421\n",
      "Iteration # 5678  loss is :  1.3212417\n",
      "Iteration # 5679  loss is :  1.3212417\n",
      "Iteration # 5680  loss is :  1.3212415\n",
      "Iteration # 5681  loss is :  1.3212413\n",
      "Iteration # 5682  loss is :  1.3212409\n",
      "Iteration # 5683  loss is :  1.3212409\n",
      "Iteration # 5684  loss is :  1.3212407\n",
      "Iteration # 5685  loss is :  1.3212404\n",
      "Iteration # 5686  loss is :  1.3212404\n",
      "Iteration # 5687  loss is :  1.3212402\n",
      "Iteration # 5688  loss is :  1.32124\n",
      "Iteration # 5689  loss is :  1.3212398\n",
      "Iteration # 5690  loss is :  1.3212396\n",
      "Iteration # 5691  loss is :  1.3212394\n",
      "Iteration # 5692  loss is :  1.3212392\n",
      "Iteration # 5693  loss is :  1.3212389\n",
      "Iteration # 5694  loss is :  1.3212388\n",
      "Iteration # 5695  loss is :  1.3212385\n",
      "Iteration # 5696  loss is :  1.3212384\n",
      "Iteration # 5697  loss is :  1.3212383\n",
      "Iteration # 5698  loss is :  1.321238\n",
      "Iteration # 5699  loss is :  1.321238\n",
      "Iteration # 5700  loss is :  1.3212377\n",
      "Iteration # 5701  loss is :  1.3212374\n",
      "Iteration # 5702  loss is :  1.3212372\n",
      "Iteration # 5703  loss is :  1.321237\n",
      "Iteration # 5704  loss is :  1.3212368\n",
      "Iteration # 5705  loss is :  1.3212367\n",
      "Iteration # 5706  loss is :  1.3212366\n",
      "Iteration # 5707  loss is :  1.3212364\n",
      "Iteration # 5708  loss is :  1.3212361\n",
      "Iteration # 5709  loss is :  1.321236\n",
      "Iteration # 5710  loss is :  1.3212359\n",
      "Iteration # 5711  loss is :  1.3212355\n",
      "Iteration # 5712  loss is :  1.3212355\n",
      "Iteration # 5713  loss is :  1.3212353\n",
      "Iteration # 5714  loss is :  1.3212351\n",
      "Iteration # 5715  loss is :  1.321235\n",
      "Iteration # 5716  loss is :  1.3212347\n",
      "Iteration # 5717  loss is :  1.3212346\n",
      "Iteration # 5718  loss is :  1.3212343\n",
      "Iteration # 5719  loss is :  1.3212341\n",
      "Iteration # 5720  loss is :  1.321234\n",
      "Iteration # 5721  loss is :  1.3212337\n",
      "Iteration # 5722  loss is :  1.3212335\n",
      "Iteration # 5723  loss is :  1.3212334\n",
      "Iteration # 5724  loss is :  1.321233\n",
      "Iteration # 5725  loss is :  1.3212332\n",
      "Iteration # 5726  loss is :  1.3212328\n",
      "Iteration # 5727  loss is :  1.3212327\n",
      "Iteration # 5728  loss is :  1.3212324\n",
      "Iteration # 5729  loss is :  1.3212323\n",
      "Iteration # 5730  loss is :  1.3212321\n",
      "Iteration # 5731  loss is :  1.321232\n",
      "Iteration # 5732  loss is :  1.3212317\n",
      "Iteration # 5733  loss is :  1.3212315\n",
      "Iteration # 5734  loss is :  1.3212312\n",
      "Iteration # 5735  loss is :  1.3212312\n",
      "Iteration # 5736  loss is :  1.321231\n",
      "Iteration # 5737  loss is :  1.3212308\n",
      "Iteration # 5738  loss is :  1.3212306\n",
      "Iteration # 5739  loss is :  1.3212305\n",
      "Iteration # 5740  loss is :  1.3212303\n",
      "Iteration # 5741  loss is :  1.32123\n",
      "Iteration # 5742  loss is :  1.3212298\n",
      "Iteration # 5743  loss is :  1.3212298\n",
      "Iteration # 5744  loss is :  1.3212293\n",
      "Iteration # 5745  loss is :  1.3212293\n",
      "Iteration # 5746  loss is :  1.3212291\n",
      "Iteration # 5747  loss is :  1.321229\n",
      "Iteration # 5748  loss is :  1.3212287\n",
      "Iteration # 5749  loss is :  1.3212286\n",
      "Iteration # 5750  loss is :  1.3212284\n",
      "Iteration # 5751  loss is :  1.3212284\n",
      "Iteration # 5752  loss is :  1.3212279\n",
      "Iteration # 5753  loss is :  1.3212278\n",
      "Iteration # 5754  loss is :  1.3212277\n",
      "Iteration # 5755  loss is :  1.3212276\n",
      "Iteration # 5756  loss is :  1.3212272\n",
      "Iteration # 5757  loss is :  1.3212271\n",
      "Iteration # 5758  loss is :  1.3212271\n",
      "Iteration # 5759  loss is :  1.3212268\n",
      "Iteration # 5760  loss is :  1.3212266\n",
      "Iteration # 5761  loss is :  1.3212264\n",
      "Iteration # 5762  loss is :  1.3212262\n",
      "Iteration # 5763  loss is :  1.3212261\n",
      "Iteration # 5764  loss is :  1.3212258\n",
      "Iteration # 5765  loss is :  1.3212256\n",
      "Iteration # 5766  loss is :  1.3212255\n",
      "Iteration # 5767  loss is :  1.3212253\n",
      "Iteration # 5768  loss is :  1.3212252\n",
      "Iteration # 5769  loss is :  1.3212249\n",
      "Iteration # 5770  loss is :  1.3212248\n",
      "Iteration # 5771  loss is :  1.3212246\n",
      "Iteration # 5772  loss is :  1.3212243\n",
      "Iteration # 5773  loss is :  1.3212242\n",
      "Iteration # 5774  loss is :  1.3212241\n",
      "Iteration # 5775  loss is :  1.3212239\n",
      "Iteration # 5776  loss is :  1.3212236\n",
      "Iteration # 5777  loss is :  1.3212235\n",
      "Iteration # 5778  loss is :  1.3212233\n",
      "Iteration # 5779  loss is :  1.3212231\n",
      "Iteration # 5780  loss is :  1.3212229\n",
      "Iteration # 5781  loss is :  1.3212229\n",
      "Iteration # 5782  loss is :  1.3212225\n",
      "Iteration # 5783  loss is :  1.3212224\n",
      "Iteration # 5784  loss is :  1.3212222\n",
      "Iteration # 5785  loss is :  1.3212221\n",
      "Iteration # 5786  loss is :  1.3212218\n",
      "Iteration # 5787  loss is :  1.3212216\n",
      "Iteration # 5788  loss is :  1.3212216\n",
      "Iteration # 5789  loss is :  1.3212214\n",
      "Iteration # 5790  loss is :  1.3212214\n",
      "Iteration # 5791  loss is :  1.321221\n",
      "Iteration # 5792  loss is :  1.3212209\n",
      "Iteration # 5793  loss is :  1.3212206\n",
      "Iteration # 5794  loss is :  1.3212203\n",
      "Iteration # 5795  loss is :  1.3212203\n",
      "Iteration # 5796  loss is :  1.32122\n",
      "Iteration # 5797  loss is :  1.32122\n",
      "Iteration # 5798  loss is :  1.3212197\n",
      "Iteration # 5799  loss is :  1.3212194\n",
      "Iteration # 5800  loss is :  1.3212194\n",
      "Iteration # 5801  loss is :  1.3212192\n",
      "Iteration # 5802  loss is :  1.321219\n",
      "Iteration # 5803  loss is :  1.3212188\n",
      "Iteration # 5804  loss is :  1.3212186\n",
      "Iteration # 5805  loss is :  1.3212184\n",
      "Iteration # 5806  loss is :  1.3212184\n",
      "Iteration # 5807  loss is :  1.3212181\n",
      "Iteration # 5808  loss is :  1.3212179\n",
      "Iteration # 5809  loss is :  1.3212177\n",
      "Iteration # 5810  loss is :  1.3212175\n",
      "Iteration # 5811  loss is :  1.3212174\n",
      "Iteration # 5812  loss is :  1.3212173\n",
      "Iteration # 5813  loss is :  1.3212171\n",
      "Iteration # 5814  loss is :  1.3212168\n",
      "Iteration # 5815  loss is :  1.3212166\n",
      "Iteration # 5816  loss is :  1.3212166\n",
      "Iteration # 5817  loss is :  1.3212162\n",
      "Iteration # 5818  loss is :  1.3212162\n",
      "Iteration # 5819  loss is :  1.3212159\n",
      "Iteration # 5820  loss is :  1.3212157\n",
      "Iteration # 5821  loss is :  1.3212156\n",
      "Iteration # 5822  loss is :  1.3212155\n",
      "Iteration # 5823  loss is :  1.3212153\n",
      "Iteration # 5824  loss is :  1.321215\n",
      "Iteration # 5825  loss is :  1.3212149\n",
      "Iteration # 5826  loss is :  1.3212147\n",
      "Iteration # 5827  loss is :  1.3212144\n",
      "Iteration # 5828  loss is :  1.3212144\n",
      "Iteration # 5829  loss is :  1.3212142\n",
      "Iteration # 5830  loss is :  1.321214\n",
      "Iteration # 5831  loss is :  1.3212138\n",
      "Iteration # 5832  loss is :  1.3212137\n",
      "Iteration # 5833  loss is :  1.3212134\n",
      "Iteration # 5834  loss is :  1.3212134\n",
      "Iteration # 5835  loss is :  1.3212132\n",
      "Iteration # 5836  loss is :  1.321213\n",
      "Iteration # 5837  loss is :  1.3212126\n",
      "Iteration # 5838  loss is :  1.3212125\n",
      "Iteration # 5839  loss is :  1.3212124\n",
      "Iteration # 5840  loss is :  1.3212123\n",
      "Iteration # 5841  loss is :  1.3212119\n",
      "Iteration # 5842  loss is :  1.3212118\n",
      "Iteration # 5843  loss is :  1.3212117\n",
      "Iteration # 5844  loss is :  1.3212115\n",
      "Iteration # 5845  loss is :  1.3212115\n",
      "Iteration # 5846  loss is :  1.3212112\n",
      "Iteration # 5847  loss is :  1.3212111\n",
      "Iteration # 5848  loss is :  1.3212107\n",
      "Iteration # 5849  loss is :  1.3212106\n",
      "Iteration # 5850  loss is :  1.3212104\n",
      "Iteration # 5851  loss is :  1.3212103\n",
      "Iteration # 5852  loss is :  1.32121\n",
      "Iteration # 5853  loss is :  1.3212099\n",
      "Iteration # 5854  loss is :  1.3212098\n",
      "Iteration # 5855  loss is :  1.3212097\n",
      "Iteration # 5856  loss is :  1.3212093\n",
      "Iteration # 5857  loss is :  1.3212092\n",
      "Iteration # 5858  loss is :  1.3212091\n",
      "Iteration # 5859  loss is :  1.3212088\n",
      "Iteration # 5860  loss is :  1.3212087\n",
      "Iteration # 5861  loss is :  1.3212086\n",
      "Iteration # 5862  loss is :  1.3212085\n",
      "Iteration # 5863  loss is :  1.3212082\n",
      "Iteration # 5864  loss is :  1.321208\n",
      "Iteration # 5865  loss is :  1.321208\n",
      "Iteration # 5866  loss is :  1.3212078\n",
      "Iteration # 5867  loss is :  1.3212075\n",
      "Iteration # 5868  loss is :  1.3212073\n",
      "Iteration # 5869  loss is :  1.3212072\n",
      "Iteration # 5870  loss is :  1.3212069\n",
      "Iteration # 5871  loss is :  1.3212068\n",
      "Iteration # 5872  loss is :  1.3212066\n",
      "Iteration # 5873  loss is :  1.3212063\n",
      "Iteration # 5874  loss is :  1.3212063\n",
      "Iteration # 5875  loss is :  1.3212062\n",
      "Iteration # 5876  loss is :  1.3212059\n",
      "Iteration # 5877  loss is :  1.3212057\n",
      "Iteration # 5878  loss is :  1.3212056\n",
      "Iteration # 5879  loss is :  1.3212055\n",
      "Iteration # 5880  loss is :  1.3212051\n",
      "Iteration # 5881  loss is :  1.321205\n",
      "Iteration # 5882  loss is :  1.321205\n",
      "Iteration # 5883  loss is :  1.3212048\n",
      "Iteration # 5884  loss is :  1.3212044\n",
      "Iteration # 5885  loss is :  1.3212043\n",
      "Iteration # 5886  loss is :  1.3212042\n",
      "Iteration # 5887  loss is :  1.3212041\n",
      "Iteration # 5888  loss is :  1.3212038\n",
      "Iteration # 5889  loss is :  1.3212037\n",
      "Iteration # 5890  loss is :  1.3212036\n",
      "Iteration # 5891  loss is :  1.3212034\n",
      "Iteration # 5892  loss is :  1.3212031\n",
      "Iteration # 5893  loss is :  1.3212029\n",
      "Iteration # 5894  loss is :  1.3212028\n",
      "Iteration # 5895  loss is :  1.3212026\n",
      "Iteration # 5896  loss is :  1.3212023\n",
      "Iteration # 5897  loss is :  1.3212023\n",
      "Iteration # 5898  loss is :  1.3212022\n",
      "Iteration # 5899  loss is :  1.321202\n",
      "Iteration # 5900  loss is :  1.3212018\n",
      "Iteration # 5901  loss is :  1.3212016\n",
      "Iteration # 5902  loss is :  1.3212013\n",
      "Iteration # 5903  loss is :  1.3212013\n",
      "Iteration # 5904  loss is :  1.3212011\n",
      "Iteration # 5905  loss is :  1.3212008\n",
      "Iteration # 5906  loss is :  1.3212007\n",
      "Iteration # 5907  loss is :  1.3212005\n",
      "Iteration # 5908  loss is :  1.3212004\n",
      "Iteration # 5909  loss is :  1.3212003\n",
      "Iteration # 5910  loss is :  1.3212\n",
      "Iteration # 5911  loss is :  1.3212\n",
      "Iteration # 5912  loss is :  1.3211997\n",
      "Iteration # 5913  loss is :  1.3211994\n",
      "Iteration # 5914  loss is :  1.3211994\n",
      "Iteration # 5915  loss is :  1.3211992\n",
      "Iteration # 5916  loss is :  1.3211989\n",
      "Iteration # 5917  loss is :  1.3211988\n",
      "Iteration # 5918  loss is :  1.3211986\n",
      "Iteration # 5919  loss is :  1.3211985\n",
      "Iteration # 5920  loss is :  1.3211985\n",
      "Iteration # 5921  loss is :  1.3211982\n",
      "Iteration # 5922  loss is :  1.3211981\n",
      "Iteration # 5923  loss is :  1.3211979\n",
      "Iteration # 5924  loss is :  1.3211977\n",
      "Iteration # 5925  loss is :  1.3211975\n",
      "Iteration # 5926  loss is :  1.3211973\n",
      "Iteration # 5927  loss is :  1.321197\n",
      "Iteration # 5928  loss is :  1.3211969\n",
      "Iteration # 5929  loss is :  1.3211967\n",
      "Iteration # 5930  loss is :  1.3211967\n",
      "Iteration # 5931  loss is :  1.3211966\n",
      "Iteration # 5932  loss is :  1.3211964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 5933  loss is :  1.3211962\n",
      "Iteration # 5934  loss is :  1.321196\n",
      "Iteration # 5935  loss is :  1.3211958\n",
      "Iteration # 5936  loss is :  1.3211957\n",
      "Iteration # 5937  loss is :  1.3211954\n",
      "Iteration # 5938  loss is :  1.3211954\n",
      "Iteration # 5939  loss is :  1.3211951\n",
      "Iteration # 5940  loss is :  1.3211948\n",
      "Iteration # 5941  loss is :  1.3211946\n",
      "Iteration # 5942  loss is :  1.3211946\n",
      "Iteration # 5943  loss is :  1.3211944\n",
      "Iteration # 5944  loss is :  1.3211942\n",
      "Iteration # 5945  loss is :  1.321194\n",
      "Iteration # 5946  loss is :  1.3211938\n",
      "Iteration # 5947  loss is :  1.3211938\n",
      "Iteration # 5948  loss is :  1.3211936\n",
      "Iteration # 5949  loss is :  1.3211933\n",
      "Iteration # 5950  loss is :  1.3211932\n",
      "Iteration # 5951  loss is :  1.321193\n",
      "Iteration # 5952  loss is :  1.3211929\n",
      "Iteration # 5953  loss is :  1.3211927\n",
      "Iteration # 5954  loss is :  1.3211926\n",
      "Iteration # 5955  loss is :  1.3211925\n",
      "Iteration # 5956  loss is :  1.3211924\n",
      "Iteration # 5957  loss is :  1.321192\n",
      "Iteration # 5958  loss is :  1.3211918\n",
      "Iteration # 5959  loss is :  1.3211918\n",
      "Iteration # 5960  loss is :  1.3211917\n",
      "Iteration # 5961  loss is :  1.3211913\n",
      "Iteration # 5962  loss is :  1.3211911\n",
      "Iteration # 5963  loss is :  1.3211911\n",
      "Iteration # 5964  loss is :  1.3211908\n",
      "Iteration # 5965  loss is :  1.3211907\n",
      "Iteration # 5966  loss is :  1.3211906\n",
      "Iteration # 5967  loss is :  1.3211904\n",
      "Iteration # 5968  loss is :  1.3211902\n",
      "Iteration # 5969  loss is :  1.32119\n",
      "Iteration # 5970  loss is :  1.3211899\n",
      "Iteration # 5971  loss is :  1.3211898\n",
      "Iteration # 5972  loss is :  1.3211896\n",
      "Iteration # 5973  loss is :  1.3211892\n",
      "Iteration # 5974  loss is :  1.3211892\n",
      "Iteration # 5975  loss is :  1.321189\n",
      "Iteration # 5976  loss is :  1.3211889\n",
      "Iteration # 5977  loss is :  1.3211888\n",
      "Iteration # 5978  loss is :  1.3211884\n",
      "Iteration # 5979  loss is :  1.3211884\n",
      "Iteration # 5980  loss is :  1.3211882\n",
      "Iteration # 5981  loss is :  1.3211881\n",
      "Iteration # 5982  loss is :  1.3211879\n",
      "Iteration # 5983  loss is :  1.3211876\n",
      "Iteration # 5984  loss is :  1.3211876\n",
      "Iteration # 5985  loss is :  1.3211874\n",
      "Iteration # 5986  loss is :  1.3211871\n",
      "Iteration # 5987  loss is :  1.321187\n",
      "Iteration # 5988  loss is :  1.3211869\n",
      "Iteration # 5989  loss is :  1.3211868\n",
      "Iteration # 5990  loss is :  1.3211865\n",
      "Iteration # 5991  loss is :  1.3211863\n",
      "Iteration # 5992  loss is :  1.3211863\n",
      "Iteration # 5993  loss is :  1.3211861\n",
      "Iteration # 5994  loss is :  1.3211858\n",
      "Iteration # 5995  loss is :  1.3211857\n",
      "Iteration # 5996  loss is :  1.3211855\n",
      "Iteration # 5997  loss is :  1.3211854\n",
      "Iteration # 5998  loss is :  1.3211851\n",
      "Iteration # 5999  loss is :  1.321185\n",
      "Iteration # 6000  loss is :  1.321185\n",
      "Iteration # 6001  loss is :  1.3211848\n",
      "Iteration # 6002  loss is :  1.3211848\n",
      "Iteration # 6003  loss is :  1.3211844\n",
      "Iteration # 6004  loss is :  1.3211842\n",
      "Iteration # 6005  loss is :  1.3211842\n",
      "Iteration # 6006  loss is :  1.3211838\n",
      "Iteration # 6007  loss is :  1.3211838\n",
      "Iteration # 6008  loss is :  1.3211834\n",
      "Iteration # 6009  loss is :  1.3211833\n",
      "Iteration # 6010  loss is :  1.3211833\n",
      "Iteration # 6011  loss is :  1.3211831\n",
      "Iteration # 6012  loss is :  1.3211828\n",
      "Iteration # 6013  loss is :  1.3211828\n",
      "Iteration # 6014  loss is :  1.3211827\n",
      "Iteration # 6015  loss is :  1.3211824\n",
      "Iteration # 6016  loss is :  1.3211823\n",
      "Iteration # 6017  loss is :  1.3211823\n",
      "Iteration # 6018  loss is :  1.321182\n",
      "Iteration # 6019  loss is :  1.3211817\n",
      "Iteration # 6020  loss is :  1.3211814\n",
      "Iteration # 6021  loss is :  1.3211814\n",
      "Iteration # 6022  loss is :  1.3211813\n",
      "Iteration # 6023  loss is :  1.3211812\n",
      "Iteration # 6024  loss is :  1.3211809\n",
      "Iteration # 6025  loss is :  1.3211808\n",
      "Iteration # 6026  loss is :  1.3211807\n",
      "Iteration # 6027  loss is :  1.3211805\n",
      "Iteration # 6028  loss is :  1.3211802\n",
      "Iteration # 6029  loss is :  1.3211801\n",
      "Iteration # 6030  loss is :  1.3211799\n",
      "Iteration # 6031  loss is :  1.3211799\n",
      "Iteration # 6032  loss is :  1.3211795\n",
      "Iteration # 6033  loss is :  1.3211794\n",
      "Iteration # 6034  loss is :  1.3211793\n",
      "Iteration # 6035  loss is :  1.3211792\n",
      "Iteration # 6036  loss is :  1.321179\n",
      "Iteration # 6037  loss is :  1.3211788\n",
      "Iteration # 6038  loss is :  1.3211787\n",
      "Iteration # 6039  loss is :  1.3211786\n",
      "Iteration # 6040  loss is :  1.3211782\n",
      "Iteration # 6041  loss is :  1.3211782\n",
      "Iteration # 6042  loss is :  1.321178\n",
      "Iteration # 6043  loss is :  1.3211778\n",
      "Iteration # 6044  loss is :  1.3211775\n",
      "Iteration # 6045  loss is :  1.3211775\n",
      "Iteration # 6046  loss is :  1.3211774\n",
      "Iteration # 6047  loss is :  1.3211772\n",
      "Iteration # 6048  loss is :  1.3211771\n",
      "Iteration # 6049  loss is :  1.3211769\n",
      "Iteration # 6050  loss is :  1.3211766\n",
      "Iteration # 6051  loss is :  1.3211765\n",
      "Iteration # 6052  loss is :  1.3211764\n",
      "Iteration # 6053  loss is :  1.321176\n",
      "Iteration # 6054  loss is :  1.321176\n",
      "Iteration # 6055  loss is :  1.3211758\n",
      "Iteration # 6056  loss is :  1.3211757\n",
      "Iteration # 6057  loss is :  1.3211756\n",
      "Iteration # 6058  loss is :  1.3211753\n",
      "Iteration # 6059  loss is :  1.3211752\n",
      "Iteration # 6060  loss is :  1.3211751\n",
      "Iteration # 6061  loss is :  1.3211751\n",
      "Iteration # 6062  loss is :  1.3211747\n",
      "Iteration # 6063  loss is :  1.3211745\n",
      "Iteration # 6064  loss is :  1.3211745\n",
      "Iteration # 6065  loss is :  1.3211743\n",
      "Iteration # 6066  loss is :  1.3211739\n",
      "Iteration # 6067  loss is :  1.3211738\n",
      "Iteration # 6068  loss is :  1.3211737\n",
      "Iteration # 6069  loss is :  1.3211737\n",
      "Iteration # 6070  loss is :  1.3211734\n",
      "Iteration # 6071  loss is :  1.3211733\n",
      "Iteration # 6072  loss is :  1.3211732\n",
      "Iteration # 6073  loss is :  1.321173\n",
      "Iteration # 6074  loss is :  1.3211728\n",
      "Iteration # 6075  loss is :  1.3211726\n",
      "Iteration # 6076  loss is :  1.3211724\n",
      "Iteration # 6077  loss is :  1.3211724\n",
      "Iteration # 6078  loss is :  1.3211721\n",
      "Iteration # 6079  loss is :  1.321172\n",
      "Iteration # 6080  loss is :  1.3211719\n",
      "Iteration # 6081  loss is :  1.3211718\n",
      "Iteration # 6082  loss is :  1.3211716\n",
      "Iteration # 6083  loss is :  1.3211713\n",
      "Iteration # 6084  loss is :  1.3211712\n",
      "Iteration # 6085  loss is :  1.321171\n",
      "Iteration # 6086  loss is :  1.3211709\n",
      "Iteration # 6087  loss is :  1.3211707\n",
      "Iteration # 6088  loss is :  1.3211706\n",
      "Iteration # 6089  loss is :  1.3211704\n",
      "Iteration # 6090  loss is :  1.3211702\n",
      "Iteration # 6091  loss is :  1.3211701\n",
      "Iteration # 6092  loss is :  1.3211699\n",
      "Iteration # 6093  loss is :  1.3211699\n",
      "Iteration # 6094  loss is :  1.3211697\n",
      "Iteration # 6095  loss is :  1.3211695\n",
      "Iteration # 6096  loss is :  1.3211694\n",
      "Iteration # 6097  loss is :  1.3211691\n",
      "Iteration # 6098  loss is :  1.321169\n",
      "Iteration # 6099  loss is :  1.3211689\n",
      "Iteration # 6100  loss is :  1.3211685\n",
      "Iteration # 6101  loss is :  1.3211684\n",
      "Iteration # 6102  loss is :  1.3211683\n",
      "Iteration # 6103  loss is :  1.3211682\n",
      "Iteration # 6104  loss is :  1.3211681\n",
      "Iteration # 6105  loss is :  1.3211678\n",
      "Iteration # 6106  loss is :  1.3211678\n",
      "Iteration # 6107  loss is :  1.3211676\n",
      "Iteration # 6108  loss is :  1.3211673\n",
      "Iteration # 6109  loss is :  1.3211672\n",
      "Iteration # 6110  loss is :  1.3211671\n",
      "Iteration # 6111  loss is :  1.3211669\n",
      "Iteration # 6112  loss is :  1.3211668\n",
      "Iteration # 6113  loss is :  1.3211665\n",
      "Iteration # 6114  loss is :  1.3211663\n",
      "Iteration # 6115  loss is :  1.3211662\n",
      "Iteration # 6116  loss is :  1.321166\n",
      "Iteration # 6117  loss is :  1.3211659\n",
      "Iteration # 6118  loss is :  1.3211659\n",
      "Iteration # 6119  loss is :  1.3211657\n",
      "Iteration # 6120  loss is :  1.3211654\n",
      "Iteration # 6121  loss is :  1.3211653\n",
      "Iteration # 6122  loss is :  1.3211651\n",
      "Iteration # 6123  loss is :  1.321165\n",
      "Iteration # 6124  loss is :  1.3211648\n",
      "Iteration # 6125  loss is :  1.3211647\n",
      "Iteration # 6126  loss is :  1.3211645\n",
      "Iteration # 6127  loss is :  1.3211643\n",
      "Iteration # 6128  loss is :  1.3211643\n",
      "Iteration # 6129  loss is :  1.3211641\n",
      "Iteration # 6130  loss is :  1.321164\n",
      "Iteration # 6131  loss is :  1.3211638\n",
      "Iteration # 6132  loss is :  1.3211637\n",
      "Iteration # 6133  loss is :  1.3211634\n",
      "Iteration # 6134  loss is :  1.3211633\n",
      "Iteration # 6135  loss is :  1.321163\n",
      "Iteration # 6136  loss is :  1.3211629\n",
      "Iteration # 6137  loss is :  1.3211628\n",
      "Iteration # 6138  loss is :  1.3211627\n",
      "Iteration # 6139  loss is :  1.3211625\n",
      "Iteration # 6140  loss is :  1.3211623\n",
      "Iteration # 6141  loss is :  1.3211622\n",
      "Iteration # 6142  loss is :  1.321162\n",
      "Iteration # 6143  loss is :  1.321162\n",
      "Iteration # 6144  loss is :  1.3211616\n",
      "Iteration # 6145  loss is :  1.3211616\n",
      "Iteration # 6146  loss is :  1.3211614\n",
      "Iteration # 6147  loss is :  1.3211612\n",
      "Iteration # 6148  loss is :  1.321161\n",
      "Iteration # 6149  loss is :  1.3211609\n",
      "Iteration # 6150  loss is :  1.3211607\n",
      "Iteration # 6151  loss is :  1.3211606\n",
      "Iteration # 6152  loss is :  1.3211604\n",
      "Iteration # 6153  loss is :  1.3211603\n",
      "Iteration # 6154  loss is :  1.3211602\n",
      "Iteration # 6155  loss is :  1.3211601\n",
      "Iteration # 6156  loss is :  1.3211598\n",
      "Iteration # 6157  loss is :  1.3211596\n",
      "Iteration # 6158  loss is :  1.3211595\n",
      "Iteration # 6159  loss is :  1.3211595\n",
      "Iteration # 6160  loss is :  1.3211592\n",
      "Iteration # 6161  loss is :  1.3211589\n",
      "Iteration # 6162  loss is :  1.3211589\n",
      "Iteration # 6163  loss is :  1.3211586\n",
      "Iteration # 6164  loss is :  1.3211586\n",
      "Iteration # 6165  loss is :  1.3211584\n",
      "Iteration # 6166  loss is :  1.3211582\n",
      "Iteration # 6167  loss is :  1.3211582\n",
      "Iteration # 6168  loss is :  1.321158\n",
      "Iteration # 6169  loss is :  1.3211578\n",
      "Iteration # 6170  loss is :  1.3211576\n",
      "Iteration # 6171  loss is :  1.3211575\n",
      "Iteration # 6172  loss is :  1.3211573\n",
      "Iteration # 6173  loss is :  1.3211572\n",
      "Iteration # 6174  loss is :  1.3211571\n",
      "Iteration # 6175  loss is :  1.3211567\n",
      "Iteration # 6176  loss is :  1.3211567\n",
      "Iteration # 6177  loss is :  1.3211565\n",
      "Iteration # 6178  loss is :  1.3211565\n",
      "Iteration # 6179  loss is :  1.3211563\n",
      "Iteration # 6180  loss is :  1.3211561\n",
      "Iteration # 6181  loss is :  1.3211558\n",
      "Iteration # 6182  loss is :  1.3211558\n",
      "Iteration # 6183  loss is :  1.3211557\n",
      "Iteration # 6184  loss is :  1.3211554\n",
      "Iteration # 6185  loss is :  1.3211554\n",
      "Iteration # 6186  loss is :  1.3211552\n",
      "Iteration # 6187  loss is :  1.3211551\n",
      "Iteration # 6188  loss is :  1.3211548\n",
      "Iteration # 6189  loss is :  1.3211547\n",
      "Iteration # 6190  loss is :  1.3211546\n",
      "Iteration # 6191  loss is :  1.3211545\n",
      "Iteration # 6192  loss is :  1.3211544\n",
      "Iteration # 6193  loss is :  1.3211541\n",
      "Iteration # 6194  loss is :  1.3211541\n",
      "Iteration # 6195  loss is :  1.3211539\n",
      "Iteration # 6196  loss is :  1.3211536\n",
      "Iteration # 6197  loss is :  1.3211535\n",
      "Iteration # 6198  loss is :  1.3211533\n",
      "Iteration # 6199  loss is :  1.3211532\n",
      "Iteration # 6200  loss is :  1.321153\n",
      "Iteration # 6201  loss is :  1.3211529\n",
      "Iteration # 6202  loss is :  1.3211527\n",
      "Iteration # 6203  loss is :  1.3211526\n",
      "Iteration # 6204  loss is :  1.3211523\n",
      "Iteration # 6205  loss is :  1.3211523\n",
      "Iteration # 6206  loss is :  1.3211521\n",
      "Iteration # 6207  loss is :  1.321152\n",
      "Iteration # 6208  loss is :  1.3211519\n",
      "Iteration # 6209  loss is :  1.3211517\n",
      "Iteration # 6210  loss is :  1.3211515\n",
      "Iteration # 6211  loss is :  1.3211513\n",
      "Iteration # 6212  loss is :  1.3211511\n",
      "Iteration # 6213  loss is :  1.321151\n",
      "Iteration # 6214  loss is :  1.3211509\n",
      "Iteration # 6215  loss is :  1.3211508\n",
      "Iteration # 6216  loss is :  1.3211507\n",
      "Iteration # 6217  loss is :  1.3211504\n",
      "Iteration # 6218  loss is :  1.3211504\n",
      "Iteration # 6219  loss is :  1.3211502\n",
      "Iteration # 6220  loss is :  1.32115\n",
      "Iteration # 6221  loss is :  1.3211498\n",
      "Iteration # 6222  loss is :  1.3211497\n",
      "Iteration # 6223  loss is :  1.3211496\n",
      "Iteration # 6224  loss is :  1.3211495\n",
      "Iteration # 6225  loss is :  1.3211492\n",
      "Iteration # 6226  loss is :  1.3211491\n",
      "Iteration # 6227  loss is :  1.321149\n",
      "Iteration # 6228  loss is :  1.3211489\n",
      "Iteration # 6229  loss is :  1.3211485\n",
      "Iteration # 6230  loss is :  1.3211485\n",
      "Iteration # 6231  loss is :  1.3211483\n",
      "Iteration # 6232  loss is :  1.3211483\n",
      "Iteration # 6233  loss is :  1.3211479\n",
      "Iteration # 6234  loss is :  1.3211479\n",
      "Iteration # 6235  loss is :  1.3211477\n",
      "Iteration # 6236  loss is :  1.3211477\n",
      "Iteration # 6237  loss is :  1.3211474\n",
      "Iteration # 6238  loss is :  1.3211471\n",
      "Iteration # 6239  loss is :  1.3211471\n",
      "Iteration # 6240  loss is :  1.321147\n",
      "Iteration # 6241  loss is :  1.3211467\n",
      "Iteration # 6242  loss is :  1.3211466\n",
      "Iteration # 6243  loss is :  1.3211465\n",
      "Iteration # 6244  loss is :  1.3211464\n",
      "Iteration # 6245  loss is :  1.3211461\n",
      "Iteration # 6246  loss is :  1.321146\n",
      "Iteration # 6247  loss is :  1.3211458\n",
      "Iteration # 6248  loss is :  1.3211458\n",
      "Iteration # 6249  loss is :  1.3211455\n",
      "Iteration # 6250  loss is :  1.3211454\n",
      "Iteration # 6251  loss is :  1.3211452\n",
      "Iteration # 6252  loss is :  1.321145\n",
      "Iteration # 6253  loss is :  1.3211449\n",
      "Iteration # 6254  loss is :  1.3211449\n",
      "Iteration # 6255  loss is :  1.3211448\n",
      "Iteration # 6256  loss is :  1.3211445\n",
      "Iteration # 6257  loss is :  1.3211443\n",
      "Iteration # 6258  loss is :  1.3211442\n",
      "Iteration # 6259  loss is :  1.3211441\n",
      "Iteration # 6260  loss is :  1.3211439\n",
      "Iteration # 6261  loss is :  1.3211437\n",
      "Iteration # 6262  loss is :  1.3211436\n",
      "Iteration # 6263  loss is :  1.3211436\n",
      "Iteration # 6264  loss is :  1.3211434\n",
      "Iteration # 6265  loss is :  1.3211432\n",
      "Iteration # 6266  loss is :  1.3211432\n",
      "Iteration # 6267  loss is :  1.3211429\n",
      "Iteration # 6268  loss is :  1.3211428\n",
      "Iteration # 6269  loss is :  1.3211427\n",
      "Iteration # 6270  loss is :  1.3211426\n",
      "Iteration # 6271  loss is :  1.3211423\n",
      "Iteration # 6272  loss is :  1.3211421\n",
      "Iteration # 6273  loss is :  1.3211421\n",
      "Iteration # 6274  loss is :  1.3211418\n",
      "Iteration # 6275  loss is :  1.3211416\n",
      "Iteration # 6276  loss is :  1.3211415\n",
      "Iteration # 6277  loss is :  1.3211414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 6278  loss is :  1.3211412\n",
      "Iteration # 6279  loss is :  1.3211412\n",
      "Iteration # 6280  loss is :  1.321141\n",
      "Iteration # 6281  loss is :  1.3211408\n",
      "Iteration # 6282  loss is :  1.3211408\n",
      "Iteration # 6283  loss is :  1.3211405\n",
      "Iteration # 6284  loss is :  1.3211404\n",
      "Iteration # 6285  loss is :  1.3211402\n",
      "Iteration # 6286  loss is :  1.32114\n",
      "Iteration # 6287  loss is :  1.3211399\n",
      "Iteration # 6288  loss is :  1.3211398\n",
      "Iteration # 6289  loss is :  1.3211396\n",
      "Iteration # 6290  loss is :  1.3211395\n",
      "Iteration # 6291  loss is :  1.3211393\n",
      "Iteration # 6292  loss is :  1.3211392\n",
      "Iteration # 6293  loss is :  1.321139\n",
      "Iteration # 6294  loss is :  1.3211389\n",
      "Iteration # 6295  loss is :  1.3211387\n",
      "Iteration # 6296  loss is :  1.3211386\n",
      "Iteration # 6297  loss is :  1.3211386\n",
      "Iteration # 6298  loss is :  1.3211383\n",
      "Iteration # 6299  loss is :  1.3211381\n",
      "Iteration # 6300  loss is :  1.321138\n",
      "Iteration # 6301  loss is :  1.3211379\n",
      "Iteration # 6302  loss is :  1.3211378\n",
      "Iteration # 6303  loss is :  1.3211374\n",
      "Iteration # 6304  loss is :  1.3211374\n",
      "Iteration # 6305  loss is :  1.3211373\n",
      "Iteration # 6306  loss is :  1.3211372\n",
      "Iteration # 6307  loss is :  1.321137\n",
      "Iteration # 6308  loss is :  1.3211367\n",
      "Iteration # 6309  loss is :  1.3211367\n",
      "Iteration # 6310  loss is :  1.3211366\n",
      "Iteration # 6311  loss is :  1.3211365\n",
      "Iteration # 6312  loss is :  1.3211361\n",
      "Iteration # 6313  loss is :  1.3211361\n",
      "Iteration # 6314  loss is :  1.321136\n",
      "Iteration # 6315  loss is :  1.3211359\n",
      "Iteration # 6316  loss is :  1.3211358\n",
      "Iteration # 6317  loss is :  1.3211355\n",
      "Iteration # 6318  loss is :  1.3211354\n",
      "Iteration # 6319  loss is :  1.3211353\n",
      "Iteration # 6320  loss is :  1.3211352\n",
      "Iteration # 6321  loss is :  1.3211349\n",
      "Iteration # 6322  loss is :  1.3211347\n",
      "Iteration # 6323  loss is :  1.3211346\n",
      "Iteration # 6324  loss is :  1.3211346\n",
      "Iteration # 6325  loss is :  1.3211343\n",
      "Iteration # 6326  loss is :  1.3211341\n",
      "Iteration # 6327  loss is :  1.321134\n",
      "Iteration # 6328  loss is :  1.3211339\n",
      "Iteration # 6329  loss is :  1.3211337\n",
      "Iteration # 6330  loss is :  1.3211336\n",
      "Iteration # 6331  loss is :  1.3211335\n",
      "Iteration # 6332  loss is :  1.3211334\n",
      "Iteration # 6333  loss is :  1.3211333\n",
      "Iteration # 6334  loss is :  1.321133\n",
      "Iteration # 6335  loss is :  1.321133\n",
      "Iteration # 6336  loss is :  1.3211327\n",
      "Iteration # 6337  loss is :  1.3211325\n",
      "Iteration # 6338  loss is :  1.3211324\n",
      "Iteration # 6339  loss is :  1.3211324\n",
      "Iteration # 6340  loss is :  1.3211322\n",
      "Iteration # 6341  loss is :  1.321132\n",
      "Iteration # 6342  loss is :  1.3211318\n",
      "Iteration # 6343  loss is :  1.3211317\n",
      "Iteration # 6344  loss is :  1.3211317\n",
      "Iteration # 6345  loss is :  1.3211313\n",
      "Iteration # 6346  loss is :  1.3211313\n",
      "Iteration # 6347  loss is :  1.3211311\n",
      "Iteration # 6348  loss is :  1.3211311\n",
      "Iteration # 6349  loss is :  1.3211309\n",
      "Iteration # 6350  loss is :  1.3211308\n",
      "Iteration # 6351  loss is :  1.3211305\n",
      "Iteration # 6352  loss is :  1.3211305\n",
      "Iteration # 6353  loss is :  1.3211303\n",
      "Iteration # 6354  loss is :  1.32113\n",
      "Iteration # 6355  loss is :  1.3211299\n",
      "Iteration # 6356  loss is :  1.3211298\n",
      "Iteration # 6357  loss is :  1.3211298\n",
      "Iteration # 6358  loss is :  1.3211297\n",
      "Iteration # 6359  loss is :  1.3211294\n",
      "Iteration # 6360  loss is :  1.3211292\n",
      "Iteration # 6361  loss is :  1.3211292\n",
      "Iteration # 6362  loss is :  1.3211291\n",
      "Iteration # 6363  loss is :  1.321129\n",
      "Iteration # 6364  loss is :  1.3211286\n",
      "Iteration # 6365  loss is :  1.3211285\n",
      "Iteration # 6366  loss is :  1.3211284\n",
      "Iteration # 6367  loss is :  1.3211282\n",
      "Iteration # 6368  loss is :  1.3211281\n",
      "Iteration # 6369  loss is :  1.3211279\n",
      "Iteration # 6370  loss is :  1.3211279\n",
      "Iteration # 6371  loss is :  1.3211277\n",
      "Iteration # 6372  loss is :  1.3211277\n",
      "Iteration # 6373  loss is :  1.3211275\n",
      "Iteration # 6374  loss is :  1.3211272\n",
      "Iteration # 6375  loss is :  1.321127\n",
      "Iteration # 6376  loss is :  1.321127\n",
      "Iteration # 6377  loss is :  1.3211268\n",
      "Iteration # 6378  loss is :  1.3211268\n",
      "Iteration # 6379  loss is :  1.3211265\n",
      "Iteration # 6380  loss is :  1.3211263\n",
      "Iteration # 6381  loss is :  1.3211262\n",
      "Iteration # 6382  loss is :  1.3211261\n",
      "Iteration # 6383  loss is :  1.321126\n",
      "Iteration # 6384  loss is :  1.3211259\n",
      "Iteration # 6385  loss is :  1.3211257\n",
      "Iteration # 6386  loss is :  1.3211256\n",
      "Iteration # 6387  loss is :  1.3211255\n",
      "Iteration # 6388  loss is :  1.3211251\n",
      "Iteration # 6389  loss is :  1.3211251\n",
      "Iteration # 6390  loss is :  1.3211249\n",
      "Iteration # 6391  loss is :  1.3211249\n",
      "Iteration # 6392  loss is :  1.3211247\n",
      "Iteration # 6393  loss is :  1.3211244\n",
      "Iteration # 6394  loss is :  1.3211243\n",
      "Iteration # 6395  loss is :  1.3211242\n",
      "Iteration # 6396  loss is :  1.3211241\n",
      "Iteration # 6397  loss is :  1.3211241\n",
      "Iteration # 6398  loss is :  1.3211238\n",
      "Iteration # 6399  loss is :  1.3211237\n",
      "Iteration # 6400  loss is :  1.3211236\n",
      "Iteration # 6401  loss is :  1.3211235\n",
      "Iteration # 6402  loss is :  1.3211232\n",
      "Iteration # 6403  loss is :  1.321123\n",
      "Iteration # 6404  loss is :  1.321123\n",
      "Iteration # 6405  loss is :  1.3211228\n",
      "Iteration # 6406  loss is :  1.3211228\n",
      "Iteration # 6407  loss is :  1.3211225\n",
      "Iteration # 6408  loss is :  1.3211223\n",
      "Iteration # 6409  loss is :  1.3211222\n",
      "Iteration # 6410  loss is :  1.321122\n",
      "Iteration # 6411  loss is :  1.321122\n",
      "Iteration # 6412  loss is :  1.3211219\n",
      "Iteration # 6413  loss is :  1.3211217\n",
      "Iteration # 6414  loss is :  1.3211216\n",
      "Iteration # 6415  loss is :  1.3211215\n",
      "Iteration # 6416  loss is :  1.3211215\n",
      "Iteration # 6417  loss is :  1.3211211\n",
      "Iteration # 6418  loss is :  1.3211209\n",
      "Iteration # 6419  loss is :  1.3211207\n",
      "Iteration # 6420  loss is :  1.3211206\n",
      "Iteration # 6421  loss is :  1.3211206\n",
      "Iteration # 6422  loss is :  1.3211203\n",
      "Iteration # 6423  loss is :  1.3211203\n",
      "Iteration # 6424  loss is :  1.3211201\n",
      "Iteration # 6425  loss is :  1.3211201\n",
      "Iteration # 6426  loss is :  1.3211199\n",
      "Iteration # 6427  loss is :  1.3211197\n",
      "Iteration # 6428  loss is :  1.3211195\n",
      "Iteration # 6429  loss is :  1.3211194\n",
      "Iteration # 6430  loss is :  1.3211193\n",
      "Iteration # 6431  loss is :  1.3211192\n",
      "Iteration # 6432  loss is :  1.321119\n",
      "Iteration # 6433  loss is :  1.3211188\n",
      "Iteration # 6434  loss is :  1.3211187\n",
      "Iteration # 6435  loss is :  1.3211187\n",
      "Iteration # 6436  loss is :  1.3211185\n",
      "Iteration # 6437  loss is :  1.3211184\n",
      "Iteration # 6438  loss is :  1.3211182\n",
      "Iteration # 6439  loss is :  1.3211181\n",
      "Iteration # 6440  loss is :  1.321118\n",
      "Iteration # 6441  loss is :  1.3211179\n",
      "Iteration # 6442  loss is :  1.3211176\n",
      "Iteration # 6443  loss is :  1.3211175\n",
      "Iteration # 6444  loss is :  1.3211174\n",
      "Iteration # 6445  loss is :  1.3211172\n",
      "Iteration # 6446  loss is :  1.3211169\n",
      "Iteration # 6447  loss is :  1.3211169\n",
      "Iteration # 6448  loss is :  1.3211167\n",
      "Iteration # 6449  loss is :  1.3211167\n",
      "Iteration # 6450  loss is :  1.3211166\n",
      "Iteration # 6451  loss is :  1.3211163\n",
      "Iteration # 6452  loss is :  1.3211162\n",
      "Iteration # 6453  loss is :  1.3211161\n",
      "Iteration # 6454  loss is :  1.321116\n",
      "Iteration # 6455  loss is :  1.3211159\n",
      "Iteration # 6456  loss is :  1.3211156\n",
      "Iteration # 6457  loss is :  1.3211155\n",
      "Iteration # 6458  loss is :  1.3211155\n",
      "Iteration # 6459  loss is :  1.3211153\n",
      "Iteration # 6460  loss is :  1.321115\n",
      "Iteration # 6461  loss is :  1.3211149\n",
      "Iteration # 6462  loss is :  1.3211148\n",
      "Iteration # 6463  loss is :  1.3211147\n",
      "Iteration # 6464  loss is :  1.3211145\n",
      "Iteration # 6465  loss is :  1.3211145\n",
      "Iteration # 6466  loss is :  1.3211142\n",
      "Iteration # 6467  loss is :  1.3211142\n",
      "Iteration # 6468  loss is :  1.3211141\n",
      "Iteration # 6469  loss is :  1.321114\n",
      "Iteration # 6470  loss is :  1.3211138\n",
      "Iteration # 6471  loss is :  1.3211136\n",
      "Iteration # 6472  loss is :  1.3211135\n",
      "Iteration # 6473  loss is :  1.3211132\n",
      "Iteration # 6474  loss is :  1.3211132\n",
      "Iteration # 6475  loss is :  1.3211131\n",
      "Iteration # 6476  loss is :  1.3211129\n",
      "Iteration # 6477  loss is :  1.3211126\n",
      "Iteration # 6478  loss is :  1.3211126\n",
      "Iteration # 6479  loss is :  1.3211126\n",
      "Iteration # 6480  loss is :  1.3211124\n",
      "Iteration # 6481  loss is :  1.3211123\n",
      "Iteration # 6482  loss is :  1.321112\n",
      "Iteration # 6483  loss is :  1.3211119\n",
      "Iteration # 6484  loss is :  1.3211118\n",
      "Iteration # 6485  loss is :  1.3211117\n",
      "Iteration # 6486  loss is :  1.3211114\n",
      "Iteration # 6487  loss is :  1.3211113\n",
      "Iteration # 6488  loss is :  1.3211112\n",
      "Iteration # 6489  loss is :  1.3211111\n",
      "Iteration # 6490  loss is :  1.321111\n",
      "Iteration # 6491  loss is :  1.3211107\n",
      "Iteration # 6492  loss is :  1.3211107\n",
      "Iteration # 6493  loss is :  1.3211107\n",
      "Iteration # 6494  loss is :  1.3211105\n",
      "Iteration # 6495  loss is :  1.3211105\n",
      "Iteration # 6496  loss is :  1.3211101\n",
      "Iteration # 6497  loss is :  1.3211099\n",
      "Iteration # 6498  loss is :  1.3211099\n",
      "Iteration # 6499  loss is :  1.3211098\n",
      "Iteration # 6500  loss is :  1.3211097\n",
      "Iteration # 6501  loss is :  1.3211094\n",
      "Iteration # 6502  loss is :  1.3211093\n",
      "Iteration # 6503  loss is :  1.3211092\n",
      "Iteration # 6504  loss is :  1.321109\n",
      "Iteration # 6505  loss is :  1.3211088\n",
      "Iteration # 6506  loss is :  1.3211088\n",
      "Iteration # 6507  loss is :  1.3211086\n",
      "Iteration # 6508  loss is :  1.3211086\n",
      "Iteration # 6509  loss is :  1.3211085\n",
      "Iteration # 6510  loss is :  1.3211083\n",
      "Iteration # 6511  loss is :  1.321108\n",
      "Iteration # 6512  loss is :  1.321108\n",
      "Iteration # 6513  loss is :  1.3211079\n",
      "Iteration # 6514  loss is :  1.3211077\n",
      "Iteration # 6515  loss is :  1.3211076\n",
      "Iteration # 6516  loss is :  1.3211073\n",
      "Iteration # 6517  loss is :  1.3211071\n",
      "Iteration # 6518  loss is :  1.3211071\n",
      "Iteration # 6519  loss is :  1.3211069\n",
      "Iteration # 6520  loss is :  1.3211068\n",
      "Iteration # 6521  loss is :  1.3211067\n",
      "Iteration # 6522  loss is :  1.3211066\n",
      "Iteration # 6523  loss is :  1.3211064\n",
      "Iteration # 6524  loss is :  1.3211064\n",
      "Iteration # 6525  loss is :  1.3211062\n",
      "Iteration # 6526  loss is :  1.3211062\n",
      "Iteration # 6527  loss is :  1.3211058\n",
      "Iteration # 6528  loss is :  1.3211058\n",
      "Iteration # 6529  loss is :  1.3211056\n",
      "Iteration # 6530  loss is :  1.3211055\n",
      "Iteration # 6531  loss is :  1.3211052\n",
      "Iteration # 6532  loss is :  1.3211052\n",
      "Iteration # 6533  loss is :  1.3211051\n",
      "Iteration # 6534  loss is :  1.321105\n",
      "Iteration # 6535  loss is :  1.3211049\n",
      "Iteration # 6536  loss is :  1.3211046\n",
      "Iteration # 6537  loss is :  1.3211045\n",
      "Iteration # 6538  loss is :  1.3211044\n",
      "Iteration # 6539  loss is :  1.3211043\n",
      "Iteration # 6540  loss is :  1.3211042\n",
      "Iteration # 6541  loss is :  1.321104\n",
      "Iteration # 6542  loss is :  1.3211038\n",
      "Iteration # 6543  loss is :  1.3211037\n",
      "Iteration # 6544  loss is :  1.3211036\n",
      "Iteration # 6545  loss is :  1.3211035\n",
      "Iteration # 6546  loss is :  1.3211035\n",
      "Iteration # 6547  loss is :  1.3211031\n",
      "Iteration # 6548  loss is :  1.3211031\n",
      "Iteration # 6549  loss is :  1.321103\n",
      "Iteration # 6550  loss is :  1.3211027\n",
      "Iteration # 6551  loss is :  1.3211027\n",
      "Iteration # 6552  loss is :  1.3211025\n",
      "Iteration # 6553  loss is :  1.3211025\n",
      "Iteration # 6554  loss is :  1.3211024\n",
      "Iteration # 6555  loss is :  1.3211021\n",
      "Iteration # 6556  loss is :  1.3211021\n",
      "Iteration # 6557  loss is :  1.3211018\n",
      "Iteration # 6558  loss is :  1.3211018\n",
      "Iteration # 6559  loss is :  1.3211015\n",
      "Iteration # 6560  loss is :  1.3211014\n",
      "Iteration # 6561  loss is :  1.3211013\n",
      "Iteration # 6562  loss is :  1.3211012\n",
      "Iteration # 6563  loss is :  1.3211011\n",
      "Iteration # 6564  loss is :  1.321101\n",
      "Iteration # 6565  loss is :  1.3211008\n",
      "Iteration # 6566  loss is :  1.3211008\n",
      "Iteration # 6567  loss is :  1.3211006\n",
      "Iteration # 6568  loss is :  1.3211004\n",
      "Iteration # 6569  loss is :  1.3211002\n",
      "Iteration # 6570  loss is :  1.3211002\n",
      "Iteration # 6571  loss is :  1.3211\n",
      "Iteration # 6572  loss is :  1.3210999\n",
      "Iteration # 6573  loss is :  1.3210996\n",
      "Iteration # 6574  loss is :  1.3210995\n",
      "Iteration # 6575  loss is :  1.3210994\n",
      "Iteration # 6576  loss is :  1.3210994\n",
      "Iteration # 6577  loss is :  1.3210993\n",
      "Iteration # 6578  loss is :  1.321099\n",
      "Iteration # 6579  loss is :  1.3210989\n",
      "Iteration # 6580  loss is :  1.3210989\n",
      "Iteration # 6581  loss is :  1.3210987\n",
      "Iteration # 6582  loss is :  1.3210987\n",
      "Iteration # 6583  loss is :  1.3210983\n",
      "Iteration # 6584  loss is :  1.3210982\n",
      "Iteration # 6585  loss is :  1.3210981\n",
      "Iteration # 6586  loss is :  1.3210981\n",
      "Iteration # 6587  loss is :  1.3210979\n",
      "Iteration # 6588  loss is :  1.3210977\n",
      "Iteration # 6589  loss is :  1.3210975\n",
      "Iteration # 6590  loss is :  1.3210975\n",
      "Iteration # 6591  loss is :  1.3210974\n",
      "Iteration # 6592  loss is :  1.3210973\n",
      "Iteration # 6593  loss is :  1.321097\n",
      "Iteration # 6594  loss is :  1.321097\n",
      "Iteration # 6595  loss is :  1.3210968\n",
      "Iteration # 6596  loss is :  1.3210967\n",
      "Iteration # 6597  loss is :  1.3210965\n",
      "Iteration # 6598  loss is :  1.3210965\n",
      "Iteration # 6599  loss is :  1.3210962\n",
      "Iteration # 6600  loss is :  1.3210962\n",
      "Iteration # 6601  loss is :  1.3210961\n",
      "Iteration # 6602  loss is :  1.3210958\n",
      "Iteration # 6603  loss is :  1.3210958\n",
      "Iteration # 6604  loss is :  1.3210956\n",
      "Iteration # 6605  loss is :  1.3210955\n",
      "Iteration # 6606  loss is :  1.3210955\n",
      "Iteration # 6607  loss is :  1.3210952\n",
      "Iteration # 6608  loss is :  1.3210952\n",
      "Iteration # 6609  loss is :  1.321095\n",
      "Iteration # 6610  loss is :  1.3210949\n",
      "Iteration # 6611  loss is :  1.3210946\n",
      "Iteration # 6612  loss is :  1.3210946\n",
      "Iteration # 6613  loss is :  1.3210945\n",
      "Iteration # 6614  loss is :  1.3210943\n",
      "Iteration # 6615  loss is :  1.321094\n",
      "Iteration # 6616  loss is :  1.321094\n",
      "Iteration # 6617  loss is :  1.3210939\n",
      "Iteration # 6618  loss is :  1.3210938\n",
      "Iteration # 6619  loss is :  1.3210936\n",
      "Iteration # 6620  loss is :  1.3210936\n",
      "Iteration # 6621  loss is :  1.3210933\n",
      "Iteration # 6622  loss is :  1.3210933\n",
      "Iteration # 6623  loss is :  1.3210933\n",
      "Iteration # 6624  loss is :  1.321093\n",
      "Iteration # 6625  loss is :  1.3210927\n",
      "Iteration # 6626  loss is :  1.3210927\n",
      "Iteration # 6627  loss is :  1.3210927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 6628  loss is :  1.3210925\n",
      "Iteration # 6629  loss is :  1.3210922\n",
      "Iteration # 6630  loss is :  1.3210922\n",
      "Iteration # 6631  loss is :  1.321092\n",
      "Iteration # 6632  loss is :  1.3210919\n",
      "Iteration # 6633  loss is :  1.3210918\n",
      "Iteration # 6634  loss is :  1.3210917\n",
      "Iteration # 6635  loss is :  1.3210917\n",
      "Iteration # 6636  loss is :  1.3210914\n",
      "Iteration # 6637  loss is :  1.3210914\n",
      "Iteration # 6638  loss is :  1.3210912\n",
      "Iteration # 6639  loss is :  1.321091\n",
      "Iteration # 6640  loss is :  1.3210908\n",
      "Iteration # 6641  loss is :  1.3210908\n",
      "Iteration # 6642  loss is :  1.3210906\n",
      "Iteration # 6643  loss is :  1.3210906\n",
      "Iteration # 6644  loss is :  1.3210903\n",
      "Iteration # 6645  loss is :  1.3210902\n",
      "Iteration # 6646  loss is :  1.3210901\n",
      "Iteration # 6647  loss is :  1.32109\n",
      "Iteration # 6648  loss is :  1.3210897\n",
      "Iteration # 6649  loss is :  1.3210897\n",
      "Iteration # 6650  loss is :  1.3210897\n",
      "Iteration # 6651  loss is :  1.3210895\n",
      "Iteration # 6652  loss is :  1.3210894\n",
      "Iteration # 6653  loss is :  1.3210893\n",
      "Iteration # 6654  loss is :  1.321089\n",
      "Iteration # 6655  loss is :  1.321089\n",
      "Iteration # 6656  loss is :  1.3210888\n",
      "Iteration # 6657  loss is :  1.3210887\n",
      "Iteration # 6658  loss is :  1.3210886\n",
      "Iteration # 6659  loss is :  1.3210884\n",
      "Iteration # 6660  loss is :  1.3210883\n",
      "Iteration # 6661  loss is :  1.3210882\n",
      "Iteration # 6662  loss is :  1.3210881\n",
      "Iteration # 6663  loss is :  1.3210878\n",
      "Iteration # 6664  loss is :  1.3210878\n",
      "Iteration # 6665  loss is :  1.3210877\n",
      "Iteration # 6666  loss is :  1.3210876\n",
      "Iteration # 6667  loss is :  1.3210875\n",
      "Iteration # 6668  loss is :  1.3210874\n",
      "Iteration # 6669  loss is :  1.3210871\n",
      "Iteration # 6670  loss is :  1.3210871\n",
      "Iteration # 6671  loss is :  1.321087\n",
      "Iteration # 6672  loss is :  1.3210868\n",
      "Iteration # 6673  loss is :  1.3210866\n",
      "Iteration # 6674  loss is :  1.3210865\n",
      "Iteration # 6675  loss is :  1.3210863\n",
      "Iteration # 6676  loss is :  1.3210863\n",
      "Iteration # 6677  loss is :  1.3210862\n",
      "Iteration # 6678  loss is :  1.3210859\n",
      "Iteration # 6679  loss is :  1.3210859\n",
      "Iteration # 6680  loss is :  1.3210858\n",
      "Iteration # 6681  loss is :  1.3210856\n",
      "Iteration # 6682  loss is :  1.3210856\n",
      "Iteration # 6683  loss is :  1.3210853\n",
      "Iteration # 6684  loss is :  1.3210852\n",
      "Iteration # 6685  loss is :  1.3210852\n",
      "Iteration # 6686  loss is :  1.321085\n",
      "Iteration # 6687  loss is :  1.321085\n",
      "Iteration # 6688  loss is :  1.3210846\n",
      "Iteration # 6689  loss is :  1.3210846\n",
      "Iteration # 6690  loss is :  1.3210844\n",
      "Iteration # 6691  loss is :  1.3210843\n",
      "Iteration # 6692  loss is :  1.3210841\n",
      "Iteration # 6693  loss is :  1.321084\n",
      "Iteration # 6694  loss is :  1.3210839\n",
      "Iteration # 6695  loss is :  1.3210839\n",
      "Iteration # 6696  loss is :  1.3210837\n",
      "Iteration # 6697  loss is :  1.3210837\n",
      "Iteration # 6698  loss is :  1.3210835\n",
      "Iteration # 6699  loss is :  1.3210833\n",
      "Iteration # 6700  loss is :  1.3210831\n",
      "Iteration # 6701  loss is :  1.3210831\n",
      "Iteration # 6702  loss is :  1.3210828\n",
      "Iteration # 6703  loss is :  1.3210828\n",
      "Iteration # 6704  loss is :  1.3210826\n",
      "Iteration # 6705  loss is :  1.3210825\n",
      "Iteration # 6706  loss is :  1.3210824\n",
      "Iteration # 6707  loss is :  1.3210822\n",
      "Iteration # 6708  loss is :  1.3210821\n",
      "Iteration # 6709  loss is :  1.321082\n",
      "Iteration # 6710  loss is :  1.3210819\n",
      "Iteration # 6711  loss is :  1.3210819\n",
      "Iteration # 6712  loss is :  1.3210818\n",
      "Iteration # 6713  loss is :  1.3210815\n",
      "Iteration # 6714  loss is :  1.3210815\n",
      "Iteration # 6715  loss is :  1.3210813\n",
      "Iteration # 6716  loss is :  1.3210812\n",
      "Iteration # 6717  loss is :  1.321081\n",
      "Iteration # 6718  loss is :  1.3210809\n",
      "Iteration # 6719  loss is :  1.3210807\n",
      "Iteration # 6720  loss is :  1.3210806\n",
      "Iteration # 6721  loss is :  1.3210804\n",
      "Iteration # 6722  loss is :  1.3210803\n",
      "Iteration # 6723  loss is :  1.3210802\n",
      "Iteration # 6724  loss is :  1.3210802\n",
      "Iteration # 6725  loss is :  1.3210801\n",
      "Iteration # 6726  loss is :  1.3210799\n",
      "Iteration # 6727  loss is :  1.3210799\n",
      "Iteration # 6728  loss is :  1.3210796\n",
      "Iteration # 6729  loss is :  1.3210796\n",
      "Iteration # 6730  loss is :  1.3210794\n",
      "Iteration # 6731  loss is :  1.3210794\n",
      "Iteration # 6732  loss is :  1.321079\n",
      "Iteration # 6733  loss is :  1.321079\n",
      "Iteration # 6734  loss is :  1.3210788\n",
      "Iteration # 6735  loss is :  1.3210788\n",
      "Iteration # 6736  loss is :  1.3210785\n",
      "Iteration # 6737  loss is :  1.3210784\n",
      "Iteration # 6738  loss is :  1.3210784\n",
      "Iteration # 6739  loss is :  1.3210783\n",
      "Iteration # 6740  loss is :  1.3210782\n",
      "Iteration # 6741  loss is :  1.3210781\n",
      "Iteration # 6742  loss is :  1.3210781\n",
      "Iteration # 6743  loss is :  1.3210777\n",
      "Iteration # 6744  loss is :  1.3210776\n",
      "Iteration # 6745  loss is :  1.3210775\n",
      "Iteration # 6746  loss is :  1.3210775\n",
      "Iteration # 6747  loss is :  1.3210773\n",
      "Iteration # 6748  loss is :  1.3210771\n",
      "Iteration # 6749  loss is :  1.3210771\n",
      "Iteration # 6750  loss is :  1.3210769\n",
      "Iteration # 6751  loss is :  1.3210768\n",
      "Iteration # 6752  loss is :  1.3210766\n",
      "Iteration # 6753  loss is :  1.3210765\n",
      "Iteration # 6754  loss is :  1.3210764\n",
      "Iteration # 6755  loss is :  1.3210763\n",
      "Iteration # 6756  loss is :  1.3210762\n",
      "Iteration # 6757  loss is :  1.3210762\n",
      "Iteration # 6758  loss is :  1.3210759\n",
      "Iteration # 6759  loss is :  1.3210758\n",
      "Iteration # 6760  loss is :  1.3210756\n",
      "Iteration # 6761  loss is :  1.3210756\n",
      "Iteration # 6762  loss is :  1.3210753\n",
      "Iteration # 6763  loss is :  1.3210753\n",
      "Iteration # 6764  loss is :  1.3210751\n",
      "Iteration # 6765  loss is :  1.321075\n",
      "Iteration # 6766  loss is :  1.3210748\n",
      "Iteration # 6767  loss is :  1.3210747\n",
      "Iteration # 6768  loss is :  1.3210745\n",
      "Iteration # 6769  loss is :  1.3210745\n",
      "Iteration # 6770  loss is :  1.3210744\n",
      "Iteration # 6771  loss is :  1.3210742\n",
      "Iteration # 6772  loss is :  1.3210742\n",
      "Iteration # 6773  loss is :  1.321074\n",
      "Iteration # 6774  loss is :  1.321074\n",
      "Iteration # 6775  loss is :  1.3210738\n",
      "Iteration # 6776  loss is :  1.3210735\n",
      "Iteration # 6777  loss is :  1.3210734\n",
      "Iteration # 6778  loss is :  1.3210734\n",
      "Iteration # 6779  loss is :  1.3210732\n",
      "Iteration # 6780  loss is :  1.3210732\n",
      "Iteration # 6781  loss is :  1.3210729\n",
      "Iteration # 6782  loss is :  1.3210728\n",
      "Iteration # 6783  loss is :  1.3210728\n",
      "Iteration # 6784  loss is :  1.3210726\n",
      "Iteration # 6785  loss is :  1.3210726\n",
      "Iteration # 6786  loss is :  1.3210725\n",
      "Iteration # 6787  loss is :  1.3210722\n",
      "Iteration # 6788  loss is :  1.3210721\n",
      "Iteration # 6789  loss is :  1.3210721\n",
      "Iteration # 6790  loss is :  1.321072\n",
      "Iteration # 6791  loss is :  1.3210719\n",
      "Iteration # 6792  loss is :  1.3210715\n",
      "Iteration # 6793  loss is :  1.3210715\n",
      "Iteration # 6794  loss is :  1.3210714\n",
      "Iteration # 6795  loss is :  1.3210713\n",
      "Iteration # 6796  loss is :  1.3210711\n",
      "Iteration # 6797  loss is :  1.321071\n",
      "Iteration # 6798  loss is :  1.3210709\n",
      "Iteration # 6799  loss is :  1.3210707\n",
      "Iteration # 6800  loss is :  1.3210707\n",
      "Iteration # 6801  loss is :  1.3210707\n",
      "Iteration # 6802  loss is :  1.3210706\n",
      "Iteration # 6803  loss is :  1.3210703\n",
      "Iteration # 6804  loss is :  1.3210702\n",
      "Iteration # 6805  loss is :  1.32107\n",
      "Iteration # 6806  loss is :  1.32107\n",
      "Iteration # 6807  loss is :  1.3210698\n",
      "Iteration # 6808  loss is :  1.3210698\n",
      "Iteration # 6809  loss is :  1.3210695\n",
      "Iteration # 6810  loss is :  1.3210694\n",
      "Iteration # 6811  loss is :  1.3210692\n",
      "Iteration # 6812  loss is :  1.3210691\n",
      "Iteration # 6813  loss is :  1.3210691\n",
      "Iteration # 6814  loss is :  1.3210688\n",
      "Iteration # 6815  loss is :  1.3210688\n",
      "Iteration # 6816  loss is :  1.3210688\n",
      "Iteration # 6817  loss is :  1.3210686\n",
      "Iteration # 6818  loss is :  1.3210685\n",
      "Iteration # 6819  loss is :  1.3210684\n",
      "Iteration # 6820  loss is :  1.3210682\n",
      "Iteration # 6821  loss is :  1.321068\n",
      "Iteration # 6822  loss is :  1.321068\n",
      "Iteration # 6823  loss is :  1.3210678\n",
      "Iteration # 6824  loss is :  1.3210678\n",
      "Iteration # 6825  loss is :  1.3210676\n",
      "Iteration # 6826  loss is :  1.3210675\n",
      "Iteration # 6827  loss is :  1.3210673\n",
      "Iteration # 6828  loss is :  1.3210672\n",
      "Iteration # 6829  loss is :  1.3210671\n",
      "Iteration # 6830  loss is :  1.321067\n",
      "Iteration # 6831  loss is :  1.3210669\n",
      "Iteration # 6832  loss is :  1.3210667\n",
      "Iteration # 6833  loss is :  1.3210667\n",
      "Iteration # 6834  loss is :  1.3210665\n",
      "Iteration # 6835  loss is :  1.3210665\n",
      "Iteration # 6836  loss is :  1.3210663\n",
      "Iteration # 6837  loss is :  1.3210661\n",
      "Iteration # 6838  loss is :  1.321066\n",
      "Iteration # 6839  loss is :  1.3210659\n",
      "Iteration # 6840  loss is :  1.3210659\n",
      "Iteration # 6841  loss is :  1.3210657\n",
      "Iteration # 6842  loss is :  1.3210655\n",
      "Iteration # 6843  loss is :  1.3210653\n",
      "Iteration # 6844  loss is :  1.3210653\n",
      "Iteration # 6845  loss is :  1.3210651\n",
      "Iteration # 6846  loss is :  1.321065\n",
      "Iteration # 6847  loss is :  1.321065\n",
      "Iteration # 6848  loss is :  1.3210648\n",
      "Iteration # 6849  loss is :  1.3210647\n",
      "Iteration # 6850  loss is :  1.3210646\n",
      "Iteration # 6851  loss is :  1.3210645\n",
      "Iteration # 6852  loss is :  1.3210644\n",
      "Iteration # 6853  loss is :  1.3210642\n",
      "Iteration # 6854  loss is :  1.321064\n",
      "Iteration # 6855  loss is :  1.321064\n",
      "Iteration # 6856  loss is :  1.3210638\n",
      "Iteration # 6857  loss is :  1.3210636\n",
      "Iteration # 6858  loss is :  1.3210635\n",
      "Iteration # 6859  loss is :  1.3210635\n",
      "Iteration # 6860  loss is :  1.3210633\n",
      "Iteration # 6861  loss is :  1.3210632\n",
      "Iteration # 6862  loss is :  1.321063\n",
      "Iteration # 6863  loss is :  1.321063\n",
      "Iteration # 6864  loss is :  1.321063\n",
      "Iteration # 6865  loss is :  1.3210627\n",
      "Iteration # 6866  loss is :  1.3210627\n",
      "Iteration # 6867  loss is :  1.3210624\n",
      "Iteration # 6868  loss is :  1.3210624\n",
      "Iteration # 6869  loss is :  1.3210623\n",
      "Iteration # 6870  loss is :  1.3210622\n",
      "Iteration # 6871  loss is :  1.321062\n",
      "Iteration # 6872  loss is :  1.3210618\n",
      "Iteration # 6873  loss is :  1.3210618\n",
      "Iteration # 6874  loss is :  1.3210617\n",
      "Iteration # 6875  loss is :  1.3210616\n",
      "Iteration # 6876  loss is :  1.3210614\n",
      "Iteration # 6877  loss is :  1.3210613\n",
      "Iteration # 6878  loss is :  1.3210611\n",
      "Iteration # 6879  loss is :  1.3210611\n",
      "Iteration # 6880  loss is :  1.321061\n",
      "Iteration # 6881  loss is :  1.3210609\n",
      "Iteration # 6882  loss is :  1.3210608\n",
      "Iteration # 6883  loss is :  1.3210605\n",
      "Iteration # 6884  loss is :  1.3210604\n",
      "Iteration # 6885  loss is :  1.3210603\n",
      "Iteration # 6886  loss is :  1.3210602\n",
      "Iteration # 6887  loss is :  1.3210602\n",
      "Iteration # 6888  loss is :  1.32106\n",
      "Iteration # 6889  loss is :  1.3210598\n",
      "Iteration # 6890  loss is :  1.3210597\n",
      "Iteration # 6891  loss is :  1.3210597\n",
      "Iteration # 6892  loss is :  1.3210595\n",
      "Iteration # 6893  loss is :  1.3210593\n",
      "Iteration # 6894  loss is :  1.3210592\n",
      "Iteration # 6895  loss is :  1.3210592\n",
      "Iteration # 6896  loss is :  1.3210591\n",
      "Iteration # 6897  loss is :  1.321059\n",
      "Iteration # 6898  loss is :  1.3210589\n",
      "Iteration # 6899  loss is :  1.3210588\n",
      "Iteration # 6900  loss is :  1.3210585\n",
      "Iteration # 6901  loss is :  1.3210584\n",
      "Iteration # 6902  loss is :  1.3210584\n",
      "Iteration # 6903  loss is :  1.3210583\n",
      "Iteration # 6904  loss is :  1.3210582\n",
      "Iteration # 6905  loss is :  1.3210579\n",
      "Iteration # 6906  loss is :  1.3210578\n",
      "Iteration # 6907  loss is :  1.3210577\n",
      "Iteration # 6908  loss is :  1.3210577\n",
      "Iteration # 6909  loss is :  1.3210576\n",
      "Iteration # 6910  loss is :  1.3210573\n",
      "Iteration # 6911  loss is :  1.3210572\n",
      "Iteration # 6912  loss is :  1.3210571\n",
      "Iteration # 6913  loss is :  1.3210571\n",
      "Iteration # 6914  loss is :  1.3210571\n",
      "Iteration # 6915  loss is :  1.3210568\n",
      "Iteration # 6916  loss is :  1.3210567\n",
      "Iteration # 6917  loss is :  1.3210565\n",
      "Iteration # 6918  loss is :  1.3210565\n",
      "Iteration # 6919  loss is :  1.3210564\n",
      "Iteration # 6920  loss is :  1.3210562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 6921  loss is :  1.321056\n",
      "Iteration # 6922  loss is :  1.3210559\n",
      "Iteration # 6923  loss is :  1.3210558\n",
      "Iteration # 6924  loss is :  1.3210557\n",
      "Iteration # 6925  loss is :  1.3210555\n",
      "Iteration # 6926  loss is :  1.3210554\n",
      "Iteration # 6927  loss is :  1.3210554\n",
      "Iteration # 6928  loss is :  1.3210553\n",
      "Iteration # 6929  loss is :  1.3210552\n",
      "Iteration # 6930  loss is :  1.321055\n",
      "Iteration # 6931  loss is :  1.3210549\n",
      "Iteration # 6932  loss is :  1.3210549\n",
      "Iteration # 6933  loss is :  1.3210547\n",
      "Iteration # 6934  loss is :  1.3210546\n",
      "Iteration # 6935  loss is :  1.3210543\n",
      "Iteration # 6936  loss is :  1.3210542\n",
      "Iteration # 6937  loss is :  1.3210541\n",
      "Iteration # 6938  loss is :  1.3210541\n",
      "Iteration # 6939  loss is :  1.3210541\n",
      "Iteration # 6940  loss is :  1.3210537\n",
      "Iteration # 6941  loss is :  1.3210536\n",
      "Iteration # 6942  loss is :  1.3210535\n",
      "Iteration # 6943  loss is :  1.3210535\n",
      "Iteration # 6944  loss is :  1.3210534\n",
      "Iteration # 6945  loss is :  1.3210534\n",
      "Iteration # 6946  loss is :  1.3210531\n",
      "Iteration # 6947  loss is :  1.321053\n",
      "Iteration # 6948  loss is :  1.3210529\n",
      "Iteration # 6949  loss is :  1.3210528\n",
      "Iteration # 6950  loss is :  1.3210527\n",
      "Iteration # 6951  loss is :  1.3210526\n",
      "Iteration # 6952  loss is :  1.3210524\n",
      "Iteration # 6953  loss is :  1.3210523\n",
      "Iteration # 6954  loss is :  1.3210522\n",
      "Iteration # 6955  loss is :  1.321052\n",
      "Iteration # 6956  loss is :  1.321052\n",
      "Iteration # 6957  loss is :  1.3210518\n",
      "Iteration # 6958  loss is :  1.3210516\n",
      "Iteration # 6959  loss is :  1.3210516\n",
      "Iteration # 6960  loss is :  1.3210515\n",
      "Iteration # 6961  loss is :  1.3210514\n",
      "Iteration # 6962  loss is :  1.3210514\n",
      "Iteration # 6963  loss is :  1.3210512\n",
      "Iteration # 6964  loss is :  1.3210511\n",
      "Iteration # 6965  loss is :  1.3210509\n",
      "Iteration # 6966  loss is :  1.3210509\n",
      "Iteration # 6967  loss is :  1.3210508\n",
      "Iteration # 6968  loss is :  1.3210506\n",
      "Iteration # 6969  loss is :  1.3210504\n",
      "Iteration # 6970  loss is :  1.3210503\n",
      "Iteration # 6971  loss is :  1.32105\n",
      "Iteration # 6972  loss is :  1.32105\n",
      "Iteration # 6973  loss is :  1.32105\n",
      "Iteration # 6974  loss is :  1.3210499\n",
      "Iteration # 6975  loss is :  1.3210498\n",
      "Iteration # 6976  loss is :  1.3210496\n",
      "Iteration # 6977  loss is :  1.3210496\n",
      "Iteration # 6978  loss is :  1.3210493\n",
      "Iteration # 6979  loss is :  1.3210493\n",
      "Iteration # 6980  loss is :  1.3210492\n",
      "Iteration # 6981  loss is :  1.321049\n",
      "Iteration # 6982  loss is :  1.321049\n",
      "Iteration # 6983  loss is :  1.3210487\n",
      "Iteration # 6984  loss is :  1.3210487\n",
      "Iteration # 6985  loss is :  1.3210486\n",
      "Iteration # 6986  loss is :  1.3210485\n",
      "Iteration # 6987  loss is :  1.3210484\n",
      "Iteration # 6988  loss is :  1.3210481\n",
      "Iteration # 6989  loss is :  1.321048\n",
      "Iteration # 6990  loss is :  1.321048\n",
      "Iteration # 6991  loss is :  1.3210479\n",
      "Iteration # 6992  loss is :  1.3210478\n",
      "Iteration # 6993  loss is :  1.3210477\n",
      "Iteration # 6994  loss is :  1.3210475\n",
      "Iteration # 6995  loss is :  1.3210474\n",
      "Iteration # 6996  loss is :  1.3210474\n",
      "Iteration # 6997  loss is :  1.3210472\n",
      "Iteration # 6998  loss is :  1.3210472\n",
      "Iteration # 6999  loss is :  1.321047\n",
      "Iteration # 7000  loss is :  1.321047\n",
      "Iteration # 7001  loss is :  1.3210467\n",
      "Iteration # 7002  loss is :  1.3210466\n",
      "Iteration # 7003  loss is :  1.3210464\n",
      "Iteration # 7004  loss is :  1.3210464\n",
      "Iteration # 7005  loss is :  1.3210462\n",
      "Iteration # 7006  loss is :  1.3210461\n",
      "Iteration # 7007  loss is :  1.321046\n",
      "Iteration # 7008  loss is :  1.321046\n",
      "Iteration # 7009  loss is :  1.3210459\n",
      "Iteration # 7010  loss is :  1.3210459\n",
      "Iteration # 7011  loss is :  1.3210456\n",
      "Iteration # 7012  loss is :  1.3210455\n",
      "Iteration # 7013  loss is :  1.3210454\n",
      "Iteration # 7014  loss is :  1.3210453\n",
      "Iteration # 7015  loss is :  1.3210452\n",
      "Iteration # 7016  loss is :  1.3210452\n",
      "Iteration # 7017  loss is :  1.3210449\n",
      "Iteration # 7018  loss is :  1.3210447\n",
      "Iteration # 7019  loss is :  1.3210447\n",
      "Iteration # 7020  loss is :  1.3210446\n",
      "Iteration # 7021  loss is :  1.3210444\n",
      "Iteration # 7022  loss is :  1.3210444\n",
      "Iteration # 7023  loss is :  1.3210441\n",
      "Iteration # 7024  loss is :  1.3210441\n",
      "Iteration # 7025  loss is :  1.321044\n",
      "Iteration # 7026  loss is :  1.321044\n",
      "Iteration # 7027  loss is :  1.3210438\n",
      "Iteration # 7028  loss is :  1.3210437\n",
      "Iteration # 7029  loss is :  1.3210435\n",
      "Iteration # 7030  loss is :  1.3210435\n",
      "Iteration # 7031  loss is :  1.3210433\n",
      "Iteration # 7032  loss is :  1.3210433\n",
      "Iteration # 7033  loss is :  1.3210431\n",
      "Iteration # 7034  loss is :  1.3210429\n",
      "Iteration # 7035  loss is :  1.3210428\n",
      "Iteration # 7036  loss is :  1.3210428\n",
      "Iteration # 7037  loss is :  1.3210425\n",
      "Iteration # 7038  loss is :  1.3210425\n",
      "Iteration # 7039  loss is :  1.3210423\n",
      "Iteration # 7040  loss is :  1.3210423\n",
      "Iteration # 7041  loss is :  1.3210422\n",
      "Iteration # 7042  loss is :  1.3210421\n",
      "Iteration # 7043  loss is :  1.3210421\n",
      "Iteration # 7044  loss is :  1.321042\n",
      "Iteration # 7045  loss is :  1.3210418\n",
      "Iteration # 7046  loss is :  1.3210418\n",
      "Iteration # 7047  loss is :  1.3210415\n",
      "Iteration # 7048  loss is :  1.3210412\n",
      "Iteration # 7049  loss is :  1.3210412\n",
      "Iteration # 7050  loss is :  1.3210412\n",
      "Iteration # 7051  loss is :  1.321041\n",
      "Iteration # 7052  loss is :  1.321041\n",
      "Iteration # 7053  loss is :  1.3210406\n",
      "Iteration # 7054  loss is :  1.3210406\n",
      "Iteration # 7055  loss is :  1.3210406\n",
      "Iteration # 7056  loss is :  1.3210404\n",
      "Iteration # 7057  loss is :  1.3210404\n",
      "Iteration # 7058  loss is :  1.3210402\n",
      "Iteration # 7059  loss is :  1.3210402\n",
      "Iteration # 7060  loss is :  1.32104\n",
      "Iteration # 7061  loss is :  1.3210399\n",
      "Iteration # 7062  loss is :  1.3210399\n",
      "Iteration # 7063  loss is :  1.3210398\n",
      "Iteration # 7064  loss is :  1.3210397\n",
      "Iteration # 7065  loss is :  1.3210396\n",
      "Iteration # 7066  loss is :  1.3210394\n",
      "Iteration # 7067  loss is :  1.3210392\n",
      "Iteration # 7068  loss is :  1.3210392\n",
      "Iteration # 7069  loss is :  1.3210391\n",
      "Iteration # 7070  loss is :  1.321039\n",
      "Iteration # 7071  loss is :  1.3210388\n",
      "Iteration # 7072  loss is :  1.3210386\n",
      "Iteration # 7073  loss is :  1.3210385\n",
      "Iteration # 7074  loss is :  1.3210385\n",
      "Iteration # 7075  loss is :  1.3210384\n",
      "Iteration # 7076  loss is :  1.3210382\n",
      "Iteration # 7077  loss is :  1.3210382\n",
      "Iteration # 7078  loss is :  1.321038\n",
      "Iteration # 7079  loss is :  1.321038\n",
      "Iteration # 7080  loss is :  1.3210378\n",
      "Iteration # 7081  loss is :  1.3210378\n",
      "Iteration # 7082  loss is :  1.3210377\n",
      "Iteration # 7083  loss is :  1.3210375\n",
      "Iteration # 7084  loss is :  1.3210373\n",
      "Iteration # 7085  loss is :  1.3210372\n",
      "Iteration # 7086  loss is :  1.3210372\n",
      "Iteration # 7087  loss is :  1.321037\n",
      "Iteration # 7088  loss is :  1.3210369\n",
      "Iteration # 7089  loss is :  1.3210368\n",
      "Iteration # 7090  loss is :  1.3210366\n",
      "Iteration # 7091  loss is :  1.3210366\n",
      "Iteration # 7092  loss is :  1.3210363\n",
      "Iteration # 7093  loss is :  1.3210363\n",
      "Iteration # 7094  loss is :  1.3210362\n",
      "Iteration # 7095  loss is :  1.3210362\n",
      "Iteration # 7096  loss is :  1.321036\n",
      "Iteration # 7097  loss is :  1.3210359\n",
      "Iteration # 7098  loss is :  1.3210357\n",
      "Iteration # 7099  loss is :  1.3210356\n",
      "Iteration # 7100  loss is :  1.3210356\n",
      "Iteration # 7101  loss is :  1.3210354\n",
      "Iteration # 7102  loss is :  1.3210353\n",
      "Iteration # 7103  loss is :  1.3210353\n",
      "Iteration # 7104  loss is :  1.321035\n",
      "Iteration # 7105  loss is :  1.321035\n",
      "Iteration # 7106  loss is :  1.3210349\n",
      "Iteration # 7107  loss is :  1.3210348\n",
      "Iteration # 7108  loss is :  1.3210347\n",
      "Iteration # 7109  loss is :  1.3210344\n",
      "Iteration # 7110  loss is :  1.3210344\n",
      "Iteration # 7111  loss is :  1.3210344\n",
      "Iteration # 7112  loss is :  1.3210343\n",
      "Iteration # 7113  loss is :  1.3210343\n",
      "Iteration # 7114  loss is :  1.3210341\n",
      "Iteration # 7115  loss is :  1.321034\n",
      "Iteration # 7116  loss is :  1.3210338\n",
      "Iteration # 7117  loss is :  1.3210337\n",
      "Iteration # 7118  loss is :  1.3210336\n",
      "Iteration # 7119  loss is :  1.3210335\n",
      "Iteration # 7120  loss is :  1.3210334\n",
      "Iteration # 7121  loss is :  1.3210332\n",
      "Iteration # 7122  loss is :  1.3210331\n",
      "Iteration # 7123  loss is :  1.321033\n",
      "Iteration # 7124  loss is :  1.3210329\n",
      "Iteration # 7125  loss is :  1.3210326\n",
      "Iteration # 7126  loss is :  1.3210326\n",
      "Iteration # 7127  loss is :  1.3210325\n",
      "Iteration # 7128  loss is :  1.3210324\n",
      "Iteration # 7129  loss is :  1.3210324\n",
      "Iteration # 7130  loss is :  1.3210322\n",
      "Iteration # 7131  loss is :  1.3210322\n",
      "Iteration # 7132  loss is :  1.3210322\n",
      "Iteration # 7133  loss is :  1.321032\n",
      "Iteration # 7134  loss is :  1.3210318\n",
      "Iteration # 7135  loss is :  1.3210316\n",
      "Iteration # 7136  loss is :  1.3210316\n",
      "Iteration # 7137  loss is :  1.3210316\n",
      "Iteration # 7138  loss is :  1.3210313\n",
      "Iteration # 7139  loss is :  1.3210313\n",
      "Iteration # 7140  loss is :  1.321031\n",
      "Iteration # 7141  loss is :  1.3210311\n",
      "Iteration # 7142  loss is :  1.321031\n",
      "Iteration # 7143  loss is :  1.3210307\n",
      "Iteration # 7144  loss is :  1.3210307\n",
      "Iteration # 7145  loss is :  1.3210306\n",
      "Iteration # 7146  loss is :  1.3210305\n",
      "Iteration # 7147  loss is :  1.3210304\n",
      "Iteration # 7148  loss is :  1.3210303\n",
      "Iteration # 7149  loss is :  1.3210303\n",
      "Iteration # 7150  loss is :  1.32103\n",
      "Iteration # 7151  loss is :  1.32103\n",
      "Iteration # 7152  loss is :  1.3210299\n",
      "Iteration # 7153  loss is :  1.3210298\n",
      "Iteration # 7154  loss is :  1.3210295\n",
      "Iteration # 7155  loss is :  1.3210294\n",
      "Iteration # 7156  loss is :  1.3210294\n",
      "Iteration # 7157  loss is :  1.3210293\n",
      "Iteration # 7158  loss is :  1.3210291\n",
      "Iteration # 7159  loss is :  1.3210291\n",
      "Iteration # 7160  loss is :  1.3210288\n",
      "Iteration # 7161  loss is :  1.3210288\n",
      "Iteration # 7162  loss is :  1.3210288\n",
      "Iteration # 7163  loss is :  1.3210287\n",
      "Iteration # 7164  loss is :  1.3210287\n",
      "Iteration # 7165  loss is :  1.3210285\n",
      "Iteration # 7166  loss is :  1.3210284\n",
      "Iteration # 7167  loss is :  1.3210281\n",
      "Iteration # 7168  loss is :  1.3210281\n",
      "Iteration # 7169  loss is :  1.3210281\n",
      "Iteration # 7170  loss is :  1.3210279\n",
      "Iteration # 7171  loss is :  1.3210278\n",
      "Iteration # 7172  loss is :  1.3210276\n",
      "Iteration # 7173  loss is :  1.3210275\n",
      "Iteration # 7174  loss is :  1.3210274\n",
      "Iteration # 7175  loss is :  1.3210274\n",
      "Iteration # 7176  loss is :  1.3210273\n",
      "Iteration # 7177  loss is :  1.321027\n",
      "Iteration # 7178  loss is :  1.3210269\n",
      "Iteration # 7179  loss is :  1.3210268\n",
      "Iteration # 7180  loss is :  1.3210268\n",
      "Iteration # 7181  loss is :  1.3210268\n",
      "Iteration # 7182  loss is :  1.3210267\n",
      "Iteration # 7183  loss is :  1.3210264\n",
      "Iteration # 7184  loss is :  1.3210264\n",
      "Iteration # 7185  loss is :  1.3210262\n",
      "Iteration # 7186  loss is :  1.3210262\n",
      "Iteration # 7187  loss is :  1.3210261\n",
      "Iteration # 7188  loss is :  1.321026\n",
      "Iteration # 7189  loss is :  1.3210256\n",
      "Iteration # 7190  loss is :  1.3210256\n",
      "Iteration # 7191  loss is :  1.3210256\n",
      "Iteration # 7192  loss is :  1.3210256\n",
      "Iteration # 7193  loss is :  1.3210254\n",
      "Iteration # 7194  loss is :  1.3210253\n",
      "Iteration # 7195  loss is :  1.3210253\n",
      "Iteration # 7196  loss is :  1.321025\n",
      "Iteration # 7197  loss is :  1.3210249\n",
      "Iteration # 7198  loss is :  1.3210249\n",
      "Iteration # 7199  loss is :  1.3210249\n",
      "Iteration # 7200  loss is :  1.3210247\n",
      "Iteration # 7201  loss is :  1.3210247\n",
      "Iteration # 7202  loss is :  1.3210244\n",
      "Iteration # 7203  loss is :  1.3210243\n",
      "Iteration # 7204  loss is :  1.3210242\n",
      "Iteration # 7205  loss is :  1.3210242\n",
      "Iteration # 7206  loss is :  1.3210241\n",
      "Iteration # 7207  loss is :  1.321024\n",
      "Iteration # 7208  loss is :  1.3210238\n",
      "Iteration # 7209  loss is :  1.3210237\n",
      "Iteration # 7210  loss is :  1.3210235\n",
      "Iteration # 7211  loss is :  1.3210235\n",
      "Iteration # 7212  loss is :  1.3210233\n",
      "Iteration # 7213  loss is :  1.3210233\n",
      "Iteration # 7214  loss is :  1.3210231\n",
      "Iteration # 7215  loss is :  1.321023\n",
      "Iteration # 7216  loss is :  1.321023\n",
      "Iteration # 7217  loss is :  1.3210229\n",
      "Iteration # 7218  loss is :  1.3210227\n",
      "Iteration # 7219  loss is :  1.3210226\n",
      "Iteration # 7220  loss is :  1.3210226\n",
      "Iteration # 7221  loss is :  1.3210224\n",
      "Iteration # 7222  loss is :  1.3210223\n",
      "Iteration # 7223  loss is :  1.3210222\n",
      "Iteration # 7224  loss is :  1.3210222\n",
      "Iteration # 7225  loss is :  1.3210219\n",
      "Iteration # 7226  loss is :  1.3210219\n",
      "Iteration # 7227  loss is :  1.3210218\n",
      "Iteration # 7228  loss is :  1.3210216\n",
      "Iteration # 7229  loss is :  1.3210214\n",
      "Iteration # 7230  loss is :  1.3210214\n",
      "Iteration # 7231  loss is :  1.3210213\n",
      "Iteration # 7232  loss is :  1.3210211\n",
      "Iteration # 7233  loss is :  1.3210211\n",
      "Iteration # 7234  loss is :  1.3210211\n",
      "Iteration # 7235  loss is :  1.3210208\n",
      "Iteration # 7236  loss is :  1.3210208\n",
      "Iteration # 7237  loss is :  1.3210207\n",
      "Iteration # 7238  loss is :  1.3210206\n",
      "Iteration # 7239  loss is :  1.3210205\n",
      "Iteration # 7240  loss is :  1.3210204\n",
      "Iteration # 7241  loss is :  1.3210202\n",
      "Iteration # 7242  loss is :  1.3210201\n",
      "Iteration # 7243  loss is :  1.32102\n",
      "Iteration # 7244  loss is :  1.32102\n",
      "Iteration # 7245  loss is :  1.3210198\n",
      "Iteration # 7246  loss is :  1.3210198\n",
      "Iteration # 7247  loss is :  1.3210194\n",
      "Iteration # 7248  loss is :  1.3210194\n",
      "Iteration # 7249  loss is :  1.3210194\n",
      "Iteration # 7250  loss is :  1.3210193\n",
      "Iteration # 7251  loss is :  1.3210192\n",
      "Iteration # 7252  loss is :  1.3210192\n",
      "Iteration # 7253  loss is :  1.321019\n",
      "Iteration # 7254  loss is :  1.3210188\n",
      "Iteration # 7255  loss is :  1.3210188\n",
      "Iteration # 7256  loss is :  1.3210187\n",
      "Iteration # 7257  loss is :  1.3210186\n",
      "Iteration # 7258  loss is :  1.3210185\n",
      "Iteration # 7259  loss is :  1.3210183\n",
      "Iteration # 7260  loss is :  1.3210182\n",
      "Iteration # 7261  loss is :  1.3210181\n",
      "Iteration # 7262  loss is :  1.321018\n",
      "Iteration # 7263  loss is :  1.3210179\n",
      "Iteration # 7264  loss is :  1.3210177\n",
      "Iteration # 7265  loss is :  1.3210177\n",
      "Iteration # 7266  loss is :  1.3210176\n",
      "Iteration # 7267  loss is :  1.3210175\n",
      "Iteration # 7268  loss is :  1.3210173\n",
      "Iteration # 7269  loss is :  1.3210173\n",
      "Iteration # 7270  loss is :  1.3210173\n",
      "Iteration # 7271  loss is :  1.3210171\n",
      "Iteration # 7272  loss is :  1.321017\n",
      "Iteration # 7273  loss is :  1.3210169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 7274  loss is :  1.3210168\n",
      "Iteration # 7275  loss is :  1.3210167\n",
      "Iteration # 7276  loss is :  1.3210166\n",
      "Iteration # 7277  loss is :  1.3210166\n",
      "Iteration # 7278  loss is :  1.3210166\n",
      "Iteration # 7279  loss is :  1.3210163\n",
      "Iteration # 7280  loss is :  1.3210162\n",
      "Iteration # 7281  loss is :  1.3210161\n",
      "Iteration # 7282  loss is :  1.3210158\n",
      "Iteration # 7283  loss is :  1.3210158\n",
      "Iteration # 7284  loss is :  1.3210157\n",
      "Iteration # 7285  loss is :  1.3210156\n",
      "Iteration # 7286  loss is :  1.3210154\n",
      "Iteration # 7287  loss is :  1.3210154\n",
      "Iteration # 7288  loss is :  1.3210152\n",
      "Iteration # 7289  loss is :  1.3210152\n",
      "Iteration # 7290  loss is :  1.3210152\n",
      "Iteration # 7291  loss is :  1.3210152\n",
      "Iteration # 7292  loss is :  1.321015\n",
      "Iteration # 7293  loss is :  1.3210146\n",
      "Iteration # 7294  loss is :  1.3210146\n",
      "Iteration # 7295  loss is :  1.3210146\n",
      "Iteration # 7296  loss is :  1.3210144\n",
      "Iteration # 7297  loss is :  1.3210144\n",
      "Iteration # 7298  loss is :  1.3210144\n",
      "Iteration # 7299  loss is :  1.321014\n",
      "Iteration # 7300  loss is :  1.3210139\n",
      "Iteration # 7301  loss is :  1.3210139\n",
      "Iteration # 7302  loss is :  1.3210138\n",
      "Iteration # 7303  loss is :  1.3210137\n",
      "Iteration # 7304  loss is :  1.3210136\n",
      "Iteration # 7305  loss is :  1.3210135\n",
      "Iteration # 7306  loss is :  1.3210135\n",
      "Iteration # 7307  loss is :  1.3210133\n",
      "Iteration # 7308  loss is :  1.3210133\n",
      "Iteration # 7309  loss is :  1.3210131\n",
      "Iteration # 7310  loss is :  1.321013\n",
      "Iteration # 7311  loss is :  1.321013\n",
      "Iteration # 7312  loss is :  1.3210129\n",
      "Iteration # 7313  loss is :  1.3210127\n",
      "Iteration # 7314  loss is :  1.3210125\n",
      "Iteration # 7315  loss is :  1.3210125\n",
      "Iteration # 7316  loss is :  1.3210123\n",
      "Iteration # 7317  loss is :  1.3210123\n",
      "Iteration # 7318  loss is :  1.3210121\n",
      "Iteration # 7319  loss is :  1.3210119\n",
      "Iteration # 7320  loss is :  1.3210119\n",
      "Iteration # 7321  loss is :  1.3210118\n",
      "Iteration # 7322  loss is :  1.3210117\n",
      "Iteration # 7323  loss is :  1.3210117\n",
      "Iteration # 7324  loss is :  1.3210115\n",
      "Iteration # 7325  loss is :  1.3210115\n",
      "Iteration # 7326  loss is :  1.3210113\n",
      "Iteration # 7327  loss is :  1.3210112\n",
      "Iteration # 7328  loss is :  1.3210112\n",
      "Iteration # 7329  loss is :  1.3210111\n",
      "Iteration # 7330  loss is :  1.321011\n",
      "Iteration # 7331  loss is :  1.321011\n",
      "Iteration # 7332  loss is :  1.3210106\n",
      "Iteration # 7333  loss is :  1.3210106\n",
      "Iteration # 7334  loss is :  1.3210105\n",
      "Iteration # 7335  loss is :  1.3210105\n",
      "Iteration # 7336  loss is :  1.3210104\n",
      "Iteration # 7337  loss is :  1.3210102\n",
      "Iteration # 7338  loss is :  1.3210101\n",
      "Iteration # 7339  loss is :  1.32101\n",
      "Iteration # 7340  loss is :  1.3210098\n",
      "Iteration # 7341  loss is :  1.3210098\n",
      "Iteration # 7342  loss is :  1.3210096\n",
      "Iteration # 7343  loss is :  1.3210096\n",
      "Iteration # 7344  loss is :  1.3210096\n",
      "Iteration # 7345  loss is :  1.3210094\n",
      "Iteration # 7346  loss is :  1.3210093\n",
      "Iteration # 7347  loss is :  1.3210093\n",
      "Iteration # 7348  loss is :  1.3210092\n",
      "Iteration # 7349  loss is :  1.321009\n",
      "Iteration # 7350  loss is :  1.321009\n",
      "Iteration # 7351  loss is :  1.3210088\n",
      "Iteration # 7352  loss is :  1.3210088\n",
      "Iteration # 7353  loss is :  1.3210084\n",
      "Iteration # 7354  loss is :  1.3210084\n",
      "Iteration # 7355  loss is :  1.3210084\n",
      "Iteration # 7356  loss is :  1.3210082\n",
      "Iteration # 7357  loss is :  1.3210082\n",
      "Iteration # 7358  loss is :  1.3210081\n",
      "Iteration # 7359  loss is :  1.3210078\n",
      "Iteration # 7360  loss is :  1.3210077\n",
      "Iteration # 7361  loss is :  1.3210077\n",
      "Iteration # 7362  loss is :  1.3210077\n",
      "Iteration # 7363  loss is :  1.3210076\n",
      "Iteration # 7364  loss is :  1.3210075\n",
      "Iteration # 7365  loss is :  1.3210074\n",
      "Iteration # 7366  loss is :  1.3210073\n",
      "Iteration # 7367  loss is :  1.3210071\n",
      "Iteration # 7368  loss is :  1.3210071\n",
      "Iteration # 7369  loss is :  1.3210069\n",
      "Iteration # 7370  loss is :  1.3210069\n",
      "Iteration # 7371  loss is :  1.3210067\n",
      "Iteration # 7372  loss is :  1.3210065\n",
      "Iteration # 7373  loss is :  1.3210065\n",
      "Iteration # 7374  loss is :  1.3210063\n",
      "Iteration # 7375  loss is :  1.3210063\n",
      "Iteration # 7376  loss is :  1.3210062\n",
      "Iteration # 7377  loss is :  1.3210062\n",
      "Iteration # 7378  loss is :  1.3210061\n",
      "Iteration # 7379  loss is :  1.3210058\n",
      "Iteration # 7380  loss is :  1.3210058\n",
      "Iteration # 7381  loss is :  1.3210058\n",
      "Iteration # 7382  loss is :  1.3210056\n",
      "Iteration # 7383  loss is :  1.3210056\n",
      "Iteration # 7384  loss is :  1.3210055\n",
      "Iteration # 7385  loss is :  1.3210053\n",
      "Iteration # 7386  loss is :  1.3210052\n",
      "Iteration # 7387  loss is :  1.3210051\n",
      "Iteration # 7388  loss is :  1.321005\n",
      "Iteration # 7389  loss is :  1.3210049\n",
      "Iteration # 7390  loss is :  1.3210047\n",
      "Iteration # 7391  loss is :  1.3210047\n",
      "Iteration # 7392  loss is :  1.3210046\n",
      "Iteration # 7393  loss is :  1.3210044\n",
      "Iteration # 7394  loss is :  1.3210044\n",
      "Iteration # 7395  loss is :  1.3210043\n",
      "Iteration # 7396  loss is :  1.3210042\n",
      "Iteration # 7397  loss is :  1.3210042\n",
      "Iteration # 7398  loss is :  1.3210039\n",
      "Iteration # 7399  loss is :  1.3210039\n",
      "Iteration # 7400  loss is :  1.3210038\n",
      "Iteration # 7401  loss is :  1.3210038\n",
      "Iteration # 7402  loss is :  1.3210037\n",
      "Iteration # 7403  loss is :  1.3210036\n",
      "Iteration # 7404  loss is :  1.3210034\n",
      "Iteration # 7405  loss is :  1.3210033\n",
      "Iteration # 7406  loss is :  1.3210031\n",
      "Iteration # 7407  loss is :  1.3210031\n",
      "Iteration # 7408  loss is :  1.321003\n",
      "Iteration # 7409  loss is :  1.3210028\n",
      "Iteration # 7410  loss is :  1.3210028\n",
      "Iteration # 7411  loss is :  1.3210027\n",
      "Iteration # 7412  loss is :  1.3210026\n",
      "Iteration # 7413  loss is :  1.3210024\n",
      "Iteration # 7414  loss is :  1.3210024\n",
      "Iteration # 7415  loss is :  1.3210022\n",
      "Iteration # 7416  loss is :  1.3210021\n",
      "Iteration # 7417  loss is :  1.321002\n",
      "Iteration # 7418  loss is :  1.321002\n",
      "Iteration # 7419  loss is :  1.321002\n",
      "Iteration # 7420  loss is :  1.3210018\n",
      "Iteration # 7421  loss is :  1.3210018\n",
      "Iteration # 7422  loss is :  1.3210015\n",
      "Iteration # 7423  loss is :  1.3210015\n",
      "Iteration # 7424  loss is :  1.3210013\n",
      "Iteration # 7425  loss is :  1.3210013\n",
      "Iteration # 7426  loss is :  1.3210013\n",
      "Iteration # 7427  loss is :  1.3210009\n",
      "Iteration # 7428  loss is :  1.3210009\n",
      "Iteration # 7429  loss is :  1.3210009\n",
      "Iteration # 7430  loss is :  1.3210007\n",
      "Iteration # 7431  loss is :  1.3210007\n",
      "Iteration # 7432  loss is :  1.3210006\n",
      "Iteration # 7433  loss is :  1.3210005\n",
      "Iteration # 7434  loss is :  1.3210003\n",
      "Iteration # 7435  loss is :  1.3210002\n",
      "Iteration # 7436  loss is :  1.3210001\n",
      "Iteration # 7437  loss is :  1.3210001\n",
      "Iteration # 7438  loss is :  1.3210001\n",
      "Iteration # 7439  loss is :  1.321\n",
      "Iteration # 7440  loss is :  1.3209999\n",
      "Iteration # 7441  loss is :  1.3209996\n",
      "Iteration # 7442  loss is :  1.3209997\n",
      "Iteration # 7443  loss is :  1.3209995\n",
      "Iteration # 7444  loss is :  1.3209994\n",
      "Iteration # 7445  loss is :  1.3209994\n",
      "Iteration # 7446  loss is :  1.3209993\n",
      "Iteration # 7447  loss is :  1.320999\n",
      "Iteration # 7448  loss is :  1.3209989\n",
      "Iteration # 7449  loss is :  1.3209988\n",
      "Iteration # 7450  loss is :  1.3209988\n",
      "Iteration # 7451  loss is :  1.3209988\n",
      "Iteration # 7452  loss is :  1.3209987\n",
      "Iteration # 7453  loss is :  1.3209984\n",
      "Iteration # 7454  loss is :  1.3209984\n",
      "Iteration # 7455  loss is :  1.3209983\n",
      "Iteration # 7456  loss is :  1.3209982\n",
      "Iteration # 7457  loss is :  1.3209981\n",
      "Iteration # 7458  loss is :  1.3209981\n",
      "Iteration # 7459  loss is :  1.320998\n",
      "Iteration # 7460  loss is :  1.3209978\n",
      "Iteration # 7461  loss is :  1.3209977\n",
      "Iteration # 7462  loss is :  1.3209976\n",
      "Iteration # 7463  loss is :  1.3209975\n",
      "Iteration # 7464  loss is :  1.3209974\n",
      "Iteration # 7465  loss is :  1.3209972\n",
      "Iteration # 7466  loss is :  1.3209972\n",
      "Iteration # 7467  loss is :  1.3209972\n",
      "Iteration # 7468  loss is :  1.3209969\n",
      "Iteration # 7469  loss is :  1.3209969\n",
      "Iteration # 7470  loss is :  1.3209968\n",
      "Iteration # 7471  loss is :  1.3209968\n",
      "Iteration # 7472  loss is :  1.3209966\n",
      "Iteration # 7473  loss is :  1.3209965\n",
      "Iteration # 7474  loss is :  1.3209964\n",
      "Iteration # 7475  loss is :  1.3209963\n",
      "Iteration # 7476  loss is :  1.3209963\n",
      "Iteration # 7477  loss is :  1.3209962\n",
      "Iteration # 7478  loss is :  1.3209962\n",
      "Iteration # 7479  loss is :  1.3209959\n",
      "Iteration # 7480  loss is :  1.3209959\n",
      "Iteration # 7481  loss is :  1.3209958\n",
      "Iteration # 7482  loss is :  1.3209956\n",
      "Iteration # 7483  loss is :  1.3209955\n",
      "Iteration # 7484  loss is :  1.3209953\n",
      "Iteration # 7485  loss is :  1.3209953\n",
      "Iteration # 7486  loss is :  1.3209953\n",
      "Iteration # 7487  loss is :  1.3209951\n",
      "Iteration # 7488  loss is :  1.3209951\n",
      "Iteration # 7489  loss is :  1.320995\n",
      "Iteration # 7490  loss is :  1.3209949\n",
      "Iteration # 7491  loss is :  1.3209947\n",
      "Iteration # 7492  loss is :  1.3209945\n",
      "Iteration # 7493  loss is :  1.3209945\n",
      "Iteration # 7494  loss is :  1.3209944\n",
      "Iteration # 7495  loss is :  1.3209944\n",
      "Iteration # 7496  loss is :  1.3209943\n",
      "Iteration # 7497  loss is :  1.3209943\n",
      "Iteration # 7498  loss is :  1.320994\n",
      "Iteration # 7499  loss is :  1.320994\n",
      "Iteration # 7500  loss is :  1.3209939\n",
      "Iteration # 7501  loss is :  1.3209938\n",
      "Iteration # 7502  loss is :  1.3209938\n",
      "Iteration # 7503  loss is :  1.3209935\n",
      "Iteration # 7504  loss is :  1.3209934\n",
      "Iteration # 7505  loss is :  1.3209933\n",
      "Iteration # 7506  loss is :  1.3209932\n",
      "Iteration # 7507  loss is :  1.3209932\n",
      "Iteration # 7508  loss is :  1.3209932\n",
      "Iteration # 7509  loss is :  1.320993\n",
      "Iteration # 7510  loss is :  1.3209928\n",
      "Iteration # 7511  loss is :  1.3209927\n",
      "Iteration # 7512  loss is :  1.3209926\n",
      "Iteration # 7513  loss is :  1.3209925\n",
      "Iteration # 7514  loss is :  1.3209925\n",
      "Iteration # 7515  loss is :  1.3209925\n",
      "Iteration # 7516  loss is :  1.3209924\n",
      "Iteration # 7517  loss is :  1.3209921\n",
      "Iteration # 7518  loss is :  1.3209921\n",
      "Iteration # 7519  loss is :  1.3209921\n",
      "Iteration # 7520  loss is :  1.3209919\n",
      "Iteration # 7521  loss is :  1.3209919\n",
      "Iteration # 7522  loss is :  1.3209919\n",
      "Iteration # 7523  loss is :  1.3209916\n",
      "Iteration # 7524  loss is :  1.3209915\n",
      "Iteration # 7525  loss is :  1.3209913\n",
      "Iteration # 7526  loss is :  1.3209913\n",
      "Iteration # 7527  loss is :  1.3209913\n",
      "Iteration # 7528  loss is :  1.320991\n",
      "Iteration # 7529  loss is :  1.320991\n",
      "Iteration # 7530  loss is :  1.320991\n",
      "Iteration # 7531  loss is :  1.3209907\n",
      "Iteration # 7532  loss is :  1.3209907\n",
      "Iteration # 7533  loss is :  1.3209907\n",
      "Iteration # 7534  loss is :  1.3209906\n",
      "Iteration # 7535  loss is :  1.3209904\n",
      "Iteration # 7536  loss is :  1.3209904\n",
      "Iteration # 7537  loss is :  1.3209903\n",
      "Iteration # 7538  loss is :  1.3209902\n",
      "Iteration # 7539  loss is :  1.3209901\n",
      "Iteration # 7540  loss is :  1.32099\n",
      "Iteration # 7541  loss is :  1.32099\n",
      "Iteration # 7542  loss is :  1.3209897\n",
      "Iteration # 7543  loss is :  1.3209897\n",
      "Iteration # 7544  loss is :  1.3209897\n",
      "Iteration # 7545  loss is :  1.3209894\n",
      "Iteration # 7546  loss is :  1.3209894\n",
      "Iteration # 7547  loss is :  1.3209894\n",
      "Iteration # 7548  loss is :  1.3209893\n",
      "Iteration # 7549  loss is :  1.3209891\n",
      "Iteration # 7550  loss is :  1.320989\n",
      "Iteration # 7551  loss is :  1.3209889\n",
      "Iteration # 7552  loss is :  1.3209888\n",
      "Iteration # 7553  loss is :  1.3209887\n",
      "Iteration # 7554  loss is :  1.3209887\n",
      "Iteration # 7555  loss is :  1.3209887\n",
      "Iteration # 7556  loss is :  1.3209885\n",
      "Iteration # 7557  loss is :  1.3209884\n",
      "Iteration # 7558  loss is :  1.3209883\n",
      "Iteration # 7559  loss is :  1.320988\n",
      "Iteration # 7560  loss is :  1.320988\n",
      "Iteration # 7561  loss is :  1.3209878\n",
      "Iteration # 7562  loss is :  1.3209878\n",
      "Iteration # 7563  loss is :  1.3209878\n",
      "Iteration # 7564  loss is :  1.3209877\n",
      "Iteration # 7565  loss is :  1.3209877\n",
      "Iteration # 7566  loss is :  1.3209875\n",
      "Iteration # 7567  loss is :  1.3209873\n",
      "Iteration # 7568  loss is :  1.3209872\n",
      "Iteration # 7569  loss is :  1.3209872\n",
      "Iteration # 7570  loss is :  1.3209871\n",
      "Iteration # 7571  loss is :  1.320987\n",
      "Iteration # 7572  loss is :  1.320987\n",
      "Iteration # 7573  loss is :  1.3209867\n",
      "Iteration # 7574  loss is :  1.3209867\n",
      "Iteration # 7575  loss is :  1.3209866\n",
      "Iteration # 7576  loss is :  1.3209865\n",
      "Iteration # 7577  loss is :  1.3209865\n",
      "Iteration # 7578  loss is :  1.3209864\n",
      "Iteration # 7579  loss is :  1.3209863\n",
      "Iteration # 7580  loss is :  1.3209863\n",
      "Iteration # 7581  loss is :  1.320986\n",
      "Iteration # 7582  loss is :  1.3209859\n",
      "Iteration # 7583  loss is :  1.3209859\n",
      "Iteration # 7584  loss is :  1.3209857\n",
      "Iteration # 7585  loss is :  1.3209857\n",
      "Iteration # 7586  loss is :  1.3209857\n",
      "Iteration # 7587  loss is :  1.3209854\n",
      "Iteration # 7588  loss is :  1.3209853\n",
      "Iteration # 7589  loss is :  1.3209853\n",
      "Iteration # 7590  loss is :  1.3209851\n",
      "Iteration # 7591  loss is :  1.3209851\n",
      "Iteration # 7592  loss is :  1.320985\n",
      "Iteration # 7593  loss is :  1.3209848\n",
      "Iteration # 7594  loss is :  1.3209848\n",
      "Iteration # 7595  loss is :  1.3209848\n",
      "Iteration # 7596  loss is :  1.3209846\n",
      "Iteration # 7597  loss is :  1.3209845\n",
      "Iteration # 7598  loss is :  1.3209844\n",
      "Iteration # 7599  loss is :  1.3209844\n",
      "Iteration # 7600  loss is :  1.3209842\n",
      "Iteration # 7601  loss is :  1.3209842\n",
      "Iteration # 7602  loss is :  1.320984\n",
      "Iteration # 7603  loss is :  1.3209839\n",
      "Iteration # 7604  loss is :  1.3209838\n",
      "Iteration # 7605  loss is :  1.3209838\n",
      "Iteration # 7606  loss is :  1.3209836\n",
      "Iteration # 7607  loss is :  1.3209835\n",
      "Iteration # 7608  loss is :  1.3209835\n",
      "Iteration # 7609  loss is :  1.3209834\n",
      "Iteration # 7610  loss is :  1.3209832\n",
      "Iteration # 7611  loss is :  1.3209832\n",
      "Iteration # 7612  loss is :  1.320983\n",
      "Iteration # 7613  loss is :  1.3209829\n",
      "Iteration # 7614  loss is :  1.3209829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 7615  loss is :  1.3209829\n",
      "Iteration # 7616  loss is :  1.3209828\n",
      "Iteration # 7617  loss is :  1.3209827\n",
      "Iteration # 7618  loss is :  1.3209825\n",
      "Iteration # 7619  loss is :  1.3209825\n",
      "Iteration # 7620  loss is :  1.3209825\n",
      "Iteration # 7621  loss is :  1.3209822\n",
      "Iteration # 7622  loss is :  1.3209822\n",
      "Iteration # 7623  loss is :  1.3209821\n",
      "Iteration # 7624  loss is :  1.3209819\n",
      "Iteration # 7625  loss is :  1.3209817\n",
      "Iteration # 7626  loss is :  1.3209817\n",
      "Iteration # 7627  loss is :  1.3209816\n",
      "Iteration # 7628  loss is :  1.3209816\n",
      "Iteration # 7629  loss is :  1.3209816\n",
      "Iteration # 7630  loss is :  1.3209814\n",
      "Iteration # 7631  loss is :  1.3209813\n",
      "Iteration # 7632  loss is :  1.3209813\n",
      "Iteration # 7633  loss is :  1.320981\n",
      "Iteration # 7634  loss is :  1.320981\n",
      "Iteration # 7635  loss is :  1.320981\n",
      "Iteration # 7636  loss is :  1.3209809\n",
      "Iteration # 7637  loss is :  1.3209808\n",
      "Iteration # 7638  loss is :  1.3209805\n",
      "Iteration # 7639  loss is :  1.3209805\n",
      "Iteration # 7640  loss is :  1.3209804\n",
      "Iteration # 7641  loss is :  1.3209804\n",
      "Iteration # 7642  loss is :  1.3209803\n",
      "Iteration # 7643  loss is :  1.3209803\n",
      "Iteration # 7644  loss is :  1.3209801\n",
      "Iteration # 7645  loss is :  1.3209801\n",
      "Iteration # 7646  loss is :  1.3209798\n",
      "Iteration # 7647  loss is :  1.3209797\n",
      "Iteration # 7648  loss is :  1.3209796\n",
      "Iteration # 7649  loss is :  1.3209795\n",
      "Iteration # 7650  loss is :  1.3209795\n",
      "Iteration # 7651  loss is :  1.3209795\n",
      "Iteration # 7652  loss is :  1.3209794\n",
      "Iteration # 7653  loss is :  1.3209791\n",
      "Iteration # 7654  loss is :  1.3209791\n",
      "Iteration # 7655  loss is :  1.3209791\n",
      "Iteration # 7656  loss is :  1.320979\n",
      "Iteration # 7657  loss is :  1.320979\n",
      "Iteration # 7658  loss is :  1.3209789\n",
      "Iteration # 7659  loss is :  1.3209788\n",
      "Iteration # 7660  loss is :  1.3209786\n",
      "Iteration # 7661  loss is :  1.3209785\n",
      "Iteration # 7662  loss is :  1.3209784\n",
      "Iteration # 7663  loss is :  1.3209783\n",
      "Iteration # 7664  loss is :  1.3209783\n",
      "Iteration # 7665  loss is :  1.3209782\n",
      "Iteration # 7666  loss is :  1.3209782\n",
      "Iteration # 7667  loss is :  1.3209779\n",
      "Iteration # 7668  loss is :  1.3209777\n",
      "Iteration # 7669  loss is :  1.3209777\n",
      "Iteration # 7670  loss is :  1.3209776\n",
      "Iteration # 7671  loss is :  1.3209776\n",
      "Iteration # 7672  loss is :  1.3209774\n",
      "Iteration # 7673  loss is :  1.3209773\n",
      "Iteration # 7674  loss is :  1.3209773\n",
      "Iteration # 7675  loss is :  1.3209772\n",
      "Iteration # 7676  loss is :  1.3209771\n",
      "Iteration # 7677  loss is :  1.3209771\n",
      "Iteration # 7678  loss is :  1.3209771\n",
      "Iteration # 7679  loss is :  1.3209769\n",
      "Iteration # 7680  loss is :  1.3209767\n",
      "Iteration # 7681  loss is :  1.3209766\n",
      "Iteration # 7682  loss is :  1.3209765\n",
      "Iteration # 7683  loss is :  1.3209764\n",
      "Iteration # 7684  loss is :  1.3209764\n",
      "Iteration # 7685  loss is :  1.3209763\n",
      "Iteration # 7686  loss is :  1.3209763\n",
      "Iteration # 7687  loss is :  1.3209761\n",
      "Iteration # 7688  loss is :  1.320976\n",
      "Iteration # 7689  loss is :  1.320976\n",
      "Iteration # 7690  loss is :  1.3209758\n",
      "Iteration # 7691  loss is :  1.3209757\n",
      "Iteration # 7692  loss is :  1.3209755\n",
      "Iteration # 7693  loss is :  1.3209754\n",
      "Iteration # 7694  loss is :  1.3209754\n",
      "Iteration # 7695  loss is :  1.3209753\n",
      "Iteration # 7696  loss is :  1.3209753\n",
      "Iteration # 7697  loss is :  1.3209752\n",
      "Iteration # 7698  loss is :  1.3209752\n",
      "Iteration # 7699  loss is :  1.320975\n",
      "Iteration # 7700  loss is :  1.320975\n",
      "Iteration # 7701  loss is :  1.3209748\n",
      "Iteration # 7702  loss is :  1.3209747\n",
      "Iteration # 7703  loss is :  1.3209747\n",
      "Iteration # 7704  loss is :  1.3209746\n",
      "Iteration # 7705  loss is :  1.3209743\n",
      "Iteration # 7706  loss is :  1.3209743\n",
      "Iteration # 7707  loss is :  1.3209742\n",
      "Iteration # 7708  loss is :  1.3209741\n",
      "Iteration # 7709  loss is :  1.3209741\n",
      "Iteration # 7710  loss is :  1.3209739\n",
      "Iteration # 7711  loss is :  1.3209739\n",
      "Iteration # 7712  loss is :  1.3209738\n",
      "Iteration # 7713  loss is :  1.3209736\n",
      "Iteration # 7714  loss is :  1.3209735\n",
      "Iteration # 7715  loss is :  1.3209734\n",
      "Iteration # 7716  loss is :  1.3209734\n",
      "Iteration # 7717  loss is :  1.3209734\n",
      "Iteration # 7718  loss is :  1.3209734\n",
      "Iteration # 7719  loss is :  1.3209732\n",
      "Iteration # 7720  loss is :  1.320973\n",
      "Iteration # 7721  loss is :  1.320973\n",
      "Iteration # 7722  loss is :  1.3209729\n",
      "Iteration # 7723  loss is :  1.3209728\n",
      "Iteration # 7724  loss is :  1.3209727\n",
      "Iteration # 7725  loss is :  1.3209726\n",
      "Iteration # 7726  loss is :  1.3209726\n",
      "Iteration # 7727  loss is :  1.3209723\n",
      "Iteration # 7728  loss is :  1.3209722\n",
      "Iteration # 7729  loss is :  1.3209722\n",
      "Iteration # 7730  loss is :  1.3209721\n",
      "Iteration # 7731  loss is :  1.320972\n",
      "Iteration # 7732  loss is :  1.320972\n",
      "Iteration # 7733  loss is :  1.320972\n",
      "Iteration # 7734  loss is :  1.3209716\n",
      "Iteration # 7735  loss is :  1.3209716\n",
      "Iteration # 7736  loss is :  1.3209715\n",
      "Iteration # 7737  loss is :  1.3209715\n",
      "Iteration # 7738  loss is :  1.3209715\n",
      "Iteration # 7739  loss is :  1.3209715\n",
      "Iteration # 7740  loss is :  1.3209714\n",
      "Iteration # 7741  loss is :  1.3209713\n",
      "Iteration # 7742  loss is :  1.320971\n",
      "Iteration # 7743  loss is :  1.320971\n",
      "Iteration # 7744  loss is :  1.3209709\n",
      "Iteration # 7745  loss is :  1.3209709\n",
      "Iteration # 7746  loss is :  1.3209707\n",
      "Iteration # 7747  loss is :  1.3209707\n",
      "Iteration # 7748  loss is :  1.3209704\n",
      "Iteration # 7749  loss is :  1.3209705\n",
      "Iteration # 7750  loss is :  1.3209703\n",
      "Iteration # 7751  loss is :  1.3209702\n",
      "Iteration # 7752  loss is :  1.32097\n",
      "Iteration # 7753  loss is :  1.32097\n",
      "Iteration # 7754  loss is :  1.32097\n",
      "Iteration # 7755  loss is :  1.3209698\n",
      "Iteration # 7756  loss is :  1.3209698\n",
      "Iteration # 7757  loss is :  1.3209696\n",
      "Iteration # 7758  loss is :  1.3209696\n",
      "Iteration # 7759  loss is :  1.3209696\n",
      "Iteration # 7760  loss is :  1.3209695\n",
      "Iteration # 7761  loss is :  1.3209693\n",
      "Iteration # 7762  loss is :  1.3209693\n",
      "Iteration # 7763  loss is :  1.3209693\n",
      "Iteration # 7764  loss is :  1.3209691\n",
      "Iteration # 7765  loss is :  1.320969\n",
      "Iteration # 7766  loss is :  1.320969\n",
      "Iteration # 7767  loss is :  1.3209689\n",
      "Iteration # 7768  loss is :  1.3209687\n",
      "Iteration # 7769  loss is :  1.3209686\n",
      "Iteration # 7770  loss is :  1.3209685\n",
      "Iteration # 7771  loss is :  1.3209684\n",
      "Iteration # 7772  loss is :  1.3209683\n",
      "Iteration # 7773  loss is :  1.3209682\n",
      "Iteration # 7774  loss is :  1.3209682\n",
      "Iteration # 7775  loss is :  1.320968\n",
      "Iteration # 7776  loss is :  1.3209679\n",
      "Iteration # 7777  loss is :  1.3209679\n",
      "Iteration # 7778  loss is :  1.3209678\n",
      "Iteration # 7779  loss is :  1.3209677\n",
      "Iteration # 7780  loss is :  1.3209677\n",
      "Iteration # 7781  loss is :  1.3209677\n",
      "Iteration # 7782  loss is :  1.3209674\n",
      "Iteration # 7783  loss is :  1.3209674\n",
      "Iteration # 7784  loss is :  1.3209674\n",
      "Iteration # 7785  loss is :  1.3209672\n",
      "Iteration # 7786  loss is :  1.3209672\n",
      "Iteration # 7787  loss is :  1.320967\n",
      "Iteration # 7788  loss is :  1.3209668\n",
      "Iteration # 7789  loss is :  1.3209668\n",
      "Iteration # 7790  loss is :  1.3209666\n",
      "Iteration # 7791  loss is :  1.3209666\n",
      "Iteration # 7792  loss is :  1.3209666\n",
      "Iteration # 7793  loss is :  1.3209666\n",
      "Iteration # 7794  loss is :  1.3209664\n",
      "Iteration # 7795  loss is :  1.3209662\n",
      "Iteration # 7796  loss is :  1.320966\n",
      "Iteration # 7797  loss is :  1.320966\n",
      "Iteration # 7798  loss is :  1.320966\n",
      "Iteration # 7799  loss is :  1.320966\n",
      "Iteration # 7800  loss is :  1.3209658\n",
      "Iteration # 7801  loss is :  1.3209658\n",
      "Iteration # 7802  loss is :  1.3209658\n",
      "Iteration # 7803  loss is :  1.3209656\n",
      "Iteration # 7804  loss is :  1.3209655\n",
      "Iteration # 7805  loss is :  1.3209654\n",
      "Iteration # 7806  loss is :  1.3209653\n",
      "Iteration # 7807  loss is :  1.3209653\n",
      "Iteration # 7808  loss is :  1.3209652\n",
      "Iteration # 7809  loss is :  1.320965\n",
      "Iteration # 7810  loss is :  1.3209649\n",
      "Iteration # 7811  loss is :  1.3209647\n",
      "Iteration # 7812  loss is :  1.3209647\n",
      "Iteration # 7813  loss is :  1.3209647\n",
      "Iteration # 7814  loss is :  1.3209646\n",
      "Iteration # 7815  loss is :  1.3209645\n",
      "Iteration # 7816  loss is :  1.3209645\n",
      "Iteration # 7817  loss is :  1.3209645\n",
      "Iteration # 7818  loss is :  1.3209641\n",
      "Iteration # 7819  loss is :  1.3209641\n",
      "Iteration # 7820  loss is :  1.3209641\n",
      "Iteration # 7821  loss is :  1.3209639\n",
      "Iteration # 7822  loss is :  1.3209639\n",
      "Iteration # 7823  loss is :  1.3209639\n",
      "Iteration # 7824  loss is :  1.3209637\n",
      "Iteration # 7825  loss is :  1.3209636\n",
      "Iteration # 7826  loss is :  1.3209635\n",
      "Iteration # 7827  loss is :  1.3209635\n",
      "Iteration # 7828  loss is :  1.3209634\n",
      "Iteration # 7829  loss is :  1.3209633\n",
      "Iteration # 7830  loss is :  1.3209631\n",
      "Iteration # 7831  loss is :  1.3209631\n",
      "Iteration # 7832  loss is :  1.320963\n",
      "Iteration # 7833  loss is :  1.3209629\n",
      "Iteration # 7834  loss is :  1.3209628\n",
      "Iteration # 7835  loss is :  1.3209627\n",
      "Iteration # 7836  loss is :  1.3209627\n",
      "Iteration # 7837  loss is :  1.3209625\n",
      "Iteration # 7838  loss is :  1.3209624\n",
      "Iteration # 7839  loss is :  1.3209624\n",
      "Iteration # 7840  loss is :  1.3209623\n",
      "Iteration # 7841  loss is :  1.3209622\n",
      "Iteration # 7842  loss is :  1.3209621\n",
      "Iteration # 7843  loss is :  1.320962\n",
      "Iteration # 7844  loss is :  1.320962\n",
      "Iteration # 7845  loss is :  1.3209618\n",
      "Iteration # 7846  loss is :  1.3209618\n",
      "Iteration # 7847  loss is :  1.3209617\n",
      "Iteration # 7848  loss is :  1.3209616\n",
      "Iteration # 7849  loss is :  1.3209615\n",
      "Iteration # 7850  loss is :  1.3209615\n",
      "Iteration # 7851  loss is :  1.3209612\n",
      "Iteration # 7852  loss is :  1.3209612\n",
      "Iteration # 7853  loss is :  1.3209611\n",
      "Iteration # 7854  loss is :  1.320961\n",
      "Iteration # 7855  loss is :  1.320961\n",
      "Iteration # 7856  loss is :  1.320961\n",
      "Iteration # 7857  loss is :  1.3209606\n",
      "Iteration # 7858  loss is :  1.3209606\n",
      "Iteration # 7859  loss is :  1.3209606\n",
      "Iteration # 7860  loss is :  1.3209605\n",
      "Iteration # 7861  loss is :  1.3209604\n",
      "Iteration # 7862  loss is :  1.3209604\n",
      "Iteration # 7863  loss is :  1.3209602\n",
      "Iteration # 7864  loss is :  1.32096\n",
      "Iteration # 7865  loss is :  1.32096\n",
      "Iteration # 7866  loss is :  1.32096\n",
      "Iteration # 7867  loss is :  1.3209599\n",
      "Iteration # 7868  loss is :  1.3209599\n",
      "Iteration # 7869  loss is :  1.3209598\n",
      "Iteration # 7870  loss is :  1.3209597\n",
      "Iteration # 7871  loss is :  1.3209597\n",
      "Iteration # 7872  loss is :  1.3209593\n",
      "Iteration # 7873  loss is :  1.3209593\n",
      "Iteration # 7874  loss is :  1.3209593\n",
      "Iteration # 7875  loss is :  1.3209593\n",
      "Iteration # 7876  loss is :  1.3209591\n",
      "Iteration # 7877  loss is :  1.3209591\n",
      "Iteration # 7878  loss is :  1.320959\n",
      "Iteration # 7879  loss is :  1.3209589\n",
      "Iteration # 7880  loss is :  1.3209587\n",
      "Iteration # 7881  loss is :  1.3209586\n",
      "Iteration # 7882  loss is :  1.3209586\n",
      "Iteration # 7883  loss is :  1.3209585\n",
      "Iteration # 7884  loss is :  1.3209585\n",
      "Iteration # 7885  loss is :  1.3209583\n",
      "Iteration # 7886  loss is :  1.3209583\n",
      "Iteration # 7887  loss is :  1.3209581\n",
      "Iteration # 7888  loss is :  1.320958\n",
      "Iteration # 7889  loss is :  1.320958\n",
      "Iteration # 7890  loss is :  1.3209579\n",
      "Iteration # 7891  loss is :  1.3209579\n",
      "Iteration # 7892  loss is :  1.3209578\n",
      "Iteration # 7893  loss is :  1.3209578\n",
      "Iteration # 7894  loss is :  1.3209577\n",
      "Iteration # 7895  loss is :  1.3209575\n",
      "Iteration # 7896  loss is :  1.3209573\n",
      "Iteration # 7897  loss is :  1.3209573\n",
      "Iteration # 7898  loss is :  1.3209572\n",
      "Iteration # 7899  loss is :  1.3209572\n",
      "Iteration # 7900  loss is :  1.3209572\n",
      "Iteration # 7901  loss is :  1.320957\n",
      "Iteration # 7902  loss is :  1.3209568\n",
      "Iteration # 7903  loss is :  1.3209567\n",
      "Iteration # 7904  loss is :  1.3209566\n",
      "Iteration # 7905  loss is :  1.3209566\n",
      "Iteration # 7906  loss is :  1.3209565\n",
      "Iteration # 7907  loss is :  1.3209563\n",
      "Iteration # 7908  loss is :  1.3209563\n",
      "Iteration # 7909  loss is :  1.3209562\n",
      "Iteration # 7910  loss is :  1.3209562\n",
      "Iteration # 7911  loss is :  1.3209561\n",
      "Iteration # 7912  loss is :  1.3209561\n",
      "Iteration # 7913  loss is :  1.3209559\n",
      "Iteration # 7914  loss is :  1.3209559\n",
      "Iteration # 7915  loss is :  1.3209559\n",
      "Iteration # 7916  loss is :  1.3209556\n",
      "Iteration # 7917  loss is :  1.3209556\n",
      "Iteration # 7918  loss is :  1.3209555\n",
      "Iteration # 7919  loss is :  1.3209554\n",
      "Iteration # 7920  loss is :  1.3209553\n",
      "Iteration # 7921  loss is :  1.3209552\n",
      "Iteration # 7922  loss is :  1.3209552\n",
      "Iteration # 7923  loss is :  1.320955\n",
      "Iteration # 7924  loss is :  1.320955\n",
      "Iteration # 7925  loss is :  1.3209549\n",
      "Iteration # 7926  loss is :  1.3209548\n",
      "Iteration # 7927  loss is :  1.3209547\n",
      "Iteration # 7928  loss is :  1.3209546\n",
      "Iteration # 7929  loss is :  1.3209546\n",
      "Iteration # 7930  loss is :  1.3209543\n",
      "Iteration # 7931  loss is :  1.3209544\n",
      "Iteration # 7932  loss is :  1.3209543\n",
      "Iteration # 7933  loss is :  1.3209543\n",
      "Iteration # 7934  loss is :  1.3209542\n",
      "Iteration # 7935  loss is :  1.320954\n",
      "Iteration # 7936  loss is :  1.320954\n",
      "Iteration # 7937  loss is :  1.3209538\n",
      "Iteration # 7938  loss is :  1.3209537\n",
      "Iteration # 7939  loss is :  1.3209537\n",
      "Iteration # 7940  loss is :  1.3209536\n",
      "Iteration # 7941  loss is :  1.3209535\n",
      "Iteration # 7942  loss is :  1.3209535\n",
      "Iteration # 7943  loss is :  1.3209534\n",
      "Iteration # 7944  loss is :  1.3209531\n",
      "Iteration # 7945  loss is :  1.3209531\n",
      "Iteration # 7946  loss is :  1.3209531\n",
      "Iteration # 7947  loss is :  1.3209529\n",
      "Iteration # 7948  loss is :  1.3209529\n",
      "Iteration # 7949  loss is :  1.3209528\n",
      "Iteration # 7950  loss is :  1.3209528\n",
      "Iteration # 7951  loss is :  1.3209525\n",
      "Iteration # 7952  loss is :  1.3209525\n",
      "Iteration # 7953  loss is :  1.3209524\n",
      "Iteration # 7954  loss is :  1.3209524\n",
      "Iteration # 7955  loss is :  1.3209524\n",
      "Iteration # 7956  loss is :  1.3209523\n",
      "Iteration # 7957  loss is :  1.3209522\n",
      "Iteration # 7958  loss is :  1.3209522\n",
      "Iteration # 7959  loss is :  1.320952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 7960  loss is :  1.3209518\n",
      "Iteration # 7961  loss is :  1.3209518\n",
      "Iteration # 7962  loss is :  1.3209517\n",
      "Iteration # 7963  loss is :  1.3209517\n",
      "Iteration # 7964  loss is :  1.3209516\n",
      "Iteration # 7965  loss is :  1.3209516\n",
      "Iteration # 7966  loss is :  1.3209513\n",
      "Iteration # 7967  loss is :  1.3209512\n",
      "Iteration # 7968  loss is :  1.3209512\n",
      "Iteration # 7969  loss is :  1.320951\n",
      "Iteration # 7970  loss is :  1.320951\n",
      "Iteration # 7971  loss is :  1.320951\n",
      "Iteration # 7972  loss is :  1.3209509\n",
      "Iteration # 7973  loss is :  1.3209507\n",
      "Iteration # 7974  loss is :  1.3209507\n",
      "Iteration # 7975  loss is :  1.3209505\n",
      "Iteration # 7976  loss is :  1.3209505\n",
      "Iteration # 7977  loss is :  1.3209505\n",
      "Iteration # 7978  loss is :  1.3209504\n",
      "Iteration # 7979  loss is :  1.3209503\n",
      "Iteration # 7980  loss is :  1.3209503\n",
      "Iteration # 7981  loss is :  1.3209502\n",
      "Iteration # 7982  loss is :  1.32095\n",
      "Iteration # 7983  loss is :  1.3209499\n",
      "Iteration # 7984  loss is :  1.3209499\n",
      "Iteration # 7985  loss is :  1.3209498\n",
      "Iteration # 7986  loss is :  1.3209497\n",
      "Iteration # 7987  loss is :  1.3209497\n",
      "Iteration # 7988  loss is :  1.3209494\n",
      "Iteration # 7989  loss is :  1.3209494\n",
      "Iteration # 7990  loss is :  1.3209493\n",
      "Iteration # 7991  loss is :  1.3209493\n",
      "Iteration # 7992  loss is :  1.3209492\n",
      "Iteration # 7993  loss is :  1.3209491\n",
      "Iteration # 7994  loss is :  1.3209491\n",
      "Iteration # 7995  loss is :  1.3209488\n",
      "Iteration # 7996  loss is :  1.3209488\n",
      "Iteration # 7997  loss is :  1.3209488\n",
      "Iteration # 7998  loss is :  1.3209486\n",
      "Iteration # 7999  loss is :  1.3209486\n",
      "Iteration # 8000  loss is :  1.3209485\n",
      "Iteration # 8001  loss is :  1.3209485\n",
      "Iteration # 8002  loss is :  1.3209484\n",
      "Iteration # 8003  loss is :  1.3209484\n",
      "Iteration # 8004  loss is :  1.3209482\n",
      "Iteration # 8005  loss is :  1.3209481\n",
      "Iteration # 8006  loss is :  1.3209481\n",
      "Iteration # 8007  loss is :  1.320948\n",
      "Iteration # 8008  loss is :  1.3209478\n",
      "Iteration # 8009  loss is :  1.3209478\n",
      "Iteration # 8010  loss is :  1.3209478\n",
      "Iteration # 8011  loss is :  1.3209475\n",
      "Iteration # 8012  loss is :  1.3209475\n",
      "Iteration # 8013  loss is :  1.3209474\n",
      "Iteration # 8014  loss is :  1.3209473\n",
      "Iteration # 8015  loss is :  1.3209473\n",
      "Iteration # 8016  loss is :  1.3209472\n",
      "Iteration # 8017  loss is :  1.3209472\n",
      "Iteration # 8018  loss is :  1.3209469\n",
      "Iteration # 8019  loss is :  1.3209469\n",
      "Iteration # 8020  loss is :  1.3209468\n",
      "Iteration # 8021  loss is :  1.3209467\n",
      "Iteration # 8022  loss is :  1.3209467\n",
      "Iteration # 8023  loss is :  1.3209467\n",
      "Iteration # 8024  loss is :  1.3209465\n",
      "Iteration # 8025  loss is :  1.3209465\n",
      "Iteration # 8026  loss is :  1.3209463\n",
      "Iteration # 8027  loss is :  1.3209462\n",
      "Iteration # 8028  loss is :  1.3209462\n",
      "Iteration # 8029  loss is :  1.3209462\n",
      "Iteration # 8030  loss is :  1.320946\n",
      "Iteration # 8031  loss is :  1.320946\n",
      "Iteration # 8032  loss is :  1.3209457\n",
      "Iteration # 8033  loss is :  1.3209457\n",
      "Iteration # 8034  loss is :  1.3209456\n",
      "Iteration # 8035  loss is :  1.3209456\n",
      "Iteration # 8036  loss is :  1.3209455\n",
      "Iteration # 8037  loss is :  1.3209454\n",
      "Iteration # 8038  loss is :  1.3209454\n",
      "Iteration # 8039  loss is :  1.3209453\n",
      "Iteration # 8040  loss is :  1.3209451\n",
      "Iteration # 8041  loss is :  1.320945\n",
      "Iteration # 8042  loss is :  1.320945\n",
      "Iteration # 8043  loss is :  1.3209449\n",
      "Iteration # 8044  loss is :  1.3209448\n",
      "Iteration # 8045  loss is :  1.3209448\n",
      "Iteration # 8046  loss is :  1.3209448\n",
      "Iteration # 8047  loss is :  1.3209447\n",
      "Iteration # 8048  loss is :  1.3209445\n",
      "Iteration # 8049  loss is :  1.3209443\n",
      "Iteration # 8050  loss is :  1.3209443\n",
      "Iteration # 8051  loss is :  1.3209443\n",
      "Iteration # 8052  loss is :  1.3209441\n",
      "Iteration # 8053  loss is :  1.3209441\n",
      "Iteration # 8054  loss is :  1.3209441\n",
      "Iteration # 8055  loss is :  1.3209441\n",
      "Iteration # 8056  loss is :  1.3209437\n",
      "Iteration # 8057  loss is :  1.3209437\n",
      "Iteration # 8058  loss is :  1.3209436\n",
      "Iteration # 8059  loss is :  1.3209436\n",
      "Iteration # 8060  loss is :  1.3209435\n",
      "Iteration # 8061  loss is :  1.3209434\n",
      "Iteration # 8062  loss is :  1.3209432\n",
      "Iteration # 8063  loss is :  1.3209432\n",
      "Iteration # 8064  loss is :  1.3209432\n",
      "Iteration # 8065  loss is :  1.320943\n",
      "Iteration # 8066  loss is :  1.3209429\n",
      "Iteration # 8067  loss is :  1.3209429\n",
      "Iteration # 8068  loss is :  1.3209429\n",
      "Iteration # 8069  loss is :  1.3209428\n",
      "Iteration # 8070  loss is :  1.3209428\n",
      "Iteration # 8071  loss is :  1.3209426\n",
      "Iteration # 8072  loss is :  1.3209426\n",
      "Iteration # 8073  loss is :  1.3209424\n",
      "Iteration # 8074  loss is :  1.3209423\n",
      "Iteration # 8075  loss is :  1.3209424\n",
      "Iteration # 8076  loss is :  1.3209422\n",
      "Iteration # 8077  loss is :  1.3209422\n",
      "Iteration # 8078  loss is :  1.320942\n",
      "Iteration # 8079  loss is :  1.3209419\n",
      "Iteration # 8080  loss is :  1.3209419\n",
      "Iteration # 8081  loss is :  1.3209417\n",
      "Iteration # 8082  loss is :  1.3209416\n",
      "Iteration # 8083  loss is :  1.3209416\n",
      "Iteration # 8084  loss is :  1.3209416\n",
      "Iteration # 8085  loss is :  1.3209414\n",
      "Iteration # 8086  loss is :  1.3209413\n",
      "Iteration # 8087  loss is :  1.3209413\n",
      "Iteration # 8088  loss is :  1.3209413\n",
      "Iteration # 8089  loss is :  1.3209412\n",
      "Iteration # 8090  loss is :  1.320941\n",
      "Iteration # 8091  loss is :  1.320941\n",
      "Iteration # 8092  loss is :  1.3209409\n",
      "Iteration # 8093  loss is :  1.3209409\n",
      "Iteration # 8094  loss is :  1.3209409\n",
      "Iteration # 8095  loss is :  1.3209409\n",
      "Iteration # 8096  loss is :  1.3209406\n",
      "Iteration # 8097  loss is :  1.3209406\n",
      "Iteration # 8098  loss is :  1.3209404\n",
      "Iteration # 8099  loss is :  1.3209403\n",
      "Iteration # 8100  loss is :  1.3209403\n",
      "Iteration # 8101  loss is :  1.3209401\n",
      "Iteration # 8102  loss is :  1.32094\n",
      "Iteration # 8103  loss is :  1.32094\n",
      "Iteration # 8104  loss is :  1.32094\n",
      "Iteration # 8105  loss is :  1.3209398\n",
      "Iteration # 8106  loss is :  1.3209397\n",
      "Iteration # 8107  loss is :  1.3209397\n",
      "Iteration # 8108  loss is :  1.3209395\n",
      "Iteration # 8109  loss is :  1.3209394\n",
      "Iteration # 8110  loss is :  1.3209394\n",
      "Iteration # 8111  loss is :  1.3209393\n",
      "Iteration # 8112  loss is :  1.3209392\n",
      "Iteration # 8113  loss is :  1.3209392\n",
      "Iteration # 8114  loss is :  1.3209391\n",
      "Iteration # 8115  loss is :  1.320939\n",
      "Iteration # 8116  loss is :  1.320939\n",
      "Iteration # 8117  loss is :  1.320939\n",
      "Iteration # 8118  loss is :  1.3209387\n",
      "Iteration # 8119  loss is :  1.3209387\n",
      "Iteration # 8120  loss is :  1.3209387\n",
      "Iteration # 8121  loss is :  1.3209385\n",
      "Iteration # 8122  loss is :  1.3209385\n",
      "Iteration # 8123  loss is :  1.3209382\n",
      "Iteration # 8124  loss is :  1.3209381\n",
      "Iteration # 8125  loss is :  1.3209381\n",
      "Iteration # 8126  loss is :  1.3209381\n",
      "Iteration # 8127  loss is :  1.3209381\n",
      "Iteration # 8128  loss is :  1.3209379\n",
      "Iteration # 8129  loss is :  1.3209379\n",
      "Iteration # 8130  loss is :  1.3209378\n",
      "Iteration # 8131  loss is :  1.3209376\n",
      "Iteration # 8132  loss is :  1.3209375\n",
      "Iteration # 8133  loss is :  1.3209374\n",
      "Iteration # 8134  loss is :  1.3209374\n",
      "Iteration # 8135  loss is :  1.3209373\n",
      "Iteration # 8136  loss is :  1.3209373\n",
      "Iteration # 8137  loss is :  1.3209373\n",
      "Iteration # 8138  loss is :  1.3209372\n",
      "Iteration # 8139  loss is :  1.3209372\n",
      "Iteration # 8140  loss is :  1.320937\n",
      "Iteration # 8141  loss is :  1.3209369\n",
      "Iteration # 8142  loss is :  1.3209368\n",
      "Iteration # 8143  loss is :  1.3209368\n",
      "Iteration # 8144  loss is :  1.3209366\n",
      "Iteration # 8145  loss is :  1.3209366\n",
      "Iteration # 8146  loss is :  1.3209366\n",
      "Iteration # 8147  loss is :  1.3209364\n",
      "Iteration # 8148  loss is :  1.3209362\n",
      "Iteration # 8149  loss is :  1.3209362\n",
      "Iteration # 8150  loss is :  1.3209362\n",
      "Iteration # 8151  loss is :  1.3209361\n",
      "Iteration # 8152  loss is :  1.320936\n",
      "Iteration # 8153  loss is :  1.3209358\n",
      "Iteration # 8154  loss is :  1.3209357\n",
      "Iteration # 8155  loss is :  1.3209358\n",
      "Iteration # 8156  loss is :  1.3209357\n",
      "Iteration # 8157  loss is :  1.3209355\n",
      "Iteration # 8158  loss is :  1.3209354\n",
      "Iteration # 8159  loss is :  1.3209354\n",
      "Iteration # 8160  loss is :  1.3209354\n",
      "Iteration # 8161  loss is :  1.3209352\n",
      "Iteration # 8162  loss is :  1.3209352\n",
      "Iteration # 8163  loss is :  1.3209352\n",
      "Iteration # 8164  loss is :  1.320935\n",
      "Iteration # 8165  loss is :  1.320935\n",
      "Iteration # 8166  loss is :  1.3209349\n",
      "Iteration # 8167  loss is :  1.3209349\n",
      "Iteration # 8168  loss is :  1.3209348\n",
      "Iteration # 8169  loss is :  1.3209347\n",
      "Iteration # 8170  loss is :  1.3209347\n",
      "Iteration # 8171  loss is :  1.3209345\n",
      "Iteration # 8172  loss is :  1.3209344\n",
      "Iteration # 8173  loss is :  1.3209343\n",
      "Iteration # 8174  loss is :  1.3209342\n",
      "Iteration # 8175  loss is :  1.320934\n",
      "Iteration # 8176  loss is :  1.320934\n",
      "Iteration # 8177  loss is :  1.320934\n",
      "Iteration # 8178  loss is :  1.3209338\n",
      "Iteration # 8179  loss is :  1.3209338\n",
      "Iteration # 8180  loss is :  1.3209337\n",
      "Iteration # 8181  loss is :  1.3209336\n",
      "Iteration # 8182  loss is :  1.3209336\n",
      "Iteration # 8183  loss is :  1.3209335\n",
      "Iteration # 8184  loss is :  1.3209335\n",
      "Iteration # 8185  loss is :  1.3209333\n",
      "Iteration # 8186  loss is :  1.3209333\n",
      "Iteration # 8187  loss is :  1.3209332\n",
      "Iteration # 8188  loss is :  1.3209331\n",
      "Iteration # 8189  loss is :  1.3209331\n",
      "Iteration # 8190  loss is :  1.3209331\n",
      "Iteration # 8191  loss is :  1.320933\n",
      "Iteration # 8192  loss is :  1.3209327\n",
      "Iteration # 8193  loss is :  1.3209327\n",
      "Iteration # 8194  loss is :  1.3209327\n",
      "Iteration # 8195  loss is :  1.3209325\n",
      "Iteration # 8196  loss is :  1.3209325\n",
      "Iteration # 8197  loss is :  1.3209324\n",
      "Iteration # 8198  loss is :  1.3209324\n",
      "Iteration # 8199  loss is :  1.3209323\n",
      "Iteration # 8200  loss is :  1.3209321\n",
      "Iteration # 8201  loss is :  1.320932\n",
      "Iteration # 8202  loss is :  1.3209319\n",
      "Iteration # 8203  loss is :  1.3209319\n",
      "Iteration # 8204  loss is :  1.3209319\n",
      "Iteration # 8205  loss is :  1.3209317\n",
      "Iteration # 8206  loss is :  1.3209317\n",
      "Iteration # 8207  loss is :  1.3209317\n",
      "Iteration # 8208  loss is :  1.3209316\n",
      "Iteration # 8209  loss is :  1.3209314\n",
      "Iteration # 8210  loss is :  1.3209314\n",
      "Iteration # 8211  loss is :  1.3209313\n",
      "Iteration # 8212  loss is :  1.3209312\n",
      "Iteration # 8213  loss is :  1.3209312\n",
      "Iteration # 8214  loss is :  1.3209312\n",
      "Iteration # 8215  loss is :  1.320931\n",
      "Iteration # 8216  loss is :  1.3209311\n",
      "Iteration # 8217  loss is :  1.3209308\n",
      "Iteration # 8218  loss is :  1.3209307\n",
      "Iteration # 8219  loss is :  1.3209307\n",
      "Iteration # 8220  loss is :  1.3209306\n",
      "Iteration # 8221  loss is :  1.3209306\n",
      "Iteration # 8222  loss is :  1.3209305\n",
      "Iteration # 8223  loss is :  1.3209305\n",
      "Iteration # 8224  loss is :  1.3209304\n",
      "Iteration # 8225  loss is :  1.3209302\n",
      "Iteration # 8226  loss is :  1.3209301\n",
      "Iteration # 8227  loss is :  1.32093\n",
      "Iteration # 8228  loss is :  1.32093\n",
      "Iteration # 8229  loss is :  1.3209299\n",
      "Iteration # 8230  loss is :  1.3209298\n",
      "Iteration # 8231  loss is :  1.3209298\n",
      "Iteration # 8232  loss is :  1.3209298\n",
      "Iteration # 8233  loss is :  1.3209295\n",
      "Iteration # 8234  loss is :  1.3209295\n",
      "Iteration # 8235  loss is :  1.3209294\n",
      "Iteration # 8236  loss is :  1.3209294\n",
      "Iteration # 8237  loss is :  1.3209293\n",
      "Iteration # 8238  loss is :  1.3209292\n",
      "Iteration # 8239  loss is :  1.3209292\n",
      "Iteration # 8240  loss is :  1.320929\n",
      "Iteration # 8241  loss is :  1.320929\n",
      "Iteration # 8242  loss is :  1.3209287\n",
      "Iteration # 8243  loss is :  1.3209287\n",
      "Iteration # 8244  loss is :  1.3209287\n",
      "Iteration # 8245  loss is :  1.3209286\n",
      "Iteration # 8246  loss is :  1.3209285\n",
      "Iteration # 8247  loss is :  1.3209285\n",
      "Iteration # 8248  loss is :  1.3209285\n",
      "Iteration # 8249  loss is :  1.3209283\n",
      "Iteration # 8250  loss is :  1.3209282\n",
      "Iteration # 8251  loss is :  1.3209281\n",
      "Iteration # 8252  loss is :  1.320928\n",
      "Iteration # 8253  loss is :  1.3209279\n",
      "Iteration # 8254  loss is :  1.3209279\n",
      "Iteration # 8255  loss is :  1.3209279\n",
      "Iteration # 8256  loss is :  1.3209277\n",
      "Iteration # 8257  loss is :  1.3209276\n",
      "Iteration # 8258  loss is :  1.3209276\n",
      "Iteration # 8259  loss is :  1.3209276\n",
      "Iteration # 8260  loss is :  1.3209274\n",
      "Iteration # 8261  loss is :  1.3209274\n",
      "Iteration # 8262  loss is :  1.3209274\n",
      "Iteration # 8263  loss is :  1.3209271\n",
      "Iteration # 8264  loss is :  1.3209271\n",
      "Iteration # 8265  loss is :  1.3209271\n",
      "Iteration # 8266  loss is :  1.3209269\n",
      "Iteration # 8267  loss is :  1.3209269\n",
      "Iteration # 8268  loss is :  1.3209269\n",
      "Iteration # 8269  loss is :  1.3209268\n",
      "Iteration # 8270  loss is :  1.3209265\n",
      "Iteration # 8271  loss is :  1.3209265\n",
      "Iteration # 8272  loss is :  1.3209265\n",
      "Iteration # 8273  loss is :  1.3209265\n",
      "Iteration # 8274  loss is :  1.3209264\n",
      "Iteration # 8275  loss is :  1.3209262\n",
      "Iteration # 8276  loss is :  1.3209263\n",
      "Iteration # 8277  loss is :  1.320926\n",
      "Iteration # 8278  loss is :  1.320926\n",
      "Iteration # 8279  loss is :  1.320926\n",
      "Iteration # 8280  loss is :  1.3209257\n",
      "Iteration # 8281  loss is :  1.3209258\n",
      "Iteration # 8282  loss is :  1.3209257\n",
      "Iteration # 8283  loss is :  1.3209257\n",
      "Iteration # 8284  loss is :  1.3209256\n",
      "Iteration # 8285  loss is :  1.3209256\n",
      "Iteration # 8286  loss is :  1.3209254\n",
      "Iteration # 8287  loss is :  1.3209254\n",
      "Iteration # 8288  loss is :  1.3209252\n",
      "Iteration # 8289  loss is :  1.3209252\n",
      "Iteration # 8290  loss is :  1.320925\n",
      "Iteration # 8291  loss is :  1.320925\n",
      "Iteration # 8292  loss is :  1.320925\n",
      "Iteration # 8293  loss is :  1.320925\n",
      "Iteration # 8294  loss is :  1.3209249\n",
      "Iteration # 8295  loss is :  1.3209246\n",
      "Iteration # 8296  loss is :  1.3209246\n",
      "Iteration # 8297  loss is :  1.3209245\n",
      "Iteration # 8298  loss is :  1.3209244\n",
      "Iteration # 8299  loss is :  1.3209244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 8300  loss is :  1.3209244\n",
      "Iteration # 8301  loss is :  1.3209242\n",
      "Iteration # 8302  loss is :  1.3209242\n",
      "Iteration # 8303  loss is :  1.3209242\n",
      "Iteration # 8304  loss is :  1.3209239\n",
      "Iteration # 8305  loss is :  1.3209238\n",
      "Iteration # 8306  loss is :  1.3209238\n",
      "Iteration # 8307  loss is :  1.3209238\n",
      "Iteration # 8308  loss is :  1.3209238\n",
      "Iteration # 8309  loss is :  1.3209238\n",
      "Iteration # 8310  loss is :  1.3209237\n",
      "Iteration # 8311  loss is :  1.3209234\n",
      "Iteration # 8312  loss is :  1.3209233\n",
      "Iteration # 8313  loss is :  1.3209233\n",
      "Iteration # 8314  loss is :  1.3209232\n",
      "Iteration # 8315  loss is :  1.3209231\n",
      "Iteration # 8316  loss is :  1.3209231\n",
      "Iteration # 8317  loss is :  1.320923\n",
      "Iteration # 8318  loss is :  1.320923\n",
      "Iteration # 8319  loss is :  1.320923\n",
      "Iteration # 8320  loss is :  1.3209229\n",
      "Iteration # 8321  loss is :  1.3209229\n",
      "Iteration # 8322  loss is :  1.3209226\n",
      "Iteration # 8323  loss is :  1.3209226\n",
      "Iteration # 8324  loss is :  1.3209223\n",
      "Iteration # 8325  loss is :  1.3209224\n",
      "Iteration # 8326  loss is :  1.3209224\n",
      "Iteration # 8327  loss is :  1.3209223\n",
      "Iteration # 8328  loss is :  1.3209221\n",
      "Iteration # 8329  loss is :  1.320922\n",
      "Iteration # 8330  loss is :  1.3209219\n",
      "Iteration # 8331  loss is :  1.3209219\n",
      "Iteration # 8332  loss is :  1.3209219\n",
      "Iteration # 8333  loss is :  1.3209218\n",
      "Iteration # 8334  loss is :  1.3209218\n",
      "Iteration # 8335  loss is :  1.3209218\n",
      "Iteration # 8336  loss is :  1.3209217\n",
      "Iteration # 8337  loss is :  1.3209215\n",
      "Iteration # 8338  loss is :  1.3209215\n",
      "Iteration # 8339  loss is :  1.3209213\n",
      "Iteration # 8340  loss is :  1.3209213\n",
      "Iteration # 8341  loss is :  1.3209212\n",
      "Iteration # 8342  loss is :  1.3209212\n",
      "Iteration # 8343  loss is :  1.3209211\n",
      "Iteration # 8344  loss is :  1.320921\n",
      "Iteration # 8345  loss is :  1.320921\n",
      "Iteration # 8346  loss is :  1.320921\n",
      "Iteration # 8347  loss is :  1.3209207\n",
      "Iteration # 8348  loss is :  1.3209206\n",
      "Iteration # 8349  loss is :  1.3209203\n",
      "Iteration # 8350  loss is :  1.3209205\n",
      "Iteration # 8351  loss is :  1.3209203\n",
      "Iteration # 8352  loss is :  1.3209202\n",
      "Iteration # 8353  loss is :  1.3209202\n",
      "Iteration # 8354  loss is :  1.3209201\n",
      "Iteration # 8355  loss is :  1.3209201\n",
      "Iteration # 8356  loss is :  1.32092\n",
      "Iteration # 8357  loss is :  1.32092\n",
      "Iteration # 8358  loss is :  1.3209199\n",
      "Iteration # 8359  loss is :  1.3209199\n",
      "Iteration # 8360  loss is :  1.3209199\n",
      "Iteration # 8361  loss is :  1.3209196\n",
      "Iteration # 8362  loss is :  1.3209196\n",
      "Iteration # 8363  loss is :  1.3209195\n",
      "Iteration # 8364  loss is :  1.3209194\n",
      "Iteration # 8365  loss is :  1.3209194\n",
      "Iteration # 8366  loss is :  1.3209193\n",
      "Iteration # 8367  loss is :  1.3209192\n",
      "Iteration # 8368  loss is :  1.320919\n",
      "Iteration # 8369  loss is :  1.320919\n",
      "Iteration # 8370  loss is :  1.320919\n",
      "Iteration # 8371  loss is :  1.3209189\n",
      "Iteration # 8372  loss is :  1.3209189\n",
      "Iteration # 8373  loss is :  1.3209188\n",
      "Iteration # 8374  loss is :  1.3209188\n",
      "Iteration # 8375  loss is :  1.3209187\n",
      "Iteration # 8376  loss is :  1.3209184\n",
      "Iteration # 8377  loss is :  1.3209184\n",
      "Iteration # 8378  loss is :  1.3209183\n",
      "Iteration # 8379  loss is :  1.3209182\n",
      "Iteration # 8380  loss is :  1.3209182\n",
      "Iteration # 8381  loss is :  1.3209182\n",
      "Iteration # 8382  loss is :  1.3209181\n",
      "Iteration # 8383  loss is :  1.3209181\n",
      "Iteration # 8384  loss is :  1.320918\n",
      "Iteration # 8385  loss is :  1.320918\n",
      "Iteration # 8386  loss is :  1.3209178\n",
      "Iteration # 8387  loss is :  1.3209177\n",
      "Iteration # 8388  loss is :  1.3209177\n",
      "Iteration # 8389  loss is :  1.3209176\n",
      "Iteration # 8390  loss is :  1.3209176\n",
      "Iteration # 8391  loss is :  1.3209175\n",
      "Iteration # 8392  loss is :  1.3209174\n",
      "Iteration # 8393  loss is :  1.3209171\n",
      "Iteration # 8394  loss is :  1.3209171\n",
      "Iteration # 8395  loss is :  1.3209171\n",
      "Iteration # 8396  loss is :  1.3209169\n",
      "Iteration # 8397  loss is :  1.3209169\n",
      "Iteration # 8398  loss is :  1.3209169\n",
      "Iteration # 8399  loss is :  1.3209167\n",
      "Iteration # 8400  loss is :  1.3209167\n",
      "Iteration # 8401  loss is :  1.3209167\n",
      "Iteration # 8402  loss is :  1.3209165\n",
      "Iteration # 8403  loss is :  1.3209163\n",
      "Iteration # 8404  loss is :  1.3209163\n",
      "Iteration # 8405  loss is :  1.3209163\n",
      "Iteration # 8406  loss is :  1.3209162\n",
      "Iteration # 8407  loss is :  1.3209162\n",
      "Iteration # 8408  loss is :  1.3209162\n",
      "Iteration # 8409  loss is :  1.3209162\n",
      "Iteration # 8410  loss is :  1.3209162\n",
      "Iteration # 8411  loss is :  1.3209158\n",
      "Iteration # 8412  loss is :  1.3209158\n",
      "Iteration # 8413  loss is :  1.3209158\n",
      "Iteration # 8414  loss is :  1.3209156\n",
      "Iteration # 8415  loss is :  1.3209156\n",
      "Iteration # 8416  loss is :  1.3209156\n",
      "Iteration # 8417  loss is :  1.3209156\n",
      "Iteration # 8418  loss is :  1.3209155\n",
      "Iteration # 8419  loss is :  1.3209153\n",
      "Iteration # 8420  loss is :  1.3209152\n",
      "Iteration # 8421  loss is :  1.3209151\n",
      "Iteration # 8422  loss is :  1.320915\n",
      "Iteration # 8423  loss is :  1.320915\n",
      "Iteration # 8424  loss is :  1.3209149\n",
      "Iteration # 8425  loss is :  1.3209147\n",
      "Iteration # 8426  loss is :  1.3209147\n",
      "Iteration # 8427  loss is :  1.3209147\n",
      "Iteration # 8428  loss is :  1.3209145\n",
      "Iteration # 8429  loss is :  1.3209145\n",
      "Iteration # 8430  loss is :  1.3209144\n",
      "Iteration # 8431  loss is :  1.3209144\n",
      "Iteration # 8432  loss is :  1.3209143\n",
      "Iteration # 8433  loss is :  1.3209143\n",
      "Iteration # 8434  loss is :  1.3209143\n",
      "Iteration # 8435  loss is :  1.3209141\n",
      "Iteration # 8436  loss is :  1.320914\n",
      "Iteration # 8437  loss is :  1.320914\n",
      "Iteration # 8438  loss is :  1.3209139\n",
      "Iteration # 8439  loss is :  1.3209137\n",
      "Iteration # 8440  loss is :  1.3209137\n",
      "Iteration # 8441  loss is :  1.3209136\n",
      "Iteration # 8442  loss is :  1.3209136\n",
      "Iteration # 8443  loss is :  1.3209134\n",
      "Iteration # 8444  loss is :  1.3209133\n",
      "Iteration # 8445  loss is :  1.3209134\n",
      "Iteration # 8446  loss is :  1.3209132\n",
      "Iteration # 8447  loss is :  1.3209132\n",
      "Iteration # 8448  loss is :  1.3209131\n",
      "Iteration # 8449  loss is :  1.3209131\n",
      "Iteration # 8450  loss is :  1.320913\n",
      "Iteration # 8451  loss is :  1.3209128\n",
      "Iteration # 8452  loss is :  1.3209128\n",
      "Iteration # 8453  loss is :  1.3209128\n",
      "Iteration # 8454  loss is :  1.3209126\n",
      "Iteration # 8455  loss is :  1.3209127\n",
      "Iteration # 8456  loss is :  1.3209126\n",
      "Iteration # 8457  loss is :  1.3209124\n",
      "Iteration # 8458  loss is :  1.3209124\n",
      "Iteration # 8459  loss is :  1.3209124\n",
      "Iteration # 8460  loss is :  1.3209124\n",
      "Iteration # 8461  loss is :  1.3209121\n",
      "Iteration # 8462  loss is :  1.3209122\n",
      "Iteration # 8463  loss is :  1.3209121\n",
      "Iteration # 8464  loss is :  1.320912\n",
      "Iteration # 8465  loss is :  1.3209119\n",
      "Iteration # 8466  loss is :  1.3209118\n",
      "Iteration # 8467  loss is :  1.3209118\n",
      "Iteration # 8468  loss is :  1.3209118\n",
      "Iteration # 8469  loss is :  1.3209115\n",
      "Iteration # 8470  loss is :  1.3209115\n",
      "Iteration # 8471  loss is :  1.3209114\n",
      "Iteration # 8472  loss is :  1.3209114\n",
      "Iteration # 8473  loss is :  1.3209113\n",
      "Iteration # 8474  loss is :  1.3209113\n",
      "Iteration # 8475  loss is :  1.3209112\n",
      "Iteration # 8476  loss is :  1.3209109\n",
      "Iteration # 8477  loss is :  1.3209109\n",
      "Iteration # 8478  loss is :  1.3209108\n",
      "Iteration # 8479  loss is :  1.3209107\n",
      "Iteration # 8480  loss is :  1.3209107\n",
      "Iteration # 8481  loss is :  1.3209107\n",
      "Iteration # 8482  loss is :  1.3209107\n",
      "Iteration # 8483  loss is :  1.3209105\n",
      "Iteration # 8484  loss is :  1.3209106\n",
      "Iteration # 8485  loss is :  1.3209105\n",
      "Iteration # 8486  loss is :  1.3209105\n",
      "Iteration # 8487  loss is :  1.3209102\n",
      "Iteration # 8488  loss is :  1.3209102\n",
      "Iteration # 8489  loss is :  1.3209102\n",
      "Iteration # 8490  loss is :  1.3209102\n",
      "Iteration # 8491  loss is :  1.32091\n",
      "Iteration # 8492  loss is :  1.32091\n",
      "Iteration # 8493  loss is :  1.32091\n",
      "Iteration # 8494  loss is :  1.3209097\n",
      "Iteration # 8495  loss is :  1.3209096\n",
      "Iteration # 8496  loss is :  1.3209096\n",
      "Iteration # 8497  loss is :  1.3209096\n",
      "Iteration # 8498  loss is :  1.3209094\n",
      "Iteration # 8499  loss is :  1.3209094\n",
      "Iteration # 8500  loss is :  1.3209094\n",
      "Iteration # 8501  loss is :  1.3209093\n",
      "Iteration # 8502  loss is :  1.3209093\n",
      "Iteration # 8503  loss is :  1.3209091\n",
      "Iteration # 8504  loss is :  1.320909\n",
      "Iteration # 8505  loss is :  1.3209089\n",
      "Iteration # 8506  loss is :  1.3209088\n",
      "Iteration # 8507  loss is :  1.3209088\n",
      "Iteration # 8508  loss is :  1.3209087\n",
      "Iteration # 8509  loss is :  1.3209087\n",
      "Iteration # 8510  loss is :  1.3209085\n",
      "Iteration # 8511  loss is :  1.3209085\n",
      "Iteration # 8512  loss is :  1.3209085\n",
      "Iteration # 8513  loss is :  1.3209083\n",
      "Iteration # 8514  loss is :  1.3209083\n",
      "Iteration # 8515  loss is :  1.3209082\n",
      "Iteration # 8516  loss is :  1.3209081\n",
      "Iteration # 8517  loss is :  1.3209081\n",
      "Iteration # 8518  loss is :  1.3209081\n",
      "Iteration # 8519  loss is :  1.3209081\n",
      "Iteration # 8520  loss is :  1.3209078\n",
      "Iteration # 8521  loss is :  1.3209078\n",
      "Iteration # 8522  loss is :  1.3209077\n",
      "Iteration # 8523  loss is :  1.3209077\n",
      "Iteration # 8524  loss is :  1.3209075\n",
      "Iteration # 8525  loss is :  1.3209075\n",
      "Iteration # 8526  loss is :  1.3209074\n",
      "Iteration # 8527  loss is :  1.3209074\n",
      "Iteration # 8528  loss is :  1.3209072\n",
      "Iteration # 8529  loss is :  1.3209072\n",
      "Iteration # 8530  loss is :  1.3209072\n",
      "Iteration # 8531  loss is :  1.320907\n",
      "Iteration # 8532  loss is :  1.3209069\n",
      "Iteration # 8533  loss is :  1.3209069\n",
      "Iteration # 8534  loss is :  1.3209068\n",
      "Iteration # 8535  loss is :  1.3209066\n",
      "Iteration # 8536  loss is :  1.3209066\n",
      "Iteration # 8537  loss is :  1.3209066\n",
      "Iteration # 8538  loss is :  1.3209066\n",
      "Iteration # 8539  loss is :  1.3209065\n",
      "Iteration # 8540  loss is :  1.3209065\n",
      "Iteration # 8541  loss is :  1.3209064\n",
      "Iteration # 8542  loss is :  1.3209062\n",
      "Iteration # 8543  loss is :  1.3209062\n",
      "Iteration # 8544  loss is :  1.3209062\n",
      "Iteration # 8545  loss is :  1.3209062\n",
      "Iteration # 8546  loss is :  1.320906\n",
      "Iteration # 8547  loss is :  1.3209059\n",
      "Iteration # 8548  loss is :  1.3209059\n",
      "Iteration # 8549  loss is :  1.3209058\n",
      "Iteration # 8550  loss is :  1.3209056\n",
      "Iteration # 8551  loss is :  1.3209056\n",
      "Iteration # 8552  loss is :  1.3209056\n",
      "Iteration # 8553  loss is :  1.3209054\n",
      "Iteration # 8554  loss is :  1.3209053\n",
      "Iteration # 8555  loss is :  1.3209053\n",
      "Iteration # 8556  loss is :  1.3209052\n",
      "Iteration # 8557  loss is :  1.3209051\n",
      "Iteration # 8558  loss is :  1.3209051\n",
      "Iteration # 8559  loss is :  1.3209051\n",
      "Iteration # 8560  loss is :  1.3209049\n",
      "Iteration # 8561  loss is :  1.3209049\n",
      "Iteration # 8562  loss is :  1.3209047\n",
      "Iteration # 8563  loss is :  1.3209047\n",
      "Iteration # 8564  loss is :  1.3209047\n",
      "Iteration # 8565  loss is :  1.3209046\n",
      "Iteration # 8566  loss is :  1.3209045\n",
      "Iteration # 8567  loss is :  1.3209045\n",
      "Iteration # 8568  loss is :  1.3209046\n",
      "Iteration # 8569  loss is :  1.3209043\n",
      "Iteration # 8570  loss is :  1.3209043\n",
      "Iteration # 8571  loss is :  1.3209041\n",
      "Iteration # 8572  loss is :  1.3209041\n",
      "Iteration # 8573  loss is :  1.320904\n",
      "Iteration # 8574  loss is :  1.320904\n",
      "Iteration # 8575  loss is :  1.320904\n",
      "Iteration # 8576  loss is :  1.3209039\n",
      "Iteration # 8577  loss is :  1.3209038\n",
      "Iteration # 8578  loss is :  1.3209038\n",
      "Iteration # 8579  loss is :  1.3209037\n",
      "Iteration # 8580  loss is :  1.3209035\n",
      "Iteration # 8581  loss is :  1.3209034\n",
      "Iteration # 8582  loss is :  1.3209033\n",
      "Iteration # 8583  loss is :  1.3209033\n",
      "Iteration # 8584  loss is :  1.3209033\n",
      "Iteration # 8585  loss is :  1.3209033\n",
      "Iteration # 8586  loss is :  1.3209031\n",
      "Iteration # 8587  loss is :  1.3209031\n",
      "Iteration # 8588  loss is :  1.3209028\n",
      "Iteration # 8589  loss is :  1.3209028\n",
      "Iteration # 8590  loss is :  1.3209028\n",
      "Iteration # 8591  loss is :  1.3209028\n",
      "Iteration # 8592  loss is :  1.3209027\n",
      "Iteration # 8593  loss is :  1.3209027\n",
      "Iteration # 8594  loss is :  1.3209027\n",
      "Iteration # 8595  loss is :  1.3209025\n",
      "Iteration # 8596  loss is :  1.3209025\n",
      "Iteration # 8597  loss is :  1.3209025\n",
      "Iteration # 8598  loss is :  1.3209022\n",
      "Iteration # 8599  loss is :  1.3209022\n",
      "Iteration # 8600  loss is :  1.3209021\n",
      "Iteration # 8601  loss is :  1.3209021\n",
      "Iteration # 8602  loss is :  1.3209021\n",
      "Iteration # 8603  loss is :  1.3209019\n",
      "Iteration # 8604  loss is :  1.3209019\n",
      "Iteration # 8605  loss is :  1.3209016\n",
      "Iteration # 8606  loss is :  1.3209018\n",
      "Iteration # 8607  loss is :  1.3209015\n",
      "Iteration # 8608  loss is :  1.3209015\n",
      "Iteration # 8609  loss is :  1.3209014\n",
      "Iteration # 8610  loss is :  1.3209014\n",
      "Iteration # 8611  loss is :  1.3209013\n",
      "Iteration # 8612  loss is :  1.3209013\n",
      "Iteration # 8613  loss is :  1.3209013\n",
      "Iteration # 8614  loss is :  1.320901\n",
      "Iteration # 8615  loss is :  1.320901\n",
      "Iteration # 8616  loss is :  1.320901\n",
      "Iteration # 8617  loss is :  1.3209009\n",
      "Iteration # 8618  loss is :  1.3209008\n",
      "Iteration # 8619  loss is :  1.3209008\n",
      "Iteration # 8620  loss is :  1.3209007\n",
      "Iteration # 8621  loss is :  1.3209006\n",
      "Iteration # 8622  loss is :  1.3209007\n",
      "Iteration # 8623  loss is :  1.3209006\n",
      "Iteration # 8624  loss is :  1.3209006\n",
      "Iteration # 8625  loss is :  1.3209004\n",
      "Iteration # 8626  loss is :  1.3209002\n",
      "Iteration # 8627  loss is :  1.3209002\n",
      "Iteration # 8628  loss is :  1.3209001\n",
      "Iteration # 8629  loss is :  1.3209\n",
      "Iteration # 8630  loss is :  1.3209\n",
      "Iteration # 8631  loss is :  1.3208998\n",
      "Iteration # 8632  loss is :  1.3208998\n",
      "Iteration # 8633  loss is :  1.3208998\n",
      "Iteration # 8634  loss is :  1.3208997\n",
      "Iteration # 8635  loss is :  1.3208997\n",
      "Iteration # 8636  loss is :  1.3208994\n",
      "Iteration # 8637  loss is :  1.3208994\n",
      "Iteration # 8638  loss is :  1.3208994\n",
      "Iteration # 8639  loss is :  1.3208994\n",
      "Iteration # 8640  loss is :  1.3208991\n",
      "Iteration # 8641  loss is :  1.3208991\n",
      "Iteration # 8642  loss is :  1.3208991\n",
      "Iteration # 8643  loss is :  1.3208991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 8644  loss is :  1.320899\n",
      "Iteration # 8645  loss is :  1.320899\n",
      "Iteration # 8646  loss is :  1.3208989\n",
      "Iteration # 8647  loss is :  1.3208988\n",
      "Iteration # 8648  loss is :  1.3208988\n",
      "Iteration # 8649  loss is :  1.3208987\n",
      "Iteration # 8650  loss is :  1.3208987\n",
      "Iteration # 8651  loss is :  1.3208985\n",
      "Iteration # 8652  loss is :  1.3208984\n",
      "Iteration # 8653  loss is :  1.3208984\n",
      "Iteration # 8654  loss is :  1.3208984\n",
      "Iteration # 8655  loss is :  1.3208982\n",
      "Iteration # 8656  loss is :  1.320898\n",
      "Iteration # 8657  loss is :  1.320898\n",
      "Iteration # 8658  loss is :  1.320898\n",
      "Iteration # 8659  loss is :  1.3208979\n",
      "Iteration # 8660  loss is :  1.3208978\n",
      "Iteration # 8661  loss is :  1.3208978\n",
      "Iteration # 8662  loss is :  1.3208977\n",
      "Iteration # 8663  loss is :  1.3208976\n",
      "Iteration # 8664  loss is :  1.3208976\n",
      "Iteration # 8665  loss is :  1.3208975\n",
      "Iteration # 8666  loss is :  1.3208975\n",
      "Iteration # 8667  loss is :  1.3208972\n",
      "Iteration # 8668  loss is :  1.3208972\n",
      "Iteration # 8669  loss is :  1.3208972\n",
      "Iteration # 8670  loss is :  1.3208971\n",
      "Iteration # 8671  loss is :  1.3208971\n",
      "Iteration # 8672  loss is :  1.3208971\n",
      "Iteration # 8673  loss is :  1.3208971\n",
      "Iteration # 8674  loss is :  1.3208971\n",
      "Iteration # 8675  loss is :  1.3208967\n",
      "Iteration # 8676  loss is :  1.3208967\n",
      "Iteration # 8677  loss is :  1.3208967\n",
      "Iteration # 8678  loss is :  1.3208966\n",
      "Iteration # 8679  loss is :  1.3208965\n",
      "Iteration # 8680  loss is :  1.3208965\n",
      "Iteration # 8681  loss is :  1.3208964\n",
      "Iteration # 8682  loss is :  1.3208965\n",
      "Iteration # 8683  loss is :  1.3208963\n",
      "Iteration # 8684  loss is :  1.3208961\n",
      "Iteration # 8685  loss is :  1.3208961\n",
      "Iteration # 8686  loss is :  1.320896\n",
      "Iteration # 8687  loss is :  1.3208959\n",
      "Iteration # 8688  loss is :  1.3208959\n",
      "Iteration # 8689  loss is :  1.3208959\n",
      "Iteration # 8690  loss is :  1.3208958\n",
      "Iteration # 8691  loss is :  1.3208957\n",
      "Iteration # 8692  loss is :  1.3208957\n",
      "Iteration # 8693  loss is :  1.3208956\n",
      "Iteration # 8694  loss is :  1.3208954\n",
      "Iteration # 8695  loss is :  1.3208953\n",
      "Iteration # 8696  loss is :  1.3208952\n",
      "Iteration # 8697  loss is :  1.3208952\n",
      "Iteration # 8698  loss is :  1.3208952\n",
      "Iteration # 8699  loss is :  1.3208952\n",
      "Iteration # 8700  loss is :  1.3208952\n",
      "Iteration # 8701  loss is :  1.3208951\n",
      "Iteration # 8702  loss is :  1.320895\n",
      "Iteration # 8703  loss is :  1.320895\n",
      "Iteration # 8704  loss is :  1.3208948\n",
      "Iteration # 8705  loss is :  1.3208948\n",
      "Iteration # 8706  loss is :  1.3208946\n",
      "Iteration # 8707  loss is :  1.3208946\n",
      "Iteration # 8708  loss is :  1.3208946\n",
      "Iteration # 8709  loss is :  1.3208944\n",
      "Iteration # 8710  loss is :  1.3208945\n",
      "Iteration # 8711  loss is :  1.3208944\n",
      "Iteration # 8712  loss is :  1.3208944\n",
      "Iteration # 8713  loss is :  1.3208944\n",
      "Iteration # 8714  loss is :  1.3208941\n",
      "Iteration # 8715  loss is :  1.320894\n",
      "Iteration # 8716  loss is :  1.3208939\n",
      "Iteration # 8717  loss is :  1.3208939\n",
      "Iteration # 8718  loss is :  1.3208938\n",
      "Iteration # 8719  loss is :  1.3208938\n",
      "Iteration # 8720  loss is :  1.3208938\n",
      "Iteration # 8721  loss is :  1.3208935\n",
      "Iteration # 8722  loss is :  1.3208935\n",
      "Iteration # 8723  loss is :  1.3208935\n",
      "Iteration # 8724  loss is :  1.3208934\n",
      "Iteration # 8725  loss is :  1.3208933\n",
      "Iteration # 8726  loss is :  1.3208933\n",
      "Iteration # 8727  loss is :  1.3208933\n",
      "Iteration # 8728  loss is :  1.3208933\n",
      "Iteration # 8729  loss is :  1.3208932\n",
      "Iteration # 8730  loss is :  1.320893\n",
      "Iteration # 8731  loss is :  1.320893\n",
      "Iteration # 8732  loss is :  1.320893\n",
      "Iteration # 8733  loss is :  1.320893\n",
      "Iteration # 8734  loss is :  1.3208927\n",
      "Iteration # 8735  loss is :  1.3208927\n",
      "Iteration # 8736  loss is :  1.3208926\n",
      "Iteration # 8737  loss is :  1.3208927\n",
      "Iteration # 8738  loss is :  1.3208925\n",
      "Iteration # 8739  loss is :  1.3208925\n",
      "Iteration # 8740  loss is :  1.3208923\n",
      "Iteration # 8741  loss is :  1.3208923\n",
      "Iteration # 8742  loss is :  1.3208922\n",
      "Iteration # 8743  loss is :  1.3208921\n",
      "Iteration # 8744  loss is :  1.3208921\n",
      "Iteration # 8745  loss is :  1.3208919\n",
      "Iteration # 8746  loss is :  1.3208919\n",
      "Iteration # 8747  loss is :  1.3208917\n",
      "Iteration # 8748  loss is :  1.3208917\n",
      "Iteration # 8749  loss is :  1.3208919\n",
      "Iteration # 8750  loss is :  1.3208916\n",
      "Iteration # 8751  loss is :  1.3208916\n",
      "Iteration # 8752  loss is :  1.3208915\n",
      "Iteration # 8753  loss is :  1.3208914\n",
      "Iteration # 8754  loss is :  1.3208914\n",
      "Iteration # 8755  loss is :  1.3208913\n",
      "Iteration # 8756  loss is :  1.3208911\n",
      "Iteration # 8757  loss is :  1.3208911\n",
      "Iteration # 8758  loss is :  1.3208911\n",
      "Iteration # 8759  loss is :  1.3208911\n",
      "Iteration # 8760  loss is :  1.3208909\n",
      "Iteration # 8761  loss is :  1.3208909\n",
      "Iteration # 8762  loss is :  1.3208909\n",
      "Iteration # 8763  loss is :  1.3208909\n",
      "Iteration # 8764  loss is :  1.3208905\n",
      "Iteration # 8765  loss is :  1.3208905\n",
      "Iteration # 8766  loss is :  1.3208905\n",
      "Iteration # 8767  loss is :  1.3208903\n",
      "Iteration # 8768  loss is :  1.3208904\n",
      "Iteration # 8769  loss is :  1.3208903\n",
      "Iteration # 8770  loss is :  1.3208903\n",
      "Iteration # 8771  loss is :  1.3208903\n",
      "Iteration # 8772  loss is :  1.3208902\n",
      "Iteration # 8773  loss is :  1.3208901\n",
      "Iteration # 8774  loss is :  1.32089\n",
      "Iteration # 8775  loss is :  1.32089\n",
      "Iteration # 8776  loss is :  1.32089\n",
      "Iteration # 8777  loss is :  1.3208897\n",
      "Iteration # 8778  loss is :  1.3208897\n",
      "Iteration # 8779  loss is :  1.3208897\n",
      "Iteration # 8780  loss is :  1.3208896\n",
      "Iteration # 8781  loss is :  1.3208895\n",
      "Iteration # 8782  loss is :  1.3208895\n",
      "Iteration # 8783  loss is :  1.3208895\n",
      "Iteration # 8784  loss is :  1.3208895\n",
      "Iteration # 8785  loss is :  1.3208892\n",
      "Iteration # 8786  loss is :  1.3208892\n",
      "Iteration # 8787  loss is :  1.3208892\n",
      "Iteration # 8788  loss is :  1.320889\n",
      "Iteration # 8789  loss is :  1.3208891\n",
      "Iteration # 8790  loss is :  1.320889\n",
      "Iteration # 8791  loss is :  1.3208889\n",
      "Iteration # 8792  loss is :  1.3208889\n",
      "Iteration # 8793  loss is :  1.3208888\n",
      "Iteration # 8794  loss is :  1.3208886\n",
      "Iteration # 8795  loss is :  1.3208885\n",
      "Iteration # 8796  loss is :  1.3208884\n",
      "Iteration # 8797  loss is :  1.3208884\n",
      "Iteration # 8798  loss is :  1.3208884\n",
      "Iteration # 8799  loss is :  1.3208884\n",
      "Iteration # 8800  loss is :  1.3208883\n",
      "Iteration # 8801  loss is :  1.3208882\n",
      "Iteration # 8802  loss is :  1.3208882\n",
      "Iteration # 8803  loss is :  1.3208879\n",
      "Iteration # 8804  loss is :  1.3208879\n",
      "Iteration # 8805  loss is :  1.3208878\n",
      "Iteration # 8806  loss is :  1.3208878\n",
      "Iteration # 8807  loss is :  1.3208877\n",
      "Iteration # 8808  loss is :  1.3208878\n",
      "Iteration # 8809  loss is :  1.3208876\n",
      "Iteration # 8810  loss is :  1.3208876\n",
      "Iteration # 8811  loss is :  1.3208876\n",
      "Iteration # 8812  loss is :  1.3208876\n",
      "Iteration # 8813  loss is :  1.3208874\n",
      "Iteration # 8814  loss is :  1.3208873\n",
      "Iteration # 8815  loss is :  1.3208872\n",
      "Iteration # 8816  loss is :  1.3208871\n",
      "Iteration # 8817  loss is :  1.3208871\n",
      "Iteration # 8818  loss is :  1.320887\n",
      "Iteration # 8819  loss is :  1.3208871\n",
      "Iteration # 8820  loss is :  1.3208871\n",
      "Iteration # 8821  loss is :  1.3208869\n",
      "Iteration # 8822  loss is :  1.3208869\n",
      "Iteration # 8823  loss is :  1.3208869\n",
      "Iteration # 8824  loss is :  1.3208865\n",
      "Iteration # 8825  loss is :  1.3208865\n",
      "Iteration # 8826  loss is :  1.3208865\n",
      "Iteration # 8827  loss is :  1.3208864\n",
      "Iteration # 8828  loss is :  1.3208863\n",
      "Iteration # 8829  loss is :  1.3208863\n",
      "Iteration # 8830  loss is :  1.3208863\n",
      "Iteration # 8831  loss is :  1.3208861\n",
      "Iteration # 8832  loss is :  1.3208861\n",
      "Iteration # 8833  loss is :  1.320886\n",
      "Iteration # 8834  loss is :  1.320886\n",
      "Iteration # 8835  loss is :  1.3208858\n",
      "Iteration # 8836  loss is :  1.3208858\n",
      "Iteration # 8837  loss is :  1.3208857\n",
      "Iteration # 8838  loss is :  1.3208857\n",
      "Iteration # 8839  loss is :  1.3208857\n",
      "Iteration # 8840  loss is :  1.3208857\n",
      "Iteration # 8841  loss is :  1.3208855\n",
      "Iteration # 8842  loss is :  1.3208855\n",
      "Iteration # 8843  loss is :  1.3208855\n",
      "Iteration # 8844  loss is :  1.3208854\n",
      "Iteration # 8845  loss is :  1.3208852\n",
      "Iteration # 8846  loss is :  1.3208852\n",
      "Iteration # 8847  loss is :  1.3208851\n",
      "Iteration # 8848  loss is :  1.320885\n",
      "Iteration # 8849  loss is :  1.320885\n",
      "Iteration # 8850  loss is :  1.320885\n",
      "Iteration # 8851  loss is :  1.320885\n",
      "Iteration # 8852  loss is :  1.3208848\n",
      "Iteration # 8853  loss is :  1.3208847\n",
      "Iteration # 8854  loss is :  1.3208845\n",
      "Iteration # 8855  loss is :  1.3208846\n",
      "Iteration # 8856  loss is :  1.3208845\n",
      "Iteration # 8857  loss is :  1.3208845\n",
      "Iteration # 8858  loss is :  1.3208843\n",
      "Iteration # 8859  loss is :  1.3208843\n",
      "Iteration # 8860  loss is :  1.3208841\n",
      "Iteration # 8861  loss is :  1.3208841\n",
      "Iteration # 8862  loss is :  1.3208841\n",
      "Iteration # 8863  loss is :  1.3208841\n",
      "Iteration # 8864  loss is :  1.320884\n",
      "Iteration # 8865  loss is :  1.3208839\n",
      "Iteration # 8866  loss is :  1.3208838\n",
      "Iteration # 8867  loss is :  1.3208838\n",
      "Iteration # 8868  loss is :  1.3208838\n",
      "Iteration # 8869  loss is :  1.3208836\n",
      "Iteration # 8870  loss is :  1.3208836\n",
      "Iteration # 8871  loss is :  1.3208836\n",
      "Iteration # 8872  loss is :  1.3208834\n",
      "Iteration # 8873  loss is :  1.3208835\n",
      "Iteration # 8874  loss is :  1.3208834\n",
      "Iteration # 8875  loss is :  1.3208833\n",
      "Iteration # 8876  loss is :  1.3208833\n",
      "Iteration # 8877  loss is :  1.3208832\n",
      "Iteration # 8878  loss is :  1.320883\n",
      "Iteration # 8879  loss is :  1.3208829\n",
      "Iteration # 8880  loss is :  1.320883\n",
      "Iteration # 8881  loss is :  1.3208828\n",
      "Iteration # 8882  loss is :  1.3208828\n",
      "Iteration # 8883  loss is :  1.3208828\n",
      "Iteration # 8884  loss is :  1.3208826\n",
      "Iteration # 8885  loss is :  1.3208827\n",
      "Iteration # 8886  loss is :  1.3208824\n",
      "Iteration # 8887  loss is :  1.3208824\n",
      "Iteration # 8888  loss is :  1.3208824\n",
      "Iteration # 8889  loss is :  1.3208822\n",
      "Iteration # 8890  loss is :  1.3208823\n",
      "Iteration # 8891  loss is :  1.3208822\n",
      "Iteration # 8892  loss is :  1.3208822\n",
      "Iteration # 8893  loss is :  1.320882\n",
      "Iteration # 8894  loss is :  1.3208821\n",
      "Iteration # 8895  loss is :  1.320882\n",
      "Iteration # 8896  loss is :  1.3208818\n",
      "Iteration # 8897  loss is :  1.3208818\n",
      "Iteration # 8898  loss is :  1.3208817\n",
      "Iteration # 8899  loss is :  1.3208817\n",
      "Iteration # 8900  loss is :  1.3208816\n",
      "Iteration # 8901  loss is :  1.3208815\n",
      "Iteration # 8902  loss is :  1.3208815\n",
      "Iteration # 8903  loss is :  1.3208815\n",
      "Iteration # 8904  loss is :  1.3208815\n",
      "Iteration # 8905  loss is :  1.3208814\n",
      "Iteration # 8906  loss is :  1.3208811\n",
      "Iteration # 8907  loss is :  1.3208811\n",
      "Iteration # 8908  loss is :  1.320881\n",
      "Iteration # 8909  loss is :  1.320881\n",
      "Iteration # 8910  loss is :  1.3208809\n",
      "Iteration # 8911  loss is :  1.3208809\n",
      "Iteration # 8912  loss is :  1.3208809\n",
      "Iteration # 8913  loss is :  1.3208808\n",
      "Iteration # 8914  loss is :  1.3208808\n",
      "Iteration # 8915  loss is :  1.3208807\n",
      "Iteration # 8916  loss is :  1.3208807\n",
      "Iteration # 8917  loss is :  1.3208803\n",
      "Iteration # 8918  loss is :  1.3208803\n",
      "Iteration # 8919  loss is :  1.3208803\n",
      "Iteration # 8920  loss is :  1.3208802\n",
      "Iteration # 8921  loss is :  1.3208802\n",
      "Iteration # 8922  loss is :  1.32088\n",
      "Iteration # 8923  loss is :  1.32088\n",
      "Iteration # 8924  loss is :  1.32088\n",
      "Iteration # 8925  loss is :  1.3208799\n",
      "Iteration # 8926  loss is :  1.3208799\n",
      "Iteration # 8927  loss is :  1.3208798\n",
      "Iteration # 8928  loss is :  1.3208798\n",
      "Iteration # 8929  loss is :  1.3208798\n",
      "Iteration # 8930  loss is :  1.3208797\n",
      "Iteration # 8931  loss is :  1.3208796\n",
      "Iteration # 8932  loss is :  1.3208796\n",
      "Iteration # 8933  loss is :  1.3208793\n",
      "Iteration # 8934  loss is :  1.3208796\n",
      "Iteration # 8935  loss is :  1.3208793\n",
      "Iteration # 8936  loss is :  1.3208793\n",
      "Iteration # 8937  loss is :  1.3208791\n",
      "Iteration # 8938  loss is :  1.320879\n",
      "Iteration # 8939  loss is :  1.320879\n",
      "Iteration # 8940  loss is :  1.320879\n",
      "Iteration # 8941  loss is :  1.3208789\n",
      "Iteration # 8942  loss is :  1.3208789\n",
      "Iteration # 8943  loss is :  1.3208787\n",
      "Iteration # 8944  loss is :  1.3208787\n",
      "Iteration # 8945  loss is :  1.3208786\n",
      "Iteration # 8946  loss is :  1.3208785\n",
      "Iteration # 8947  loss is :  1.3208785\n",
      "Iteration # 8948  loss is :  1.3208784\n",
      "Iteration # 8949  loss is :  1.3208781\n",
      "Iteration # 8950  loss is :  1.3208783\n",
      "Iteration # 8951  loss is :  1.3208781\n",
      "Iteration # 8952  loss is :  1.3208781\n",
      "Iteration # 8953  loss is :  1.3208781\n",
      "Iteration # 8954  loss is :  1.320878\n",
      "Iteration # 8955  loss is :  1.320878\n",
      "Iteration # 8956  loss is :  1.320878\n",
      "Iteration # 8957  loss is :  1.320878\n",
      "Iteration # 8958  loss is :  1.320878\n",
      "Iteration # 8959  loss is :  1.3208777\n",
      "Iteration # 8960  loss is :  1.3208777\n",
      "Iteration # 8961  loss is :  1.3208777\n",
      "Iteration # 8962  loss is :  1.3208776\n",
      "Iteration # 8963  loss is :  1.3208776\n",
      "Iteration # 8964  loss is :  1.3208774\n",
      "Iteration # 8965  loss is :  1.3208774\n",
      "Iteration # 8966  loss is :  1.3208774\n",
      "Iteration # 8967  loss is :  1.3208773\n",
      "Iteration # 8968  loss is :  1.3208772\n",
      "Iteration # 8969  loss is :  1.3208771\n",
      "Iteration # 8970  loss is :  1.320877\n",
      "Iteration # 8971  loss is :  1.3208771\n",
      "Iteration # 8972  loss is :  1.3208768\n",
      "Iteration # 8973  loss is :  1.3208768\n",
      "Iteration # 8974  loss is :  1.3208768\n",
      "Iteration # 8975  loss is :  1.3208767\n",
      "Iteration # 8976  loss is :  1.3208766\n",
      "Iteration # 8977  loss is :  1.3208766\n",
      "Iteration # 8978  loss is :  1.3208766\n",
      "Iteration # 8979  loss is :  1.3208762\n",
      "Iteration # 8980  loss is :  1.3208764\n",
      "Iteration # 8981  loss is :  1.3208762\n",
      "Iteration # 8982  loss is :  1.3208762\n",
      "Iteration # 8983  loss is :  1.3208761\n",
      "Iteration # 8984  loss is :  1.3208761\n",
      "Iteration # 8985  loss is :  1.3208761\n",
      "Iteration # 8986  loss is :  1.3208761\n",
      "Iteration # 8987  loss is :  1.3208761\n",
      "Iteration # 8988  loss is :  1.320876\n",
      "Iteration # 8989  loss is :  1.3208758\n",
      "Iteration # 8990  loss is :  1.3208758\n",
      "Iteration # 8991  loss is :  1.3208756\n",
      "Iteration # 8992  loss is :  1.3208756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 8993  loss is :  1.3208755\n",
      "Iteration # 8994  loss is :  1.3208755\n",
      "Iteration # 8995  loss is :  1.3208755\n",
      "Iteration # 8996  loss is :  1.3208755\n",
      "Iteration # 8997  loss is :  1.3208753\n",
      "Iteration # 8998  loss is :  1.3208753\n",
      "Iteration # 8999  loss is :  1.3208753\n",
      "Iteration # 9000  loss is :  1.3208752\n",
      "Iteration # 9001  loss is :  1.320875\n",
      "Iteration # 9002  loss is :  1.3208749\n",
      "Iteration # 9003  loss is :  1.3208749\n",
      "Iteration # 9004  loss is :  1.3208748\n",
      "Iteration # 9005  loss is :  1.3208747\n",
      "Iteration # 9006  loss is :  1.3208747\n",
      "Iteration # 9007  loss is :  1.3208747\n",
      "Iteration # 9008  loss is :  1.3208746\n",
      "Iteration # 9009  loss is :  1.3208746\n",
      "Iteration # 9010  loss is :  1.3208745\n",
      "Iteration # 9011  loss is :  1.3208743\n",
      "Iteration # 9012  loss is :  1.3208742\n",
      "Iteration # 9013  loss is :  1.3208742\n",
      "Iteration # 9014  loss is :  1.3208742\n",
      "Iteration # 9015  loss is :  1.3208741\n",
      "Iteration # 9016  loss is :  1.3208742\n",
      "Iteration # 9017  loss is :  1.320874\n",
      "Iteration # 9018  loss is :  1.320874\n",
      "Iteration # 9019  loss is :  1.320874\n",
      "Iteration # 9020  loss is :  1.3208739\n",
      "Iteration # 9021  loss is :  1.3208737\n",
      "Iteration # 9022  loss is :  1.3208736\n",
      "Iteration # 9023  loss is :  1.3208736\n",
      "Iteration # 9024  loss is :  1.3208736\n",
      "Iteration # 9025  loss is :  1.3208735\n",
      "Iteration # 9026  loss is :  1.3208736\n",
      "Iteration # 9027  loss is :  1.3208734\n",
      "Iteration # 9028  loss is :  1.3208734\n",
      "Iteration # 9029  loss is :  1.3208734\n",
      "Iteration # 9030  loss is :  1.3208733\n",
      "Iteration # 9031  loss is :  1.3208731\n",
      "Iteration # 9032  loss is :  1.320873\n",
      "Iteration # 9033  loss is :  1.3208729\n",
      "Iteration # 9034  loss is :  1.320873\n",
      "Iteration # 9035  loss is :  1.3208728\n",
      "Iteration # 9036  loss is :  1.3208728\n",
      "Iteration # 9037  loss is :  1.3208728\n",
      "Iteration # 9038  loss is :  1.3208727\n",
      "Iteration # 9039  loss is :  1.3208727\n",
      "Iteration # 9040  loss is :  1.3208725\n",
      "Iteration # 9041  loss is :  1.3208725\n",
      "Iteration # 9042  loss is :  1.3208724\n",
      "Iteration # 9043  loss is :  1.3208723\n",
      "Iteration # 9044  loss is :  1.3208723\n",
      "Iteration # 9045  loss is :  1.3208723\n",
      "Iteration # 9046  loss is :  1.3208722\n",
      "Iteration # 9047  loss is :  1.3208722\n",
      "Iteration # 9048  loss is :  1.3208722\n",
      "Iteration # 9049  loss is :  1.3208721\n",
      "Iteration # 9050  loss is :  1.3208721\n",
      "Iteration # 9051  loss is :  1.3208718\n",
      "Iteration # 9052  loss is :  1.320872\n",
      "Iteration # 9053  loss is :  1.3208717\n",
      "Iteration # 9054  loss is :  1.3208717\n",
      "Iteration # 9055  loss is :  1.3208717\n",
      "Iteration # 9056  loss is :  1.3208715\n",
      "Iteration # 9057  loss is :  1.3208715\n",
      "Iteration # 9058  loss is :  1.3208715\n",
      "Iteration # 9059  loss is :  1.3208714\n",
      "Iteration # 9060  loss is :  1.3208712\n",
      "Iteration # 9061  loss is :  1.3208712\n",
      "Iteration # 9062  loss is :  1.3208712\n",
      "Iteration # 9063  loss is :  1.3208711\n",
      "Iteration # 9064  loss is :  1.3208711\n",
      "Iteration # 9065  loss is :  1.3208709\n",
      "Iteration # 9066  loss is :  1.3208709\n",
      "Iteration # 9067  loss is :  1.3208708\n",
      "Iteration # 9068  loss is :  1.3208708\n",
      "Iteration # 9069  loss is :  1.3208706\n",
      "Iteration # 9070  loss is :  1.3208706\n",
      "Iteration # 9071  loss is :  1.3208706\n",
      "Iteration # 9072  loss is :  1.3208706\n",
      "Iteration # 9073  loss is :  1.3208704\n",
      "Iteration # 9074  loss is :  1.3208704\n",
      "Iteration # 9075  loss is :  1.3208704\n",
      "Iteration # 9076  loss is :  1.3208703\n",
      "Iteration # 9077  loss is :  1.3208702\n",
      "Iteration # 9078  loss is :  1.3208702\n",
      "Iteration # 9079  loss is :  1.3208702\n",
      "Iteration # 9080  loss is :  1.32087\n",
      "Iteration # 9081  loss is :  1.32087\n",
      "Iteration # 9082  loss is :  1.32087\n",
      "Iteration # 9083  loss is :  1.3208699\n",
      "Iteration # 9084  loss is :  1.3208699\n",
      "Iteration # 9085  loss is :  1.3208698\n",
      "Iteration # 9086  loss is :  1.3208697\n",
      "Iteration # 9087  loss is :  1.3208696\n",
      "Iteration # 9088  loss is :  1.3208694\n",
      "Iteration # 9089  loss is :  1.3208696\n",
      "Iteration # 9090  loss is :  1.3208694\n",
      "Iteration # 9091  loss is :  1.3208693\n",
      "Iteration # 9092  loss is :  1.3208693\n",
      "Iteration # 9093  loss is :  1.3208693\n",
      "Iteration # 9094  loss is :  1.3208691\n",
      "Iteration # 9095  loss is :  1.3208691\n",
      "Iteration # 9096  loss is :  1.3208691\n",
      "Iteration # 9097  loss is :  1.3208688\n",
      "Iteration # 9098  loss is :  1.3208688\n",
      "Iteration # 9099  loss is :  1.3208687\n",
      "Iteration # 9100  loss is :  1.3208687\n",
      "Iteration # 9101  loss is :  1.3208687\n",
      "Iteration # 9102  loss is :  1.3208687\n",
      "Iteration # 9103  loss is :  1.3208685\n",
      "Iteration # 9104  loss is :  1.3208685\n",
      "Iteration # 9105  loss is :  1.3208685\n",
      "Iteration # 9106  loss is :  1.3208685\n",
      "Iteration # 9107  loss is :  1.3208685\n",
      "Iteration # 9108  loss is :  1.3208683\n",
      "Iteration # 9109  loss is :  1.3208683\n",
      "Iteration # 9110  loss is :  1.3208683\n",
      "Iteration # 9111  loss is :  1.3208681\n",
      "Iteration # 9112  loss is :  1.320868\n",
      "Iteration # 9113  loss is :  1.320868\n",
      "Iteration # 9114  loss is :  1.320868\n",
      "Iteration # 9115  loss is :  1.320868\n",
      "Iteration # 9116  loss is :  1.3208678\n",
      "Iteration # 9117  loss is :  1.3208679\n",
      "Iteration # 9118  loss is :  1.3208677\n",
      "Iteration # 9119  loss is :  1.3208675\n",
      "Iteration # 9120  loss is :  1.3208675\n",
      "Iteration # 9121  loss is :  1.3208674\n",
      "Iteration # 9122  loss is :  1.3208674\n",
      "Iteration # 9123  loss is :  1.3208674\n",
      "Iteration # 9124  loss is :  1.3208673\n",
      "Iteration # 9125  loss is :  1.3208673\n",
      "Iteration # 9126  loss is :  1.3208672\n",
      "Iteration # 9127  loss is :  1.3208672\n",
      "Iteration # 9128  loss is :  1.3208671\n",
      "Iteration # 9129  loss is :  1.320867\n",
      "Iteration # 9130  loss is :  1.3208668\n",
      "Iteration # 9131  loss is :  1.3208668\n",
      "Iteration # 9132  loss is :  1.3208668\n",
      "Iteration # 9133  loss is :  1.3208667\n",
      "Iteration # 9134  loss is :  1.3208666\n",
      "Iteration # 9135  loss is :  1.3208666\n",
      "Iteration # 9136  loss is :  1.3208666\n",
      "Iteration # 9137  loss is :  1.3208666\n",
      "Iteration # 9138  loss is :  1.3208665\n",
      "Iteration # 9139  loss is :  1.3208665\n",
      "Iteration # 9140  loss is :  1.3208665\n",
      "Iteration # 9141  loss is :  1.3208663\n",
      "Iteration # 9142  loss is :  1.3208662\n",
      "Iteration # 9143  loss is :  1.3208661\n",
      "Iteration # 9144  loss is :  1.3208661\n",
      "Iteration # 9145  loss is :  1.320866\n",
      "Iteration # 9146  loss is :  1.320866\n",
      "Iteration # 9147  loss is :  1.3208659\n",
      "Iteration # 9148  loss is :  1.3208659\n",
      "Iteration # 9149  loss is :  1.3208659\n",
      "Iteration # 9150  loss is :  1.3208658\n",
      "Iteration # 9151  loss is :  1.3208656\n",
      "Iteration # 9152  loss is :  1.3208655\n",
      "Iteration # 9153  loss is :  1.3208655\n",
      "Iteration # 9154  loss is :  1.3208654\n",
      "Iteration # 9155  loss is :  1.3208653\n",
      "Iteration # 9156  loss is :  1.3208653\n",
      "Iteration # 9157  loss is :  1.3208652\n",
      "Iteration # 9158  loss is :  1.3208652\n",
      "Iteration # 9159  loss is :  1.320865\n",
      "Iteration # 9160  loss is :  1.320865\n",
      "Iteration # 9161  loss is :  1.320865\n",
      "Iteration # 9162  loss is :  1.320865\n",
      "Iteration # 9163  loss is :  1.3208648\n",
      "Iteration # 9164  loss is :  1.3208648\n",
      "Iteration # 9165  loss is :  1.3208647\n",
      "Iteration # 9166  loss is :  1.3208647\n",
      "Iteration # 9167  loss is :  1.3208647\n",
      "Iteration # 9168  loss is :  1.3208647\n",
      "Iteration # 9169  loss is :  1.3208646\n",
      "Iteration # 9170  loss is :  1.3208646\n",
      "Iteration # 9171  loss is :  1.3208646\n",
      "Iteration # 9172  loss is :  1.3208646\n",
      "Iteration # 9173  loss is :  1.3208643\n",
      "Iteration # 9174  loss is :  1.3208642\n",
      "Iteration # 9175  loss is :  1.3208641\n",
      "Iteration # 9176  loss is :  1.3208642\n",
      "Iteration # 9177  loss is :  1.320864\n",
      "Iteration # 9178  loss is :  1.320864\n",
      "Iteration # 9179  loss is :  1.320864\n",
      "Iteration # 9180  loss is :  1.320864\n",
      "Iteration # 9181  loss is :  1.3208638\n",
      "Iteration # 9182  loss is :  1.3208637\n",
      "Iteration # 9183  loss is :  1.3208637\n",
      "Iteration # 9184  loss is :  1.3208637\n",
      "Iteration # 9185  loss is :  1.3208635\n",
      "Iteration # 9186  loss is :  1.3208635\n",
      "Iteration # 9187  loss is :  1.3208634\n",
      "Iteration # 9188  loss is :  1.3208634\n",
      "Iteration # 9189  loss is :  1.3208634\n",
      "Iteration # 9190  loss is :  1.3208632\n",
      "Iteration # 9191  loss is :  1.3208631\n",
      "Iteration # 9192  loss is :  1.3208631\n",
      "Iteration # 9193  loss is :  1.3208631\n",
      "Iteration # 9194  loss is :  1.320863\n",
      "Iteration # 9195  loss is :  1.3208629\n",
      "Iteration # 9196  loss is :  1.3208628\n",
      "Iteration # 9197  loss is :  1.3208628\n",
      "Iteration # 9198  loss is :  1.3208628\n",
      "Iteration # 9199  loss is :  1.3208628\n",
      "Iteration # 9200  loss is :  1.3208627\n",
      "Iteration # 9201  loss is :  1.3208627\n",
      "Iteration # 9202  loss is :  1.3208625\n",
      "Iteration # 9203  loss is :  1.3208624\n",
      "Iteration # 9204  loss is :  1.3208624\n",
      "Iteration # 9205  loss is :  1.3208624\n",
      "Iteration # 9206  loss is :  1.3208623\n",
      "Iteration # 9207  loss is :  1.3208623\n",
      "Iteration # 9208  loss is :  1.320862\n",
      "Iteration # 9209  loss is :  1.320862\n",
      "Iteration # 9210  loss is :  1.320862\n",
      "Iteration # 9211  loss is :  1.3208619\n",
      "Iteration # 9212  loss is :  1.320862\n",
      "Iteration # 9213  loss is :  1.3208618\n",
      "Iteration # 9214  loss is :  1.3208618\n",
      "Iteration # 9215  loss is :  1.3208618\n",
      "Iteration # 9216  loss is :  1.3208617\n",
      "Iteration # 9217  loss is :  1.3208616\n",
      "Iteration # 9218  loss is :  1.3208616\n",
      "Iteration # 9219  loss is :  1.3208615\n",
      "Iteration # 9220  loss is :  1.3208615\n",
      "Iteration # 9221  loss is :  1.3208612\n",
      "Iteration # 9222  loss is :  1.3208612\n",
      "Iteration # 9223  loss is :  1.3208612\n",
      "Iteration # 9224  loss is :  1.3208611\n",
      "Iteration # 9225  loss is :  1.3208611\n",
      "Iteration # 9226  loss is :  1.320861\n",
      "Iteration # 9227  loss is :  1.320861\n",
      "Iteration # 9228  loss is :  1.320861\n",
      "Iteration # 9229  loss is :  1.3208609\n",
      "Iteration # 9230  loss is :  1.3208609\n",
      "Iteration # 9231  loss is :  1.3208607\n",
      "Iteration # 9232  loss is :  1.3208607\n",
      "Iteration # 9233  loss is :  1.3208606\n",
      "Iteration # 9234  loss is :  1.3208606\n",
      "Iteration # 9235  loss is :  1.3208605\n",
      "Iteration # 9236  loss is :  1.3208605\n",
      "Iteration # 9237  loss is :  1.3208605\n",
      "Iteration # 9238  loss is :  1.3208605\n",
      "Iteration # 9239  loss is :  1.3208605\n",
      "Iteration # 9240  loss is :  1.3208603\n",
      "Iteration # 9241  loss is :  1.3208601\n",
      "Iteration # 9242  loss is :  1.32086\n",
      "Iteration # 9243  loss is :  1.32086\n",
      "Iteration # 9244  loss is :  1.3208599\n",
      "Iteration # 9245  loss is :  1.3208599\n",
      "Iteration # 9246  loss is :  1.3208598\n",
      "Iteration # 9247  loss is :  1.3208598\n",
      "Iteration # 9248  loss is :  1.3208598\n",
      "Iteration # 9249  loss is :  1.3208597\n",
      "Iteration # 9250  loss is :  1.3208597\n",
      "Iteration # 9251  loss is :  1.3208596\n",
      "Iteration # 9252  loss is :  1.3208596\n",
      "Iteration # 9253  loss is :  1.3208593\n",
      "Iteration # 9254  loss is :  1.3208593\n",
      "Iteration # 9255  loss is :  1.3208592\n",
      "Iteration # 9256  loss is :  1.3208592\n",
      "Iteration # 9257  loss is :  1.3208591\n",
      "Iteration # 9258  loss is :  1.3208591\n",
      "Iteration # 9259  loss is :  1.3208591\n",
      "Iteration # 9260  loss is :  1.320859\n",
      "Iteration # 9261  loss is :  1.3208591\n",
      "Iteration # 9262  loss is :  1.320859\n",
      "Iteration # 9263  loss is :  1.320859\n",
      "Iteration # 9264  loss is :  1.3208587\n",
      "Iteration # 9265  loss is :  1.3208588\n",
      "Iteration # 9266  loss is :  1.3208586\n",
      "Iteration # 9267  loss is :  1.3208586\n",
      "Iteration # 9268  loss is :  1.3208586\n",
      "Iteration # 9269  loss is :  1.3208586\n",
      "Iteration # 9270  loss is :  1.3208585\n",
      "Iteration # 9271  loss is :  1.3208585\n",
      "Iteration # 9272  loss is :  1.3208582\n",
      "Iteration # 9273  loss is :  1.3208584\n",
      "Iteration # 9274  loss is :  1.3208581\n",
      "Iteration # 9275  loss is :  1.3208581\n",
      "Iteration # 9276  loss is :  1.320858\n",
      "Iteration # 9277  loss is :  1.320858\n",
      "Iteration # 9278  loss is :  1.320858\n",
      "Iteration # 9279  loss is :  1.3208578\n",
      "Iteration # 9280  loss is :  1.3208578\n",
      "Iteration # 9281  loss is :  1.3208578\n",
      "Iteration # 9282  loss is :  1.3208578\n",
      "Iteration # 9283  loss is :  1.3208576\n",
      "Iteration # 9284  loss is :  1.3208576\n",
      "Iteration # 9285  loss is :  1.3208575\n",
      "Iteration # 9286  loss is :  1.3208573\n",
      "Iteration # 9287  loss is :  1.3208573\n",
      "Iteration # 9288  loss is :  1.3208572\n",
      "Iteration # 9289  loss is :  1.3208572\n",
      "Iteration # 9290  loss is :  1.3208572\n",
      "Iteration # 9291  loss is :  1.320857\n",
      "Iteration # 9292  loss is :  1.320857\n",
      "Iteration # 9293  loss is :  1.320857\n",
      "Iteration # 9294  loss is :  1.320857\n",
      "Iteration # 9295  loss is :  1.3208569\n",
      "Iteration # 9296  loss is :  1.3208569\n",
      "Iteration # 9297  loss is :  1.3208568\n",
      "Iteration # 9298  loss is :  1.3208567\n",
      "Iteration # 9299  loss is :  1.3208567\n",
      "Iteration # 9300  loss is :  1.3208567\n",
      "Iteration # 9301  loss is :  1.3208565\n",
      "Iteration # 9302  loss is :  1.3208565\n",
      "Iteration # 9303  loss is :  1.3208565\n",
      "Iteration # 9304  loss is :  1.3208565\n",
      "Iteration # 9305  loss is :  1.3208562\n",
      "Iteration # 9306  loss is :  1.3208563\n",
      "Iteration # 9307  loss is :  1.3208562\n",
      "Iteration # 9308  loss is :  1.3208562\n",
      "Iteration # 9309  loss is :  1.320856\n",
      "Iteration # 9310  loss is :  1.320856\n",
      "Iteration # 9311  loss is :  1.3208559\n",
      "Iteration # 9312  loss is :  1.3208559\n",
      "Iteration # 9313  loss is :  1.3208559\n",
      "Iteration # 9314  loss is :  1.3208559\n",
      "Iteration # 9315  loss is :  1.3208556\n",
      "Iteration # 9316  loss is :  1.3208556\n",
      "Iteration # 9317  loss is :  1.3208556\n",
      "Iteration # 9318  loss is :  1.3208556\n",
      "Iteration # 9319  loss is :  1.3208556\n",
      "Iteration # 9320  loss is :  1.3208555\n",
      "Iteration # 9321  loss is :  1.3208554\n",
      "Iteration # 9322  loss is :  1.3208554\n",
      "Iteration # 9323  loss is :  1.3208553\n",
      "Iteration # 9324  loss is :  1.3208551\n",
      "Iteration # 9325  loss is :  1.3208551\n",
      "Iteration # 9326  loss is :  1.3208551\n",
      "Iteration # 9327  loss is :  1.320855\n",
      "Iteration # 9328  loss is :  1.320855\n",
      "Iteration # 9329  loss is :  1.3208551\n",
      "Iteration # 9330  loss is :  1.3208549\n",
      "Iteration # 9331  loss is :  1.3208549\n",
      "Iteration # 9332  loss is :  1.3208548\n",
      "Iteration # 9333  loss is :  1.3208547\n",
      "Iteration # 9334  loss is :  1.3208547\n",
      "Iteration # 9335  loss is :  1.3208545\n",
      "Iteration # 9336  loss is :  1.3208545\n",
      "Iteration # 9337  loss is :  1.3208545\n",
      "Iteration # 9338  loss is :  1.3208544\n",
      "Iteration # 9339  loss is :  1.3208543\n",
      "Iteration # 9340  loss is :  1.3208543\n",
      "Iteration # 9341  loss is :  1.3208543\n",
      "Iteration # 9342  loss is :  1.3208541\n",
      "Iteration # 9343  loss is :  1.3208541\n",
      "Iteration # 9344  loss is :  1.320854\n",
      "Iteration # 9345  loss is :  1.320854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 9346  loss is :  1.320854\n",
      "Iteration # 9347  loss is :  1.3208538\n",
      "Iteration # 9348  loss is :  1.3208537\n",
      "Iteration # 9349  loss is :  1.3208538\n",
      "Iteration # 9350  loss is :  1.3208537\n",
      "Iteration # 9351  loss is :  1.3208536\n",
      "Iteration # 9352  loss is :  1.3208536\n",
      "Iteration # 9353  loss is :  1.3208535\n",
      "Iteration # 9354  loss is :  1.3208535\n",
      "Iteration # 9355  loss is :  1.3208534\n",
      "Iteration # 9356  loss is :  1.3208532\n",
      "Iteration # 9357  loss is :  1.3208532\n",
      "Iteration # 9358  loss is :  1.3208532\n",
      "Iteration # 9359  loss is :  1.3208532\n",
      "Iteration # 9360  loss is :  1.3208531\n",
      "Iteration # 9361  loss is :  1.3208531\n",
      "Iteration # 9362  loss is :  1.320853\n",
      "Iteration # 9363  loss is :  1.320853\n",
      "Iteration # 9364  loss is :  1.320853\n",
      "Iteration # 9365  loss is :  1.3208529\n",
      "Iteration # 9366  loss is :  1.3208529\n",
      "Iteration # 9367  loss is :  1.3208526\n",
      "Iteration # 9368  loss is :  1.3208526\n",
      "Iteration # 9369  loss is :  1.3208525\n",
      "Iteration # 9370  loss is :  1.3208525\n",
      "Iteration # 9371  loss is :  1.3208525\n",
      "Iteration # 9372  loss is :  1.3208524\n",
      "Iteration # 9373  loss is :  1.3208523\n",
      "Iteration # 9374  loss is :  1.3208523\n",
      "Iteration # 9375  loss is :  1.3208522\n",
      "Iteration # 9376  loss is :  1.3208522\n",
      "Iteration # 9377  loss is :  1.320852\n",
      "Iteration # 9378  loss is :  1.3208522\n",
      "Iteration # 9379  loss is :  1.3208519\n",
      "Iteration # 9380  loss is :  1.3208519\n",
      "Iteration # 9381  loss is :  1.3208518\n",
      "Iteration # 9382  loss is :  1.3208517\n",
      "Iteration # 9383  loss is :  1.3208517\n",
      "Iteration # 9384  loss is :  1.3208517\n",
      "Iteration # 9385  loss is :  1.3208516\n",
      "Iteration # 9386  loss is :  1.3208516\n",
      "Iteration # 9387  loss is :  1.3208514\n",
      "Iteration # 9388  loss is :  1.3208516\n",
      "Iteration # 9389  loss is :  1.3208513\n",
      "Iteration # 9390  loss is :  1.3208513\n",
      "Iteration # 9391  loss is :  1.3208513\n",
      "Iteration # 9392  loss is :  1.3208513\n",
      "Iteration # 9393  loss is :  1.3208512\n",
      "Iteration # 9394  loss is :  1.3208511\n",
      "Iteration # 9395  loss is :  1.3208511\n",
      "Iteration # 9396  loss is :  1.320851\n",
      "Iteration # 9397  loss is :  1.3208511\n",
      "Iteration # 9398  loss is :  1.320851\n",
      "Iteration # 9399  loss is :  1.3208508\n",
      "Iteration # 9400  loss is :  1.3208508\n",
      "Iteration # 9401  loss is :  1.3208508\n",
      "Iteration # 9402  loss is :  1.3208505\n",
      "Iteration # 9403  loss is :  1.3208505\n",
      "Iteration # 9404  loss is :  1.3208505\n",
      "Iteration # 9405  loss is :  1.3208505\n",
      "Iteration # 9406  loss is :  1.3208504\n",
      "Iteration # 9407  loss is :  1.3208503\n",
      "Iteration # 9408  loss is :  1.3208504\n",
      "Iteration # 9409  loss is :  1.3208503\n",
      "Iteration # 9410  loss is :  1.3208501\n",
      "Iteration # 9411  loss is :  1.3208503\n",
      "Iteration # 9412  loss is :  1.3208501\n",
      "Iteration # 9413  loss is :  1.32085\n",
      "Iteration # 9414  loss is :  1.3208499\n",
      "Iteration # 9415  loss is :  1.3208499\n",
      "Iteration # 9416  loss is :  1.3208498\n",
      "Iteration # 9417  loss is :  1.3208498\n",
      "Iteration # 9418  loss is :  1.3208497\n",
      "Iteration # 9419  loss is :  1.3208495\n",
      "Iteration # 9420  loss is :  1.3208497\n",
      "Iteration # 9421  loss is :  1.3208495\n",
      "Iteration # 9422  loss is :  1.3208494\n",
      "Iteration # 9423  loss is :  1.3208494\n",
      "Iteration # 9424  loss is :  1.3208494\n",
      "Iteration # 9425  loss is :  1.3208493\n",
      "Iteration # 9426  loss is :  1.3208492\n",
      "Iteration # 9427  loss is :  1.3208492\n",
      "Iteration # 9428  loss is :  1.3208492\n",
      "Iteration # 9429  loss is :  1.3208491\n",
      "Iteration # 9430  loss is :  1.3208491\n",
      "Iteration # 9431  loss is :  1.320849\n",
      "Iteration # 9432  loss is :  1.320849\n",
      "Iteration # 9433  loss is :  1.320849\n",
      "Iteration # 9434  loss is :  1.3208487\n",
      "Iteration # 9435  loss is :  1.3208487\n",
      "Iteration # 9436  loss is :  1.3208487\n",
      "Iteration # 9437  loss is :  1.3208486\n",
      "Iteration # 9438  loss is :  1.3208486\n",
      "Iteration # 9439  loss is :  1.3208486\n",
      "Iteration # 9440  loss is :  1.3208483\n",
      "Iteration # 9441  loss is :  1.3208483\n",
      "Iteration # 9442  loss is :  1.3208483\n",
      "Iteration # 9443  loss is :  1.3208483\n",
      "Iteration # 9444  loss is :  1.3208482\n",
      "Iteration # 9445  loss is :  1.3208482\n",
      "Iteration # 9446  loss is :  1.3208481\n",
      "Iteration # 9447  loss is :  1.3208481\n",
      "Iteration # 9448  loss is :  1.3208481\n",
      "Iteration # 9449  loss is :  1.320848\n",
      "Iteration # 9450  loss is :  1.3208479\n",
      "Iteration # 9451  loss is :  1.3208477\n",
      "Iteration # 9452  loss is :  1.3208477\n",
      "Iteration # 9453  loss is :  1.3208477\n",
      "Iteration # 9454  loss is :  1.3208476\n",
      "Iteration # 9455  loss is :  1.3208475\n",
      "Iteration # 9456  loss is :  1.3208475\n",
      "Iteration # 9457  loss is :  1.3208475\n",
      "Iteration # 9458  loss is :  1.3208475\n",
      "Iteration # 9459  loss is :  1.3208474\n",
      "Iteration # 9460  loss is :  1.3208473\n",
      "Iteration # 9461  loss is :  1.3208473\n",
      "Iteration # 9462  loss is :  1.3208473\n",
      "Iteration # 9463  loss is :  1.3208472\n",
      "Iteration # 9464  loss is :  1.3208472\n",
      "Iteration # 9465  loss is :  1.320847\n",
      "Iteration # 9466  loss is :  1.320847\n",
      "Iteration # 9467  loss is :  1.320847\n",
      "Iteration # 9468  loss is :  1.3208469\n",
      "Iteration # 9469  loss is :  1.3208468\n",
      "Iteration # 9470  loss is :  1.3208468\n",
      "Iteration # 9471  loss is :  1.3208467\n",
      "Iteration # 9472  loss is :  1.3208468\n",
      "Iteration # 9473  loss is :  1.3208464\n",
      "Iteration # 9474  loss is :  1.3208464\n",
      "Iteration # 9475  loss is :  1.3208464\n",
      "Iteration # 9476  loss is :  1.3208464\n",
      "Iteration # 9477  loss is :  1.3208463\n",
      "Iteration # 9478  loss is :  1.3208462\n",
      "Iteration # 9479  loss is :  1.3208462\n",
      "Iteration # 9480  loss is :  1.3208461\n",
      "Iteration # 9481  loss is :  1.3208461\n",
      "Iteration # 9482  loss is :  1.3208461\n",
      "Iteration # 9483  loss is :  1.320846\n",
      "Iteration # 9484  loss is :  1.320846\n",
      "Iteration # 9485  loss is :  1.3208458\n",
      "Iteration # 9486  loss is :  1.3208456\n",
      "Iteration # 9487  loss is :  1.3208456\n",
      "Iteration # 9488  loss is :  1.3208456\n",
      "Iteration # 9489  loss is :  1.3208456\n",
      "Iteration # 9490  loss is :  1.3208456\n",
      "Iteration # 9491  loss is :  1.3208456\n",
      "Iteration # 9492  loss is :  1.3208456\n",
      "Iteration # 9493  loss is :  1.3208455\n",
      "Iteration # 9494  loss is :  1.3208455\n",
      "Iteration # 9495  loss is :  1.3208455\n",
      "Iteration # 9496  loss is :  1.3208452\n",
      "Iteration # 9497  loss is :  1.3208451\n",
      "Iteration # 9498  loss is :  1.3208452\n",
      "Iteration # 9499  loss is :  1.3208451\n",
      "Iteration # 9500  loss is :  1.3208451\n",
      "Iteration # 9501  loss is :  1.3208449\n",
      "Iteration # 9502  loss is :  1.3208449\n",
      "Iteration # 9503  loss is :  1.3208449\n",
      "Iteration # 9504  loss is :  1.3208448\n",
      "Iteration # 9505  loss is :  1.3208449\n",
      "Iteration # 9506  loss is :  1.3208447\n",
      "Iteration # 9507  loss is :  1.3208447\n",
      "Iteration # 9508  loss is :  1.3208445\n",
      "Iteration # 9509  loss is :  1.3208444\n",
      "Iteration # 9510  loss is :  1.3208443\n",
      "Iteration # 9511  loss is :  1.3208443\n",
      "Iteration # 9512  loss is :  1.3208443\n",
      "Iteration # 9513  loss is :  1.3208443\n",
      "Iteration # 9514  loss is :  1.3208443\n",
      "Iteration # 9515  loss is :  1.3208442\n",
      "Iteration # 9516  loss is :  1.3208442\n",
      "Iteration # 9517  loss is :  1.320844\n",
      "Iteration # 9518  loss is :  1.320844\n",
      "Iteration # 9519  loss is :  1.3208439\n",
      "Iteration # 9520  loss is :  1.3208438\n",
      "Iteration # 9521  loss is :  1.3208438\n",
      "Iteration # 9522  loss is :  1.3208437\n",
      "Iteration # 9523  loss is :  1.3208437\n",
      "Iteration # 9524  loss is :  1.3208437\n",
      "Iteration # 9525  loss is :  1.3208436\n",
      "Iteration # 9526  loss is :  1.3208437\n",
      "Iteration # 9527  loss is :  1.3208436\n",
      "Iteration # 9528  loss is :  1.3208436\n",
      "Iteration # 9529  loss is :  1.3208435\n",
      "Iteration # 9530  loss is :  1.3208435\n",
      "Iteration # 9531  loss is :  1.3208433\n",
      "Iteration # 9532  loss is :  1.3208432\n",
      "Iteration # 9533  loss is :  1.320843\n",
      "Iteration # 9534  loss is :  1.3208431\n",
      "Iteration # 9535  loss is :  1.320843\n",
      "Iteration # 9536  loss is :  1.320843\n",
      "Iteration # 9537  loss is :  1.320843\n",
      "Iteration # 9538  loss is :  1.320843\n",
      "Iteration # 9539  loss is :  1.3208429\n",
      "Iteration # 9540  loss is :  1.3208427\n",
      "Iteration # 9541  loss is :  1.3208429\n",
      "Iteration # 9542  loss is :  1.3208427\n",
      "Iteration # 9543  loss is :  1.3208427\n",
      "Iteration # 9544  loss is :  1.3208426\n",
      "Iteration # 9545  loss is :  1.3208424\n",
      "Iteration # 9546  loss is :  1.3208424\n",
      "Iteration # 9547  loss is :  1.3208424\n",
      "Iteration # 9548  loss is :  1.3208423\n",
      "Iteration # 9549  loss is :  1.3208421\n",
      "Iteration # 9550  loss is :  1.3208421\n",
      "Iteration # 9551  loss is :  1.3208421\n",
      "Iteration # 9552  loss is :  1.3208421\n",
      "Iteration # 9553  loss is :  1.3208421\n",
      "Iteration # 9554  loss is :  1.320842\n",
      "Iteration # 9555  loss is :  1.3208419\n",
      "Iteration # 9556  loss is :  1.3208419\n",
      "Iteration # 9557  loss is :  1.3208418\n",
      "Iteration # 9558  loss is :  1.3208418\n",
      "Iteration # 9559  loss is :  1.3208417\n",
      "Iteration # 9560  loss is :  1.3208417\n",
      "Iteration # 9561  loss is :  1.3208417\n",
      "Iteration # 9562  loss is :  1.3208417\n",
      "Iteration # 9563  loss is :  1.3208416\n",
      "Iteration # 9564  loss is :  1.3208414\n",
      "Iteration # 9565  loss is :  1.3208414\n",
      "Iteration # 9566  loss is :  1.3208414\n",
      "Iteration # 9567  loss is :  1.3208414\n",
      "Iteration # 9568  loss is :  1.3208413\n",
      "Iteration # 9569  loss is :  1.3208412\n",
      "Iteration # 9570  loss is :  1.3208411\n",
      "Iteration # 9571  loss is :  1.3208411\n",
      "Iteration # 9572  loss is :  1.3208411\n",
      "Iteration # 9573  loss is :  1.3208408\n",
      "Iteration # 9574  loss is :  1.3208408\n",
      "Iteration # 9575  loss is :  1.3208408\n",
      "Iteration # 9576  loss is :  1.3208408\n",
      "Iteration # 9577  loss is :  1.3208408\n",
      "Iteration # 9578  loss is :  1.3208407\n",
      "Iteration # 9579  loss is :  1.3208406\n",
      "Iteration # 9580  loss is :  1.3208406\n",
      "Iteration # 9581  loss is :  1.3208406\n",
      "Iteration # 9582  loss is :  1.3208405\n",
      "Iteration # 9583  loss is :  1.3208404\n",
      "Iteration # 9584  loss is :  1.3208402\n",
      "Iteration # 9585  loss is :  1.3208402\n",
      "Iteration # 9586  loss is :  1.3208402\n",
      "Iteration # 9587  loss is :  1.3208401\n",
      "Iteration # 9588  loss is :  1.3208401\n",
      "Iteration # 9589  loss is :  1.32084\n",
      "Iteration # 9590  loss is :  1.32084\n",
      "Iteration # 9591  loss is :  1.32084\n",
      "Iteration # 9592  loss is :  1.3208399\n",
      "Iteration # 9593  loss is :  1.3208399\n",
      "Iteration # 9594  loss is :  1.3208398\n",
      "Iteration # 9595  loss is :  1.3208398\n",
      "Iteration # 9596  loss is :  1.3208396\n",
      "Iteration # 9597  loss is :  1.3208395\n",
      "Iteration # 9598  loss is :  1.3208395\n",
      "Iteration # 9599  loss is :  1.3208395\n",
      "Iteration # 9600  loss is :  1.3208395\n",
      "Iteration # 9601  loss is :  1.3208395\n",
      "Iteration # 9602  loss is :  1.3208394\n",
      "Iteration # 9603  loss is :  1.3208394\n",
      "Iteration # 9604  loss is :  1.3208393\n",
      "Iteration # 9605  loss is :  1.3208393\n",
      "Iteration # 9606  loss is :  1.320839\n",
      "Iteration # 9607  loss is :  1.320839\n",
      "Iteration # 9608  loss is :  1.3208389\n",
      "Iteration # 9609  loss is :  1.3208389\n",
      "Iteration # 9610  loss is :  1.3208389\n",
      "Iteration # 9611  loss is :  1.3208387\n",
      "Iteration # 9612  loss is :  1.3208387\n",
      "Iteration # 9613  loss is :  1.3208387\n",
      "Iteration # 9614  loss is :  1.3208387\n",
      "Iteration # 9615  loss is :  1.3208387\n",
      "Iteration # 9616  loss is :  1.3208386\n",
      "Iteration # 9617  loss is :  1.3208387\n",
      "Iteration # 9618  loss is :  1.3208383\n",
      "Iteration # 9619  loss is :  1.3208383\n",
      "Iteration # 9620  loss is :  1.3208383\n",
      "Iteration # 9621  loss is :  1.3208383\n",
      "Iteration # 9622  loss is :  1.3208381\n",
      "Iteration # 9623  loss is :  1.3208382\n",
      "Iteration # 9624  loss is :  1.3208381\n",
      "Iteration # 9625  loss is :  1.3208381\n",
      "Iteration # 9626  loss is :  1.320838\n",
      "Iteration # 9627  loss is :  1.320838\n",
      "Iteration # 9628  loss is :  1.320838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 9629  loss is :  1.320838\n",
      "Iteration # 9630  loss is :  1.320838\n",
      "Iteration # 9631  loss is :  1.3208379\n",
      "Iteration # 9632  loss is :  1.3208377\n",
      "Iteration # 9633  loss is :  1.3208376\n",
      "Iteration # 9634  loss is :  1.3208376\n",
      "Iteration # 9635  loss is :  1.3208376\n",
      "Iteration # 9636  loss is :  1.3208376\n",
      "Iteration # 9637  loss is :  1.3208375\n",
      "Iteration # 9638  loss is :  1.3208374\n",
      "Iteration # 9639  loss is :  1.3208374\n",
      "Iteration # 9640  loss is :  1.3208374\n",
      "Iteration # 9641  loss is :  1.3208374\n",
      "Iteration # 9642  loss is :  1.3208373\n",
      "Iteration # 9643  loss is :  1.320837\n",
      "Iteration # 9644  loss is :  1.320837\n",
      "Iteration # 9645  loss is :  1.320837\n",
      "Iteration # 9646  loss is :  1.3208369\n",
      "Iteration # 9647  loss is :  1.3208368\n",
      "Iteration # 9648  loss is :  1.3208369\n",
      "Iteration # 9649  loss is :  1.3208367\n",
      "Iteration # 9650  loss is :  1.3208368\n",
      "Iteration # 9651  loss is :  1.3208365\n",
      "Iteration # 9652  loss is :  1.3208367\n",
      "Iteration # 9653  loss is :  1.3208365\n",
      "Iteration # 9654  loss is :  1.3208365\n",
      "Iteration # 9655  loss is :  1.3208364\n",
      "Iteration # 9656  loss is :  1.3208362\n",
      "Iteration # 9657  loss is :  1.3208362\n",
      "Iteration # 9658  loss is :  1.3208362\n",
      "Iteration # 9659  loss is :  1.3208362\n",
      "Iteration # 9660  loss is :  1.3208361\n",
      "Iteration # 9661  loss is :  1.3208361\n",
      "Iteration # 9662  loss is :  1.3208361\n",
      "Iteration # 9663  loss is :  1.320836\n",
      "Iteration # 9664  loss is :  1.3208361\n",
      "Iteration # 9665  loss is :  1.320836\n",
      "Iteration # 9666  loss is :  1.320836\n",
      "Iteration # 9667  loss is :  1.3208358\n",
      "Iteration # 9668  loss is :  1.3208357\n",
      "Iteration # 9669  loss is :  1.3208356\n",
      "Iteration # 9670  loss is :  1.3208357\n",
      "Iteration # 9671  loss is :  1.3208355\n",
      "Iteration # 9672  loss is :  1.3208356\n",
      "Iteration # 9673  loss is :  1.3208355\n",
      "Iteration # 9674  loss is :  1.3208355\n",
      "Iteration # 9675  loss is :  1.3208355\n",
      "Iteration # 9676  loss is :  1.3208354\n",
      "Iteration # 9677  loss is :  1.3208352\n",
      "Iteration # 9678  loss is :  1.3208352\n",
      "Iteration # 9679  loss is :  1.3208352\n",
      "Iteration # 9680  loss is :  1.3208351\n",
      "Iteration # 9681  loss is :  1.320835\n",
      "Iteration # 9682  loss is :  1.3208349\n",
      "Iteration # 9683  loss is :  1.3208349\n",
      "Iteration # 9684  loss is :  1.3208348\n",
      "Iteration # 9685  loss is :  1.3208348\n",
      "Iteration # 9686  loss is :  1.3208346\n",
      "Iteration # 9687  loss is :  1.3208348\n",
      "Iteration # 9688  loss is :  1.3208346\n",
      "Iteration # 9689  loss is :  1.3208346\n",
      "Iteration # 9690  loss is :  1.3208346\n",
      "Iteration # 9691  loss is :  1.3208345\n",
      "Iteration # 9692  loss is :  1.3208344\n",
      "Iteration # 9693  loss is :  1.3208343\n",
      "Iteration # 9694  loss is :  1.3208343\n",
      "Iteration # 9695  loss is :  1.3208342\n",
      "Iteration # 9696  loss is :  1.3208342\n",
      "Iteration # 9697  loss is :  1.3208342\n",
      "Iteration # 9698  loss is :  1.3208342\n",
      "Iteration # 9699  loss is :  1.3208342\n",
      "Iteration # 9700  loss is :  1.3208342\n",
      "Iteration # 9701  loss is :  1.3208342\n",
      "Iteration # 9702  loss is :  1.3208339\n",
      "Iteration # 9703  loss is :  1.3208339\n",
      "Iteration # 9704  loss is :  1.3208339\n",
      "Iteration # 9705  loss is :  1.3208338\n",
      "Iteration # 9706  loss is :  1.3208336\n",
      "Iteration # 9707  loss is :  1.3208336\n",
      "Iteration # 9708  loss is :  1.3208336\n",
      "Iteration # 9709  loss is :  1.3208336\n",
      "Iteration # 9710  loss is :  1.3208334\n",
      "Iteration # 9711  loss is :  1.3208334\n",
      "Iteration # 9712  loss is :  1.3208333\n",
      "Iteration # 9713  loss is :  1.3208333\n",
      "Iteration # 9714  loss is :  1.3208333\n",
      "Iteration # 9715  loss is :  1.3208332\n",
      "Iteration # 9716  loss is :  1.3208331\n",
      "Iteration # 9717  loss is :  1.3208331\n",
      "Iteration # 9718  loss is :  1.320833\n",
      "Iteration # 9719  loss is :  1.320833\n",
      "Iteration # 9720  loss is :  1.3208328\n",
      "Iteration # 9721  loss is :  1.3208328\n",
      "Iteration # 9722  loss is :  1.3208327\n",
      "Iteration # 9723  loss is :  1.3208328\n",
      "Iteration # 9724  loss is :  1.3208326\n",
      "Iteration # 9725  loss is :  1.3208326\n",
      "Iteration # 9726  loss is :  1.3208326\n",
      "Iteration # 9727  loss is :  1.3208325\n",
      "Iteration # 9728  loss is :  1.3208325\n",
      "Iteration # 9729  loss is :  1.3208324\n",
      "Iteration # 9730  loss is :  1.3208325\n",
      "Iteration # 9731  loss is :  1.3208323\n",
      "Iteration # 9732  loss is :  1.3208323\n",
      "Iteration # 9733  loss is :  1.3208323\n",
      "Iteration # 9734  loss is :  1.3208323\n",
      "Iteration # 9735  loss is :  1.3208323\n",
      "Iteration # 9736  loss is :  1.3208323\n",
      "Iteration # 9737  loss is :  1.3208321\n",
      "Iteration # 9738  loss is :  1.3208321\n",
      "Iteration # 9739  loss is :  1.3208319\n",
      "Iteration # 9740  loss is :  1.320832\n",
      "Iteration # 9741  loss is :  1.3208319\n",
      "Iteration # 9742  loss is :  1.3208319\n",
      "Iteration # 9743  loss is :  1.3208318\n",
      "Iteration # 9744  loss is :  1.3208317\n",
      "Iteration # 9745  loss is :  1.3208315\n",
      "Iteration # 9746  loss is :  1.3208315\n",
      "Iteration # 9747  loss is :  1.3208314\n",
      "Iteration # 9748  loss is :  1.3208314\n",
      "Iteration # 9749  loss is :  1.3208313\n",
      "Iteration # 9750  loss is :  1.3208312\n",
      "Iteration # 9751  loss is :  1.3208313\n",
      "Iteration # 9752  loss is :  1.3208313\n",
      "Iteration # 9753  loss is :  1.3208312\n",
      "Iteration # 9754  loss is :  1.3208312\n",
      "Iteration # 9755  loss is :  1.3208311\n",
      "Iteration # 9756  loss is :  1.3208308\n",
      "Iteration # 9757  loss is :  1.3208308\n",
      "Iteration # 9758  loss is :  1.3208308\n",
      "Iteration # 9759  loss is :  1.3208308\n",
      "Iteration # 9760  loss is :  1.3208307\n",
      "Iteration # 9761  loss is :  1.3208307\n",
      "Iteration # 9762  loss is :  1.3208306\n",
      "Iteration # 9763  loss is :  1.3208306\n",
      "Iteration # 9764  loss is :  1.3208306\n",
      "Iteration # 9765  loss is :  1.3208305\n",
      "Iteration # 9766  loss is :  1.3208305\n",
      "Iteration # 9767  loss is :  1.3208303\n",
      "Iteration # 9768  loss is :  1.3208303\n",
      "Iteration # 9769  loss is :  1.3208303\n",
      "Iteration # 9770  loss is :  1.3208302\n",
      "Iteration # 9771  loss is :  1.3208302\n",
      "Iteration # 9772  loss is :  1.3208301\n",
      "Iteration # 9773  loss is :  1.3208301\n",
      "Iteration # 9774  loss is :  1.3208301\n",
      "Iteration # 9775  loss is :  1.3208301\n",
      "Iteration # 9776  loss is :  1.32083\n",
      "Iteration # 9777  loss is :  1.3208299\n",
      "Iteration # 9778  loss is :  1.3208299\n",
      "Iteration # 9779  loss is :  1.3208299\n",
      "Iteration # 9780  loss is :  1.3208297\n",
      "Iteration # 9781  loss is :  1.3208299\n",
      "Iteration # 9782  loss is :  1.3208296\n",
      "Iteration # 9783  loss is :  1.3208295\n",
      "Iteration # 9784  loss is :  1.3208295\n",
      "Iteration # 9785  loss is :  1.3208294\n",
      "Iteration # 9786  loss is :  1.3208293\n",
      "Iteration # 9787  loss is :  1.3208294\n",
      "Iteration # 9788  loss is :  1.3208293\n",
      "Iteration # 9789  loss is :  1.3208293\n",
      "Iteration # 9790  loss is :  1.3208293\n",
      "Iteration # 9791  loss is :  1.3208292\n",
      "Iteration # 9792  loss is :  1.320829\n",
      "Iteration # 9793  loss is :  1.320829\n",
      "Iteration # 9794  loss is :  1.320829\n",
      "Iteration # 9795  loss is :  1.3208289\n",
      "Iteration # 9796  loss is :  1.3208287\n",
      "Iteration # 9797  loss is :  1.3208289\n",
      "Iteration # 9798  loss is :  1.3208287\n",
      "Iteration # 9799  loss is :  1.3208287\n",
      "Iteration # 9800  loss is :  1.3208287\n",
      "Iteration # 9801  loss is :  1.3208287\n",
      "Iteration # 9802  loss is :  1.3208286\n",
      "Iteration # 9803  loss is :  1.3208284\n",
      "Iteration # 9804  loss is :  1.3208284\n",
      "Iteration # 9805  loss is :  1.3208284\n",
      "Iteration # 9806  loss is :  1.3208284\n",
      "Iteration # 9807  loss is :  1.3208283\n",
      "Iteration # 9808  loss is :  1.3208282\n",
      "Iteration # 9809  loss is :  1.3208282\n",
      "Iteration # 9810  loss is :  1.3208282\n",
      "Iteration # 9811  loss is :  1.3208281\n",
      "Iteration # 9812  loss is :  1.320828\n",
      "Iteration # 9813  loss is :  1.3208281\n",
      "Iteration # 9814  loss is :  1.320828\n",
      "Iteration # 9815  loss is :  1.320828\n",
      "Iteration # 9816  loss is :  1.320828\n",
      "Iteration # 9817  loss is :  1.3208278\n",
      "Iteration # 9818  loss is :  1.3208278\n",
      "Iteration # 9819  loss is :  1.3208278\n",
      "Iteration # 9820  loss is :  1.3208276\n",
      "Iteration # 9821  loss is :  1.3208276\n",
      "Iteration # 9822  loss is :  1.3208276\n",
      "Iteration # 9823  loss is :  1.3208274\n",
      "Iteration # 9824  loss is :  1.3208274\n",
      "Iteration # 9825  loss is :  1.3208274\n",
      "Iteration # 9826  loss is :  1.3208274\n",
      "Iteration # 9827  loss is :  1.3208272\n",
      "Iteration # 9828  loss is :  1.3208271\n",
      "Iteration # 9829  loss is :  1.3208271\n",
      "Iteration # 9830  loss is :  1.3208271\n",
      "Iteration # 9831  loss is :  1.320827\n",
      "Iteration # 9832  loss is :  1.320827\n",
      "Iteration # 9833  loss is :  1.320827\n",
      "Iteration # 9834  loss is :  1.3208268\n",
      "Iteration # 9835  loss is :  1.3208268\n",
      "Iteration # 9836  loss is :  1.3208268\n",
      "Iteration # 9837  loss is :  1.3208268\n",
      "Iteration # 9838  loss is :  1.3208268\n",
      "Iteration # 9839  loss is :  1.3208265\n",
      "Iteration # 9840  loss is :  1.3208266\n",
      "Iteration # 9841  loss is :  1.3208265\n",
      "Iteration # 9842  loss is :  1.3208265\n",
      "Iteration # 9843  loss is :  1.3208264\n",
      "Iteration # 9844  loss is :  1.3208264\n",
      "Iteration # 9845  loss is :  1.3208264\n",
      "Iteration # 9846  loss is :  1.3208263\n",
      "Iteration # 9847  loss is :  1.3208262\n",
      "Iteration # 9848  loss is :  1.3208262\n",
      "Iteration # 9849  loss is :  1.3208262\n",
      "Iteration # 9850  loss is :  1.320826\n",
      "Iteration # 9851  loss is :  1.320826\n",
      "Iteration # 9852  loss is :  1.320826\n",
      "Iteration # 9853  loss is :  1.3208259\n",
      "Iteration # 9854  loss is :  1.3208258\n",
      "Iteration # 9855  loss is :  1.3208258\n",
      "Iteration # 9856  loss is :  1.3208257\n",
      "Iteration # 9857  loss is :  1.3208257\n",
      "Iteration # 9858  loss is :  1.3208256\n",
      "Iteration # 9859  loss is :  1.3208256\n",
      "Iteration # 9860  loss is :  1.3208255\n",
      "Iteration # 9861  loss is :  1.3208255\n",
      "Iteration # 9862  loss is :  1.3208255\n",
      "Iteration # 9863  loss is :  1.3208253\n",
      "Iteration # 9864  loss is :  1.3208252\n",
      "Iteration # 9865  loss is :  1.3208252\n",
      "Iteration # 9866  loss is :  1.3208252\n",
      "Iteration # 9867  loss is :  1.3208252\n",
      "Iteration # 9868  loss is :  1.3208252\n",
      "Iteration # 9869  loss is :  1.3208251\n",
      "Iteration # 9870  loss is :  1.3208251\n",
      "Iteration # 9871  loss is :  1.320825\n",
      "Iteration # 9872  loss is :  1.320825\n",
      "Iteration # 9873  loss is :  1.3208247\n",
      "Iteration # 9874  loss is :  1.3208246\n",
      "Iteration # 9875  loss is :  1.3208247\n",
      "Iteration # 9876  loss is :  1.3208246\n",
      "Iteration # 9877  loss is :  1.3208246\n",
      "Iteration # 9878  loss is :  1.3208246\n",
      "Iteration # 9879  loss is :  1.3208246\n",
      "Iteration # 9880  loss is :  1.3208246\n",
      "Iteration # 9881  loss is :  1.3208245\n",
      "Iteration # 9882  loss is :  1.3208245\n",
      "Iteration # 9883  loss is :  1.3208245\n",
      "Iteration # 9884  loss is :  1.3208244\n",
      "Iteration # 9885  loss is :  1.3208244\n",
      "Iteration # 9886  loss is :  1.3208241\n",
      "Iteration # 9887  loss is :  1.3208241\n",
      "Iteration # 9888  loss is :  1.3208241\n",
      "Iteration # 9889  loss is :  1.3208241\n",
      "Iteration # 9890  loss is :  1.3208241\n",
      "Iteration # 9891  loss is :  1.3208239\n",
      "Iteration # 9892  loss is :  1.3208239\n",
      "Iteration # 9893  loss is :  1.3208239\n",
      "Iteration # 9894  loss is :  1.3208238\n",
      "Iteration # 9895  loss is :  1.3208237\n",
      "Iteration # 9896  loss is :  1.3208237\n",
      "Iteration # 9897  loss is :  1.3208237\n",
      "Iteration # 9898  loss is :  1.3208237\n",
      "Iteration # 9899  loss is :  1.3208234\n",
      "Iteration # 9900  loss is :  1.3208234\n",
      "Iteration # 9901  loss is :  1.3208234\n",
      "Iteration # 9902  loss is :  1.3208233\n",
      "Iteration # 9903  loss is :  1.3208233\n",
      "Iteration # 9904  loss is :  1.3208233\n",
      "Iteration # 9905  loss is :  1.3208233\n",
      "Iteration # 9906  loss is :  1.3208232\n",
      "Iteration # 9907  loss is :  1.3208231\n",
      "Iteration # 9908  loss is :  1.3208231\n",
      "Iteration # 9909  loss is :  1.3208231\n",
      "Iteration # 9910  loss is :  1.3208231\n",
      "Iteration # 9911  loss is :  1.320823\n",
      "Iteration # 9912  loss is :  1.3208227\n",
      "Iteration # 9913  loss is :  1.3208227\n",
      "Iteration # 9914  loss is :  1.3208227\n",
      "Iteration # 9915  loss is :  1.3208227\n",
      "Iteration # 9916  loss is :  1.3208227\n",
      "Iteration # 9917  loss is :  1.3208226\n",
      "Iteration # 9918  loss is :  1.3208226\n",
      "Iteration # 9919  loss is :  1.3208226\n",
      "Iteration # 9920  loss is :  1.3208225\n",
      "Iteration # 9921  loss is :  1.3208224\n",
      "Iteration # 9922  loss is :  1.3208225\n",
      "Iteration # 9923  loss is :  1.3208224\n",
      "Iteration # 9924  loss is :  1.3208224\n",
      "Iteration # 9925  loss is :  1.3208222\n",
      "Iteration # 9926  loss is :  1.3208222\n",
      "Iteration # 9927  loss is :  1.3208221\n",
      "Iteration # 9928  loss is :  1.320822\n",
      "Iteration # 9929  loss is :  1.320822\n",
      "Iteration # 9930  loss is :  1.320822\n",
      "Iteration # 9931  loss is :  1.3208219\n",
      "Iteration # 9932  loss is :  1.3208219\n",
      "Iteration # 9933  loss is :  1.3208219\n",
      "Iteration # 9934  loss is :  1.3208218\n",
      "Iteration # 9935  loss is :  1.3208218\n",
      "Iteration # 9936  loss is :  1.3208218\n",
      "Iteration # 9937  loss is :  1.3208216\n",
      "Iteration # 9938  loss is :  1.3208216\n",
      "Iteration # 9939  loss is :  1.3208214\n",
      "Iteration # 9940  loss is :  1.3208214\n",
      "Iteration # 9941  loss is :  1.3208214\n",
      "Iteration # 9942  loss is :  1.3208212\n",
      "Iteration # 9943  loss is :  1.3208213\n",
      "Iteration # 9944  loss is :  1.3208212\n",
      "Iteration # 9945  loss is :  1.3208212\n",
      "Iteration # 9946  loss is :  1.320821\n",
      "Iteration # 9947  loss is :  1.320821\n",
      "Iteration # 9948  loss is :  1.320821\n",
      "Iteration # 9949  loss is :  1.3208209\n",
      "Iteration # 9950  loss is :  1.3208209\n",
      "Iteration # 9951  loss is :  1.3208208\n",
      "Iteration # 9952  loss is :  1.3208208\n",
      "Iteration # 9953  loss is :  1.3208208\n",
      "Iteration # 9954  loss is :  1.3208208\n",
      "Iteration # 9955  loss is :  1.3208207\n",
      "Iteration # 9956  loss is :  1.3208207\n",
      "Iteration # 9957  loss is :  1.3208206\n",
      "Iteration # 9958  loss is :  1.3208206\n",
      "Iteration # 9959  loss is :  1.3208206\n",
      "Iteration # 9960  loss is :  1.3208205\n",
      "Iteration # 9961  loss is :  1.3208205\n",
      "Iteration # 9962  loss is :  1.3208205\n",
      "Iteration # 9963  loss is :  1.3208205\n",
      "Iteration # 9964  loss is :  1.3208203\n",
      "Iteration # 9965  loss is :  1.3208201\n",
      "Iteration # 9966  loss is :  1.3208201\n",
      "Iteration # 9967  loss is :  1.3208201\n",
      "Iteration # 9968  loss is :  1.32082\n",
      "Iteration # 9969  loss is :  1.3208199\n",
      "Iteration # 9970  loss is :  1.32082\n",
      "Iteration # 9971  loss is :  1.3208199\n",
      "Iteration # 9972  loss is :  1.3208199\n",
      "Iteration # 9973  loss is :  1.3208197\n",
      "Iteration # 9974  loss is :  1.3208197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 9975  loss is :  1.3208196\n",
      "Iteration # 9976  loss is :  1.3208196\n",
      "Iteration # 9977  loss is :  1.3208196\n",
      "Iteration # 9978  loss is :  1.3208196\n",
      "Iteration # 9979  loss is :  1.3208195\n",
      "Iteration # 9980  loss is :  1.3208194\n",
      "Iteration # 9981  loss is :  1.3208193\n",
      "Iteration # 9982  loss is :  1.3208193\n",
      "Iteration # 9983  loss is :  1.3208193\n",
      "Iteration # 9984  loss is :  1.3208191\n",
      "Iteration # 9985  loss is :  1.3208191\n",
      "Iteration # 9986  loss is :  1.320819\n",
      "Iteration # 9987  loss is :  1.320819\n",
      "Iteration # 9988  loss is :  1.320819\n",
      "Iteration # 9989  loss is :  1.320819\n",
      "Iteration # 9990  loss is :  1.3208189\n",
      "Iteration # 9991  loss is :  1.320819\n",
      "Iteration # 9992  loss is :  1.3208188\n",
      "Iteration # 9993  loss is :  1.3208188\n",
      "Iteration # 9994  loss is :  1.3208188\n",
      "Iteration # 9995  loss is :  1.3208188\n",
      "Iteration # 9996  loss is :  1.3208187\n",
      "Iteration # 9997  loss is :  1.3208185\n",
      "Iteration # 9998  loss is :  1.3208185\n",
      "Iteration # 9999  loss is :  1.3208185\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "# define the loss function:\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n",
    "# define the training step:\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
    "n_iters = 10000\n",
    "# train for n_iter iterations\n",
    "for i in range(n_iters):\n",
    "    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n",
    "    print('Iteration #', i, ' loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.424688   -1.9252938  -0.6388043   0.10906665  0.25840887]\n",
      " [-1.9066083  -0.10446653 -0.81790906  2.3199577  -0.54783213]\n",
      " [-1.0711946  -0.53016156  1.6015096  -2.514792    1.2271364 ]\n",
      " [-0.16368645 -0.42250228  2.1198182   0.35810795 -1.1576419 ]\n",
      " [ 0.6936929  -1.723447   -0.33726034  0.49052453  0.11122748]\n",
      " [ 1.9086014  -0.91337377  1.142567    1.4991777   0.5279376 ]\n",
      " [ 1.0896218   2.026247   -0.2336624  -0.6645414   1.7162611 ]]\n",
      "[ 0.23853897 -0.89132506 -0.04109282  0.23112632  1.668236  ]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(W1))\n",
    "print(sess.run(b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.66322696 -2.816619   -0.6798971   0.34019297  1.9266449 ]\n",
      " [-1.6680694  -0.99579155 -0.8590019   2.551084    1.1204039 ]\n",
      " [-0.83265567 -1.4214866   1.5604167  -2.2836657   2.8953724 ]\n",
      " [ 0.07485251 -1.3138273   2.0787253   0.5892343   0.5105941 ]\n",
      " [ 0.9322319  -2.614772   -0.37835315  0.72165084  1.7794635 ]\n",
      " [ 2.1471403  -1.8046988   1.1014742   1.730304    2.1961737 ]\n",
      " [ 1.3281608   1.134922   -0.2747552  -0.4334151   3.3844972 ]]\n"
     ]
    }
   ],
   "source": [
    "vectors = sess.run(W1 + b1)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07485251 -1.3138273   2.0787253   0.5892343   0.5105941 ]\n"
     ]
    }
   ],
   "source": [
    "print(vectors[ word2int['queen'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
    "\n",
    "def find_closest(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity\n",
    "    min_index = -1\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n"
     ]
    }
   ],
   "source": [
    "print(int2word[find_closest(word2int['queen'], vectors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
