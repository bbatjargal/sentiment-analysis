{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    def backward(self, x, top_diff):\n",
    "        output = self.forward(x)\n",
    "        return (1.0 - output) * output * top_diff\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "    def backward(self, x, top_diff):\n",
    "        output = self.forward(x)\n",
    "        return (1.0 - np.square(output)) * top_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplyGate:\n",
    "    def forward(self,W, x):\n",
    "        return np.dot(W, x)\n",
    "    def backward(self, W, x, dz):\n",
    "        dW = np.asarray(np.dot(np.transpose(np.asmatrix(dz)), np.asmatrix(x)))\n",
    "        dx = np.dot(np.transpose(W), dz)\n",
    "        return dW, dx\n",
    "\n",
    "class AddGate:\n",
    "    def forward(self, x1, x2):\n",
    "        return x1 + x2\n",
    "    def backward(self, x1, x2, dz):\n",
    "        dx1 = dz * np.ones_like(x1)\n",
    "        dx2 = dz * np.ones_like(x2)\n",
    "        return dx1, dx2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mulGate = MultiplyGate()\n",
    "addGate = AddGate()\n",
    "activation = Tanh()\n",
    "\n",
    "class RNNLayer:\n",
    "    def forward(self, x, prev_s, U, W, V):\n",
    "        self.mulu = mulGate.forward(U, x)\n",
    "        self.mulw = mulGate.forward(W, prev_s)\n",
    "        self.add = addGate.forward(self.mulw, self.mulu)\n",
    "        self.s = activation.forward(self.add)\n",
    "        self.mulv = mulGate.forward(V, self.s)\n",
    "        \n",
    "    def backward(self, x, prev_s, U, W, V, diff_s, dmulv):\n",
    "        self.forward(x, prev_s, U, W, V)\n",
    "        dV, dsv = mulGate.backward(V, self.s, dmulv)\n",
    "        ds = dsv + diff_s\n",
    "        dadd = activation.backward(self.add, ds)\n",
    "        dmulw, dmulu = addGate.backward(self.mulw, self.mulu, dadd)\n",
    "        dW, dprev_s = mulGate.backward(W, prev_s, dmulw)\n",
    "        dU, dx = mulGate.backward(U, x, dmulu)\n",
    "        return (dprev_s, dU, dW, dV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def predict(self, x):\n",
    "        exp_scores = np.exp(x)\n",
    "        return exp_scores / np.sum(exp_scores)\n",
    "    def loss(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        return -np.log(probs[y])\n",
    "    def diff(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        probs[y] -= 1.0\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reccurrent Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        self.U = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, word_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, hidden_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (word_dim, hidden_dim))\n",
    "\n",
    "    '''\n",
    "        forward propagation (predicting word probabilities)\n",
    "        x is one single data, and a batch of data\n",
    "        for example x = [0, 179, 341, 416], then its y = [179, 341, 416, 1]\n",
    "    '''\n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        layers = []\n",
    "        prev_s = np.zeros(self.hidden_dim)\n",
    "        # For each time step...\n",
    "        for t in range(T):\n",
    "            layer = RNNLayer()\n",
    "            input = np.zeros(self.word_dim)\n",
    "            input[x[t]] = 1\n",
    "            layer.forward(input, prev_s, self.U, self.W, self.V)\n",
    "            prev_s = layer.s\n",
    "            layers.append(layer)\n",
    "        return layers\n",
    "\n",
    "    def predict(self, x):\n",
    "        output = Softmax()\n",
    "        layers = self.forward_propagation(x)\n",
    "        return [np.argmax(output.predict(layer.mulv)) for layer in layers]\n",
    "\n",
    "    def calculate_loss(self, x, y):\n",
    "        assert len(x) == len(y)\n",
    "        output = Softmax()\n",
    "        layers = self.forward_propagation(x)\n",
    "        loss = 0.0\n",
    "        for i, layer in enumerate(layers):\n",
    "            loss += output.loss(layer.mulv, y[i])\n",
    "        return loss / float(len(y))\n",
    "\n",
    "    def calculate_total_loss(self, X, Y):\n",
    "        loss = 0.0\n",
    "        for i in range(len(Y)):\n",
    "            loss += self.calculate_loss(X[i], Y[i])\n",
    "        return loss / float(len(Y))\n",
    "\n",
    "    def bptt(self, x, y):\n",
    "        assert len(x) == len(y)\n",
    "        output = Softmax()\n",
    "        layers = self.forward_propagation(x)\n",
    "        dU = np.zeros(self.U.shape)\n",
    "        dV = np.zeros(self.V.shape)\n",
    "        dW = np.zeros(self.W.shape)\n",
    "\n",
    "        T = len(layers)\n",
    "        prev_s_t = np.zeros(self.hidden_dim)\n",
    "        diff_s = np.zeros(self.hidden_dim)\n",
    "        for t in range(0, T):\n",
    "            dmulv = output.diff(layers[t].mulv, y[t])\n",
    "            input = np.zeros(self.word_dim)\n",
    "            input[x[t]] = 1\n",
    "            dprev_s, dU_t, dW_t, dV_t = layers[t].backward(input, prev_s_t, self.U, self.W, self.V, diff_s, dmulv)\n",
    "            prev_s_t = layers[t].s\n",
    "            dmulv = np.zeros(self.word_dim)\n",
    "            for i in range(t-1, max(-1, t-self.bptt_truncate-1), -1):\n",
    "                input = np.zeros(self.word_dim)\n",
    "                input[x[i]] = 1\n",
    "                prev_s_i = np.zeros(self.hidden_dim) if i == 0 else layers[i-1].s\n",
    "                dprev_s, dU_i, dW_i, dV_i = layers[i].backward(input, prev_s_i, self.U, self.W, self.V, dprev_s, dmulv)\n",
    "                dU_t += dU_i\n",
    "                dW_t += dW_i\n",
    "            dV += dV_t\n",
    "            dU += dU_t\n",
    "            dW += dW_t\n",
    "        return (dU, dW, dV)\n",
    "\n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        dU, dW, dV = self.bptt(x, y)\n",
    "        self.U -= learning_rate * dU\n",
    "        self.V -= learning_rate * dV\n",
    "        self.W -= learning_rate * dW\n",
    "\n",
    "    def train(self, X, Y, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "        num_examples_seen = 0\n",
    "        losses = []\n",
    "        for epoch in range(nepoch):\n",
    "            if (epoch % evaluate_loss_after == 0):\n",
    "                loss = self.calculate_total_loss(X, Y)\n",
    "                losses.append((num_examples_seen, loss))\n",
    "                time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "                # Adjust the learning rate if loss increases\n",
    "                if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
    "                    learning_rate = learning_rate * 0.5\n",
    "                    print(\"Setting learning rate to %f\" % learning_rate)\n",
    "                sys.stdout.flush()\n",
    "            # For each training example...\n",
    "            for i in range(len(Y)):\n",
    "                self.sgd_step(X[i], Y[i], learning_rate)\n",
    "                num_examples_seen += 1\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-31bde3cd8591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m#    X_train, y_train = getSentenceData('data/reddit-comments-2015-08.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import itertools\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def getSentenceData(path, vocabulary_size=8000):\n",
    "    unknown_token = \"UNKNOWN_TOKEN\"\n",
    "    sentence_start_token = \"SENTENCE_START\"\n",
    "    sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "    # Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "    print(\"Reading CSV file...\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, skipinitialspace=True)\n",
    "        # Split full comments into sentences\n",
    "        sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "        # Append SENTENCE_START and SENTENCE_END\n",
    "        sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "    print(\"Parsed %d sentences.\" % (len(sentences)))\n",
    "\n",
    "    # Tokenize the sentences into words\n",
    "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    # Filter the sentences having few words (including SENTENCE_START and SENTENCE_END)\n",
    "    tokenized_sentences = list(filter(lambda x: len(x) > 3, tokenized_sentences))\n",
    "\n",
    "    # Count the word frequencies\n",
    "    word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "    # Get the most common words and build index_to_word and word_to_index vectors\n",
    "    vocab = word_freq.most_common(vocabulary_size-1)\n",
    "    index_to_word = [x[0] for x in vocab]\n",
    "    index_to_word.append(unknown_token)\n",
    "    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "    print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "    print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "    # Replace all words not in our vocabulary with the unknown token\n",
    "    for i, sent in enumerate(tokenized_sentences):\n",
    "        tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "    print(\"\\nExample sentence: '%s'\" % sentences[1])\n",
    "    print(\"\\nExample sentence after Pre-processing: '%s'\\n\" % tokenized_sentences[0])\n",
    "\n",
    "    # Create the training data\n",
    "    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "\n",
    "    print(\"X_train shape: \" + str(X_train.shape))\n",
    "    print(\"y_train shape: \" + str(y_train.shape))\n",
    "\n",
    "    # Print an training data example\n",
    "    x_example, y_example = X_train[17], y_train[17]\n",
    "    print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "    print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    X_train, y_train = getSentenceData('data/reddit-comments-2015-08.csv')\n",
    "\n",
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN language model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 63 sentences.\n",
      "Found 438 unique words tokens.\n",
      "Using vocabulary size 80.\n",
      "The least frequent word in our vocabulary is 'even' and appeared 3 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'UNKNOWN_TOKEN', 'a', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'this', 'UNKNOWN_TOKEN', 'and', 'they', 'have', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'than', 'i', \"'m\", 'UNKNOWN_TOKEN', 'to', '.', 'SENTENCE_END']'\n",
      "\n",
      "X_train shape: (62,)\n",
      "y_train shape: (62,)\n",
      "x:\n",
      "SENTENCE_START what UNKNOWN_TOKEN n't you UNKNOWN_TOKEN UNKNOWN_TOKEN this ? UNKNOWN_TOKEN\n",
      "[0, 36, 79, 16, 9, 79, 79, 30, 23, 79]\n",
      "\n",
      "y:\n",
      "what UNKNOWN_TOKEN n't you UNKNOWN_TOKEN UNKNOWN_TOKEN this ? UNKNOWN_TOKEN SENTENCE_END\n",
      "[36, 79, 16, 9, 79, 79, 30, 23, 79, 1]\n",
      "[list([0, 10, 79, 5, 79, 79, 30, 79, 11, 12, 41, 79, 79, 79, 42, 10, 31, 79, 3, 2])\n",
      " list([0, 7, 13, 5, 79, 79, 79, 79, 79, 2])\n",
      " list([0, 79, 79, 79, 32, 18, 79, 79, 79, 6, 79, 32, 79, 79, 6, 79, 32, 79, 79, 79, 6, 11, 60, 79, 18, 79, 79, 2])\n",
      " list([0, 43, 79, 14, 6, 14, 7, 79, 79, 15, 79, 44, 4, 79, 79, 18, 32, 23])\n",
      " list([0, 10, 61, 8, 4, 79, 45, 5, 79, 79, 11, 79, 15, 62, 79, 79, 79, 32, 46, 42, 4, 62, 79, 2])\n",
      " list([0, 24, 7, 19, 79, 17, 3, 79, 5, 79, 8, 4, 63, 79, 23])\n",
      " list([0, 8, 33, 79, 6, 5, 79, 79, 47, 17, 48, 4, 64, 49, 21, 65, 4, 79, 11, 66, 67, 4, 25, 3, 4, 68, 2])\n",
      " list([0, 50, 13, 34, 79, 3, 79, 7, 2])\n",
      " list([0, 26, 79, 79, 14, 35, 3, 17, 67, 4, 25, 3, 69, 70, 12, 79, 12, 71, 5, 68, 65, 4, 49, 21, 2])\n",
      " list([0, 5, 79, 79, 14, 16, 35, 3, 48, 4, 21, 8, 4, 63, 79, 2])\n",
      " list([0, 34, 72, 14, 35, 3, 19, 79, 79, 3, 48, 4, 21, 6, 79, 12, 71, 5, 68, 6, 11, 79, 79, 20, 5, 79, 79, 79, 79, 11, 79, 79, 79, 6, 9, 79, 36, 6, 79, 13, 33, 25, 79, 2, 51])\n",
      " list([0, 12, 24, 16, 48, 4, 73, 21, 8, 4, 63, 79, 6, 79, 2])\n",
      " list([0, 33, 79, 79, 14, 79, 79, 2])\n",
      " list([0, 30, 14, 79, 37, 79, 52, 53, 54, 79, 13, 26, 79, 6, 79, 17, 38, 5, 49, 21, 79, 79, 7, 24, 19, 79, 3, 67, 79, 3, 79, 23])\n",
      " list([0, 15, 74, 16, 79, 9, 23]) list([0, 79, 2])\n",
      " list([0, 79, 79, 73, 41, 15, 2])\n",
      " list([0, 36, 79, 16, 9, 79, 79, 30, 23, 79])\n",
      " list([0, 7, 13, 47, 79, 17, 79, 3, 79, 79, 2])\n",
      " list([0, 10, 79, 22, 3, 38, 7, 79, 6, 39, 50, 13, 34, 79, 8, 79, 7, 64, 2])\n",
      " list([0, 52, 53, 54, 9, 71, 47, 79, 7, 3, 38, 5, 79, 79, 49, 21, 79, 8, 79, 79, 2])\n",
      " list([0, 55, 5, 25, 79, 56, 44, 79, 34, 72, 11, 44, 79, 79, 79, 6, 10, 79, 27, 16, 79, 4, 79, 79, 5, 79, 79, 2])\n",
      " list([0, 7, 13, 37, 22, 9, 15, 79, 79, 3, 17, 79, 26, 79, 45, 79, 2])\n",
      " list([0, 10, 31, 79, 3, 79, 79, 3, 79, 79, 55, 79, 2])\n",
      " list([0, 79, 7, 40, 79, 7, 6, 79, 7, 14, 16, 35, 3, 19, 79, 64, 2])\n",
      " list([0, 17, 79, 79, 79, 2]) list([0, 79, 2])\n",
      " list([0, 12, 27, 16, 75, 79, 18, 79, 79, 79, 9, 79, 79, 33, 79, 2])\n",
      " list([0, 12, 75, 79, 70, 9, 79, 7, 2])\n",
      " list([0, 76, 5, 79, 79, 79, 36, 6, 79, 79, 23])\n",
      " list([0, 79, 15, 79, 12, 79, 79, 79, 79, 11, 66, 41, 4, 79, 3, 38, 79, 79, 79, 9, 79, 3, 79, 7, 2])\n",
      " list([0, 10, 79, 6, 79, 57, 4, 79, 79, 8, 26, 79, 79, 18, 79, 79, 8, 79, 79, 6, 10, 79, 4, 79, 79, 79, 79, 28, 29, 8, 5, 79, 79, 79, 2])\n",
      " list([0, 11, 43, 79, 79, 79, 79, 29, 79, 11, 79, 2])\n",
      " list([0, 79, 79, 29, 2]) list([0, 29, 6, 29, 73, 79, 2])\n",
      " list([0, 7, 79, 79, 76, 3, 79, 72, 79, 4, 77, 2])\n",
      " list([0, 7, 79, 79, 6, 11, 60, 20, 69, 78, 79, 79, 79, 3, 79, 78, 46, 79, 29, 2])\n",
      " list([0, 10, 79, 10, 66, 41, 79, 20, 15, 79, 79, 2])\n",
      " list([0, 15, 79, 79, 79, 22, 5, 79, 79, 79, 2])\n",
      " list([0, 34, 6, 39, 79, 79, 24, 79, 79, 55, 5, 79, 79, 2])\n",
      " list([0, 79, 79, 5, 79, 79, 18, 5, 79, 79, 79, 79, 79, 8, 4, 79, 79, 57, 9, 79, 79, 79, 79, 79, 2])\n",
      " list([0, 79, 79, 44, 26, 79, 79, 28, 79, 2])\n",
      " list([0, 79, 60, 79, 20, 79, 18, 79, 11, 79, 5, 79, 79, 3, 79, 79, 79, 58, 2])\n",
      " list([0, 79, 79, 79, 3, 79, 5, 79, 79, 65, 79, 79, 42, 69, 8, 4, 79, 79, 2])\n",
      " list([0, 30, 79, 4, 79, 20, 5, 79, 58, 79, 79, 55, 26, 79, 62, 58, 79, 79, 8, 11, 79, 79, 4, 79, 39, 74, 16, 79, 79, 18, 79, 2])\n",
      " list([0, 79, 6, 7, 79, 74, 16, 79, 79, 4, 79, 58, 79, 40, 79, 79, 79, 8, 79, 79, 2])\n",
      " list([0, 10, 79, 79, 79, 22, 30, 79, 9, 79, 22, 79, 5, 79, 57, 40, 79, 9, 79, 79, 79, 6, 11, 33, 79, 79, 9, 18, 79, 2])\n",
      " list([0, 36, 10, 24, 27, 3, 19, 28, 79, 22, 9, 2])\n",
      " list([0, 10, 31, 17, 35, 3, 79, 9, 3, 79, 40, 79, 39, 79, 9, 79, 6, 47, 27, 16, 79, 79, 20, 79, 9, 79, 3, 19, 79, 11, 27, 16, 61, 79, 28, 79, 79, 2])\n",
      " list([0, 52, 53, 54, 79, 79, 79, 79, 79, 3, 4, 79, 13, 79, 57, 79, 79, 3, 79, 79, 6, 79, 4, 79, 2])\n",
      " list([0, 79, 4, 79, 79, 79, 52, 53, 54, 51, 36, 79, 79, 3, 43, 79, 14, 79, 2])\n",
      " list([0, 4, 79, 15, 79, 79, 59, 79, 8, 79, 20, 59, 79, 79, 14, 78, 46, 79, 6, 51, 4, 79, 79, 2])\n",
      " list([0, 79, 79, 79, 56, 79, 5, 79, 79, 3, 19, 8, 79, 2])\n",
      " list([0, 79, 79, 59, 79, 2, 51])\n",
      " list([0, 7, 79, 22, 12, 79, 79, 79, 15, 12, 79, 59, 3, 79, 2])\n",
      " list([0, 10, 79, 12, 27, 2])\n",
      " list([0, 79, 15, 79, 79, 79, 4, 79, 79, 3, 79, 79, 79, 3, 79, 79, 79, 79, 20, 79, 8, 4, 79, 11, 15, 13, 79, 17, 79, 2])\n",
      " list([0, 10, 79, 9, 79, 19, 79, 43, 79, 2])\n",
      " list([0, 5, 25, 14, 5, 79, 79, 79, 9, 3, 79, 79, 79, 2])\n",
      " list([0, 17, 76, 79, 79, 79, 50, 13, 79, 79, 18, 37, 3, 38, 79, 79, 79, 79, 79, 8, 79, 2])\n",
      " list([0, 10, 31, 17, 79, 28, 79, 79, 40, 77, 37, 56, 79, 3, 79, 6, 10, 31, 79, 28, 37, 56, 24, 16, 79, 61, 77, 13, 79, 45, 79, 70, 12, 75, 79, 6, 39, 50, 13, 5, 25, 45, 79, 2])\n",
      " list([0, 4, 79, 14, 79, 46, 79, 42, 4, 79, 2])]\n",
      "2018-05-21 01:42:27: Loss after num_examples_seen=0 epoch=0: 4.391810\n",
      "2018-05-21 01:42:27: Loss after num_examples_seen=62 epoch=1: 3.764972\n",
      "2018-05-21 01:42:28: Loss after num_examples_seen=124 epoch=2: 3.189853\n",
      "2018-05-21 01:42:28: Loss after num_examples_seen=186 epoch=3: 3.111237\n",
      "2018-05-21 01:42:29: Loss after num_examples_seen=248 epoch=4: 3.076887\n",
      "2018-05-21 01:42:29: Loss after num_examples_seen=310 epoch=5: 3.053290\n",
      "2018-05-21 01:42:30: Loss after num_examples_seen=372 epoch=6: 3.031823\n",
      "2018-05-21 01:42:30: Loss after num_examples_seen=434 epoch=7: 3.009116\n",
      "2018-05-21 01:42:31: Loss after num_examples_seen=496 epoch=8: 2.983724\n",
      "2018-05-21 01:42:31: Loss after num_examples_seen=558 epoch=9: 2.955661\n"
     ]
    }
   ],
   "source": [
    "word_dim = 80\n",
    "hidden_dim = 10\n",
    "X_train, y_train = getSentenceData('data/sample-data.csv', word_dim)\n",
    "print(X_train)\n",
    "\n",
    "np.random.seed(10)\n",
    "rnn = Model(word_dim, hidden_dim)\n",
    "\n",
    "losses = rnn.train(X_train[:100], y_train[:100], learning_rate=0.005, nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.predict([0, 2, 1, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
